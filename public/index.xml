<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>My Portfolio on Natasha Smith Portfolio</title><link>http://localhost:1313/</link><description>Recent content in My Portfolio on Natasha Smith Portfolio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 20 Mar 2025 10:58:08 -0400</lastBuildDate><atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml"/><item><title>Part 1. Unveiling Sentiments: Analysing NASDAQ Companies through Big Data and Sentiment Analysis.</title><link>http://localhost:1313/projects/project4/project4_1/</link><pubDate>Sat, 10 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_1/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In an era defined by social media and digital transformation, the sheer volume of unstructured text data has emerged as a goldmine for businesses, investors, and analysts.&lt;/p>
&lt;p>Twitter, with its instantaneous and candid nature, offers a unique window into public sentiment. This blog dissects a technical project that analysed tweets related to NASDAQ-listed companies, including giants like Apple, Tesla, and Microsoft, over a five-year span (2015–2020).&lt;/p></description></item><item><title>Part 1. Transforming Breast Cancer Diagnosis with Deep Learning. The Future of AI in Histopathology: Innovations and Challenges.</title><link>http://localhost:1313/projects/project13/project13_1/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>The field of histopathology is witnessing a paradigm shift with the integration of artificial intelligence (AI) and deep learning (DL). This blog delves into a research project that explores advanced deep learning techniques to enhance breast cancer diagnosis using histopathology images (HIs). The project, based on the BreakHis dataset, leverages state-of-the-art convolutional neural networks (CNNs), ensemble methods, and interpretability tools such as Grad-CAM and LIME. These innovations aim to address critical challenges in clinical applications, including class imbalance, model reliability, and diagnostic accuracy.&lt;/p></description></item><item><title>Part 1. Building Custom CNN Architectures: From Scratch to Mastery.</title><link>http://localhost:1313/projects/project11/project11_1/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Convolutional Neural Networks (CNNs) have become the cornerstone of modern computer vision applications. From self-driving cars to medical imaging diagnostics, their applications are both transformative and ubiquitous.&lt;/p>
&lt;p>But while pre-trained models like ResNet and EfficientNet are readily available, there’s something uniquely empowering about building your own CNN architecture from scratch.&lt;/p>
&lt;p>In this blog, I’ll explore how to construct a custom CNN tailored for binary classification tasks. Whether you&amp;rsquo;re new to deep learning or looking to deepen your understanding, this guide will help you:&lt;/p></description></item><item><title>Part 1. Decoding Fashion MNIST: A Modern Benchmark for Deep Learning.</title><link>http://localhost:1313/projects/project12/project12_1/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>You’ve seen it before: the iconic handwritten digits from the MNIST dataset, the quintessential benchmark for machine learning enthusiasts.&lt;/p>
&lt;p>But here’s the thing—MNIST is old news. It’s solved, overused, and no longer representative of real-world challenges. Fashion MNIST is a modern, robust alternative that brings fresh complexity to the table.&lt;/p>
&lt;p>Fashion MNIST is a game-changer. With its focus on apparel images like shirts, sneakers, and dresses, it mirrors the kind of messy, nuanced data we deal with today.&lt;/p></description></item><item><title>PART 1. The Importance of Data Cleaning in Environmental Analysis</title><link>http://localhost:1313/projects/project9/project9_1/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Data is often called the backbone of machine learning, but in the real world, data is rarely clean or ready for use.&lt;/p>
&lt;p>This is especially true for environmental data, where missing values, outliers, and inconsistencies are common.&lt;/p>
&lt;p>When predicting PM10 pollution levels in Auckland, the first challenge wasn’t building a model but cleaning the data.&lt;/p></description></item><item><title>Part 1. Cleaning the Air: Data Pre-processing for PM10 Prediction.</title><link>http://localhost:1313/projects/project8/project8_1/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Have you ever stopped to think about the data behind environmental predictions?&lt;/p>
&lt;p>We hear a lot about air pollution and its devastating effects on our health, but what’s often overlooked is the behind-the-scenes work required to make accurate predictions.&lt;/p>
&lt;p>The first step in any data-driven environmental project is cleaning the data—and let me tell you, it’s not as simple as it sounds.&lt;/p></description></item><item><title>Part 1. Building a Data Warehouse: Technical Implementation and Challenges.</title><link>http://localhost:1313/projects/project3/project3_1/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_1/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In today&amp;rsquo;s data-driven world, the ability to analyse large datasets and derive meaningful insights is a game-changer for businesses. One of the most important tools that helps achieve this goal is a Data Warehouse (DW).&lt;/p>
&lt;p>In this blog post, I&amp;rsquo;ll walk through the technical journey of building a data warehouse for a company named &amp;lsquo;OfficeProducts&amp;rsquo;, including some of the core challenges faced and how they were addressed.&lt;/p></description></item><item><title>Part 1. Challenges in Medical Imaging Datasets.</title><link>http://localhost:1313/projects/project5/project5_1/</link><pubDate>Tue, 12 Sep 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_1/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Medical imaging datasets provide critical opportunities for deep learning (DL) applications, but they also come with unique challenges.&lt;/p>
&lt;p>In this project, aimed at detecting pneumonia using chest X-rays, we faced hurdles like &lt;strong>dataset imbalance&lt;/strong> and &lt;strong>small validation sets&lt;/strong>, which can hinder model performance.&lt;/p>
&lt;p>This blog discusses the key challenges and demonstrates pre-processing techniques—such as dataset re-sampling, data augmentation, and re-splitting—that helped us overcome these obstacles.&lt;/p></description></item><item><title>PART 1. Colour Normalisation in Histopathology. Enhancing Medical Image Consistency: Colour Normalisation Techniques for Histopathology</title><link>http://localhost:1313/projects/project10/project10_1/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Histopathology, the microscopic study of tissue to detect diseases like cancer, heavily relies on stained images. However, variations in staining protocols, imaging devices, and lighting conditions can introduce inconsistencies, which pose a challenge for machine learning (ML) models.&lt;/p>
&lt;p>Colour normalisation (CN) is a pre-processing step that standardises these images, ensuring consistency and enabling ML models to focus on disease-relevant features like cell shapes and abnormal structures. This blog explores:&lt;/p></description></item><item><title>Part 1. Preparing Recipe Data for NLP: Challenges and Techniques.</title><link>http://localhost:1313/projects/project2/project2_1/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_1/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Data preparation is one of the most crucial steps in any ML or natural language processing (NLP) project. For this project, I started with raw recipe text data, which contained a lot of unstructured information, like ingredient lists and cooking steps.
I used various data preparation techniques to clean, tokenise, and transform recipe data into a structured format. This foundation made it possible to extract meaningful insights from the data and apply techniques like clustering and topic modelling effectively.&lt;/p></description></item><item><title>Part 1. Building an AI-Powered Recipe Difficulty Classifier: A Journey Through NLP and ML.</title><link>http://localhost:1313/projects/project1/project1_1/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_1/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;div style="display: flex; align-items: center; gap: 10px;">
 &lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank" style="text-decoration: none;">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width: 40px; height: 40px; vertical-align: middle;">
 &lt;/a>
 &lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank" style="font-weight: bold; color: black;">
 View Project on GitHub
 &lt;/a>
&lt;/div>
&lt;h2 id="introduction">&lt;strong>Introduction&lt;/strong>&lt;/h2>
&lt;p>Cooking varies in complexity. Some recipes are straightforward, while others demand precision, technique, and skill. The challenge was to develop a ML model that classifies recipes into four difficulty levels—&lt;strong>Easy, Medium, Hard, and Very Hard&lt;/strong>—using &lt;strong>Natural Language Processing (NLP)&lt;/strong> and &lt;strong>Machine Learning (ML)&lt;/strong>. In this post, I focus on &lt;strong>data collection, cleaning, and pre-processing&lt;/strong>, which lay the foundation for training a robust ML model.&lt;/p></description></item><item><title>Part 2. Boosting Model Generalisation with Data Augmentation.</title><link>http://localhost:1313/projects/project5/project5_2/</link><pubDate>Fri, 20 Sep 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_2/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
&lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning (DL) models often struggle with over-fitting, especially when trained on limited datasets.&lt;/p>
&lt;p>To overcome this challenge in our pneumonia detection project, we used &lt;strong>data augmentation&lt;/strong> (DA) techniques to artificially expand the training dataset.&lt;/p>
&lt;p>DA techniques, such as rotations, scaling, flipping, and zooming, helped improve the model&amp;rsquo;s generalisation to unseen chest X-ray images.&lt;/p>
&lt;p>This blog explains the DA techniques applied, demonstrates the Python code used, and highlights how augmentation enhanced the performance of both the manual CNN and the pre-trained VGG16 models.&lt;/p></description></item><item><title>Part 2. Scalling Sentiment Analysis with MapReduce.</title><link>http://localhost:1313/projects/project4/project4_2/</link><pubDate>Sat, 10 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_2/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Sentiment analysis on massive datasets, like 4 million tweets, demands computational efficiency. Processing this data sequentially would take days or even weeks, making scalability a major concern.&lt;/p>
&lt;p>To address this, we employed &lt;strong>MapReduce&lt;/strong>, a distributed data processing model that enables parallel computation across multiple nodes.&lt;/p>
&lt;p>This blog walks through the implementation of &lt;strong>MapReduce&lt;/strong> for sentiment analysis, focusing on how it handles data at scale. We&amp;rsquo;ll include examples of mappers and reducers with Python code to explain the workflow.&lt;/p></description></item><item><title>Part 2. Handling Class Imbalance in Medical Imaging: A Deep Learning Perspective.</title><link>http://localhost:1313/projects/project13/project13_2/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="handling-class-imbalance-in-medical-imaging-a-deep-learning-perspective">&lt;strong>Handling Class Imbalance in Medical Imaging: A Deep Learning Perspective&lt;/strong>&lt;/h3>
&lt;p>Class imbalance is a common issue in histopathological datasets, such as the BreakHis dataset used for breast cancer detection. This imbalance, where benign samples constitute 31% and malignant samples 69%, can adversely affect model performance by causing the model to prioritize the majority class. In this blog, we explore techniques employed in your project, including &lt;strong>weighted loss functions&lt;/strong>, &lt;strong>data augmentation&lt;/strong>, and &lt;strong>stratified sampling&lt;/strong>, to address this challenge and enhance model performance.&lt;/p></description></item><item><title>Part 2. Designing Dense Neural Networks: Lessons from Fashion MNIST.</title><link>http://localhost:1313/projects/project12/project12_2/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Designing neural networks (NNs) is an art as much as it is a science. When faced with the challenge of classifying Fashion MNIST images, I needed a lightweight yet powerful architecture to handle the complexity of apparel images. Dense neural networks, with their fully connected layers, were the perfect choice for this task.&lt;/p>
&lt;p>In this blog, we’ll walk through:&lt;/p>
&lt;ul>
&lt;li>How dense neural networks work and their role in image classification.&lt;/li>
&lt;li>Designing an efficient architecture using activation functions and layers.&lt;/li>
&lt;li>Practical lessons learned while optimising performance with Fashion MNIST.&lt;/li>
&lt;/ul>
&lt;p>By the end, you’ll have a clear roadmap to build your own dense networks for image classification tasks.
Let’s dive in!&lt;/p></description></item><item><title>Part 2. Mastering Data Preparation and Augmentation: Building the Foundation for Better Image Classification Models.</title><link>http://localhost:1313/projects/project11/project11_2/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>The journey to building a high-performing image classification model begins long before training. Data preparation and augmentation are often overlooked but vital steps in ensuring your model learns effectively and generalises well. These processes form the bridge between raw, unstructured data and the structured inputs a machine learning model can use.&lt;/p>
&lt;p>In this blog, we will:&lt;/p>
&lt;ul>
&lt;li>Explore the essential techniques of data pre-processing, including resizing, normalization, and train-test splitting.&lt;/li>
&lt;li>Learn how data augmentation enhances model generalisation.&lt;/li>
&lt;li>Discuss strategies for addressing class imbalance to prevent biased models.&lt;/li>
&lt;li>Show how these steps contribute to real-world applications like medical imaging and fraud detection.&lt;/li>
&lt;/ul>
&lt;p>By the end, you’ll have a comprehensive understanding of why data preparation is the cornerstone of machine learning success.&lt;/p></description></item><item><title>PART 2. Understanding the Predictors of Air Pollution</title><link>http://localhost:1313/projects/project9/project9_2/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>What makes air pollution worse?&lt;/p>
&lt;p>Is it just traffic, or does the weather play a role too? Predicting air quality isn’t just about using machine learning (ML)—it’s about understanding the variables that drive pollution levels.&lt;/p>
&lt;p>In this blog, we dive into the heart of the Auckland PM10 prediction project: &lt;strong>feature selection&lt;/strong>.&lt;/p></description></item><item><title>Part 2. Exploring the Data: Understanding PM10 and Its Impact Through EDA.</title><link>http://localhost:1313/projects/project8/project8_2/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Behind every successful machine learning (ML) project is a stage that is equal parts science and art: &lt;strong>Exploratory Data Analysis (EDA)&lt;/strong>.&lt;/p>
&lt;p>This step is where we uncover the hidden stories in the data, identify patterns, and gain insights that inform the model-building process.&lt;/p>
&lt;p>When working with air pollution data, EDA plays a vital role in answering key questions:&lt;/p></description></item><item><title>Part 2. Source Data and ETL Process Explained.</title><link>http://localhost:1313/projects/project3/project3_2/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_2/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In every data warehouse project, a critical step involves understanding the source data and developing an effective ETL process to consolidate that data.&lt;/p>
&lt;p>For &amp;lsquo;OfficeProducts&amp;rsquo;, the source data was primarily gathered from two main files: the &lt;strong>Datastream&lt;/strong> file and the &lt;strong>Masterdata&lt;/strong> file. These two files formed the backbone of the entire data warehouse, feeding the transactional and product-related information necessary for meaningful analysis.&lt;/p></description></item><item><title>PART 2. Data Augmentation as a Robustness Strategy. Simplifying Pre-processing: Can Data Augmentation Replace Colour Normalisation?</title><link>http://localhost:1313/projects/project10/project10_2/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_2/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Pre-processing is the backbone of any machine learning (ML) pipeline, especially in medical imaging, where accuracy and reliability are paramount.&lt;/p>
&lt;p>Traditionally, &lt;strong>Colour Normalisation (CN)&lt;/strong> has been the gold standard for handling variability in histopathology images. However, advancements in &lt;strong>Data Augmentation (DA)&lt;/strong> techniques have opened the door to alternative workflows that promise simplicity without sacrificing performance.&lt;/p>
&lt;p>This blog investigates:&lt;/p>
&lt;ul>
&lt;li>The fundamentals of data augmentation.&lt;/li>
&lt;li>Key augmentation techniques for histopathology.&lt;/li>
&lt;li>A comparative analysis of DA and CN workflows using a DenseNet201 model.&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-data-augmentation">&lt;strong>What is Data Augmentation?&lt;/strong>&lt;/h2>
&lt;p>&lt;strong>Data augmentation&lt;/strong> artificially increases the size and diversity of a dataset by applying transformations to existing images. These transformations simulate variations the model might encounter in real-world data, improving its ability to generalise.&lt;/p></description></item><item><title>Part 2. From Words to Vectors: Embedding Techniques in Recipe Analysis.</title><link>http://localhost:1313/projects/project2/project2_2/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_2/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In a world driven by data, text is the unsung hero that powers everything from search engines to recommendation systems. For a data scientist, textual data isn&amp;rsquo;t just words—it&amp;rsquo;s a goldmine waiting to be unlocked. Recipes, for instance, are more than a collection of instructions. They&amp;rsquo;re narratives of culture, flavour profiles, and culinary creativity. But to analyse them computationally, we must first transform these words into something machines can process: &lt;em>vectors&lt;/em>. In this article, I’ll dive into how text embedding techniques like &lt;strong>TF-IDF&lt;/strong> and &lt;strong>Word2Vec&lt;/strong> can be applied to recipe data. By converting recipes into meaningful numerical representations, we uncover patterns and relationships hidden in the data.&lt;/p></description></item><item><title>Part 2. Exploring Feature Engineering for Recipe Classification: How AI Understands Cooking Complexity.</title><link>http://localhost:1313/projects/project1/project1_2/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_2/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In ML, features are measurable characteristics or properties that help a model make predictions. In recipe classification, features such as ingredient complexity, cooking techniques, and step count become powerful predictors of recipe difficulty. Feature engineering helps us take unstructured data, such as recipe instructions, and turn it into structured data that the model can understand.&lt;/p>
&lt;p>For example, a recipe with advanced ingredients (like &amp;ldquo;saffron&amp;rdquo; or &amp;ldquo;truffle oil&amp;rdquo;) is likely to be more challenging than one with everyday items like &amp;ldquo;salt&amp;rdquo; or &amp;ldquo;flour.&amp;rdquo; Similarly, recipes that involve techniques like &amp;ldquo;blanching&amp;rdquo; or &amp;ldquo;flambé&amp;rdquo; tend to require more skill than those involving basic steps like &amp;ldquo;stirring.&amp;rdquo;&lt;/p></description></item><item><title>Part 3. Manual CNN vs. Pre-Trained VGG16: A Comparative Analysis.</title><link>http://localhost:1313/projects/project5/project5_3/</link><pubDate>Fri, 20 Sep 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_3/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
&lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning (DL) provides multiple pathways to solving problems, including designing custom architectures or leveraging pre-trained models.&lt;/p>
&lt;p>In this blog, we compare the performance of a &lt;strong>manual CNN&lt;/strong> and the &lt;strong>VGG16 pre-trained model&lt;/strong> for pneumonia detection.&lt;/p>
&lt;p>While the manual CNN was lightweight and tailored to the dataset, VGG16 brought the power of transfer learning with its pre-trained &lt;strong>ImageNet&lt;/strong> weights.&lt;/p>
&lt;p>This comparative analysis explores their architectures, training strategies, and results.&lt;/p></description></item><item><title>Part 3. Visualising Market Sentiments with Hive and Kibana.</title><link>http://localhost:1313/projects/project4/project4_3/</link><pubDate>Sat, 10 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_3/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Data visualisation bridges the gap between raw data and actionable insights.&lt;/p>
&lt;p>After processing over 4 million tweets for sentiment analysis, the next step was to aggregate the results and make them accessible to analysts and decision-makers.&lt;/p>
&lt;p>Using &lt;strong>Hive&lt;/strong> for data aggregation and &lt;strong>Kibana&lt;/strong> for visualisation, we uncovered trends in public discourse around NASDAQ companies.&lt;/p>
&lt;p>This blog walks through the process of aggregating data with &lt;strong>Hive&lt;/strong> and creating interactive dashboards in &lt;strong>Kibana&lt;/strong>, complete with code snippets and visual examples.&lt;/p></description></item><item><title>Part 3. Choosing the Best CNN Architecture for Breast Cancer Detection: How Ensemble Models Improve Breast Cancer Detection with AI.</title><link>http://localhost:1313/projects/project13/project13_3/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;h3 id="choosing-the-best-cnn-architecture-for-breast-cancer-detection-how-ensemble-models-improve-accuracy">&lt;strong>Choosing the Best CNN Architecture for Breast Cancer Detection: How Ensemble Models Improve Accuracy&lt;/strong>&lt;/h3>
&lt;p>Deep learning has revolutionized breast cancer detection, especially with histopathological image analysis. Among the arsenal of Convolutional Neural Network (CNN) architectures, models like &lt;strong>ResNet&lt;/strong>, &lt;strong>DenseNet&lt;/strong>, and &lt;strong>EfficientNet&lt;/strong> have proven highly effective. However, instead of relying on a single architecture, combining them through ensemble learning often yields superior performance. In this blog, we’ll compare these top architectures and explore how an &lt;strong>ensemble approach using logistic regression&lt;/strong> as a meta-model improves diagnostic accuracy and robustness.&lt;/p></description></item><item><title>Part 3. Evaluating Model Performance: Metrics Beyond Accuracy for Better Insights.</title><link>http://localhost:1313/projects/project11/project11_3/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Accuracy is one of the most common metrics used to evaluate machine learning models, but it’s not always sufficient—especially in scenarios involving imbalanced datasets or high-stakes decisions. For example, a model with high accuracy might still fail to detect rare but critical events like fraud or disease.&lt;/p>
&lt;p>This blog aims to expand your understanding of model evaluation by:&lt;/p>
&lt;ul>
&lt;li>Exploring precision, recall, specificity, and F1-score to provide deeper insights into model performance.&lt;/li>
&lt;li>Introducing the &lt;code>Receiver Operating Characteristic (ROC)&lt;/code> curve and AUC for evaluating classification thresholds.&lt;/li>
&lt;li>Demonstrating these metrics with Python code and visualisations.&lt;/li>
&lt;/ul>
&lt;p>By the end, you’ll have the tools to evaluate your models comprehensively, ensuring they meet the demands of real-world challenges.&lt;/p></description></item><item><title>Part 3. ReLU vs Sigmoid: Which Activation Function Wins on Fashion MNIST?</title><link>http://localhost:1313/projects/project12/project12_3/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>When building neural networks (NNs), the activation function you choose can make or break your model. It’s the part of the network that decides whether a neuron &amp;ldquo;fires&amp;rdquo; and passes information forward.&lt;/p>
&lt;p>For years, Sigmoid was the go-to activation function, but then ReLU came along, revolutionising deep learning with its simplicity and effectiveness. But how do these activation functions stack up against each other in practice?&lt;/p></description></item><item><title>PART 3. Regression Models for Pollution Prediction</title><link>http://localhost:1313/projects/project9/project9_3/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Regression models form the backbone of many predictive analytics projects. They are simple yet powerful tools for understanding relationships between variables and forecasting outcomes.&lt;/p>
&lt;p>In this blog, I’ll explore how regression models were used to predict PM10 pollution levels in Auckland, their strengths and limitations, and how they provided valuable insights into air quality trends.&lt;/p></description></item><item><title>Part 3. Regression Models for Air Quality Prediction: From Simplicity to Accuracy.</title><link>http://localhost:1313/projects/project8/project8_3/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Predicting air pollution isn’t just about crunching numbers—it’s about finding patterns, building models, and learning how different variables interact with one another.&lt;/p>
&lt;p>In this blog, I take the first step toward accurate PM10 predictions by exploring regression models. These models form the backbone of many machine learning (ML) projects, providing interpretable results and insights into the relationships between variables.&lt;/p></description></item><item><title>Part 3. Designing the Snowflake Schema for Efficient Data Analysis.</title><link>http://localhost:1313/projects/project3/project3_3/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_3/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the world of data warehousing, choosing the right schema is a pivotal decision that determines the efficiency of the entire project.&lt;/p>
&lt;p>For &amp;lsquo;OfficeProducts&amp;rsquo;, we opted for a &lt;strong>Snowflake Schema&lt;/strong> to create a streamlined and high-performing data warehouse.&lt;/p>
&lt;p>This schema design allowed us to effectively reduce redundancy, maintain data integrity, and improve overall query performance, making it ideal for deep analysis of sales and product performance data.&lt;/p></description></item><item><title>PART 3. Building and Fine-Tuning DenseNet201 for Histopathology. Leveraging Deep Learning for Cancer Detection: Building a DenseNet201 Model</title><link>http://localhost:1313/projects/project10/project10_3/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_3/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning has revolutionised medical imaging, enabling precise and reliable detection of diseases like cancer.&lt;/p>
&lt;p>&lt;strong>DenseNet201&lt;/strong>, a state-of-the-art convolutional neural network (CNN), is particularly suited for histopathology image classification due to its dense connectivity and efficient feature reuse.&lt;/p>
&lt;p>This blog provides a step-by-step guide to building and fine-tuning a DenseNet201 model for classifying histopathology images into benign and malignant categories. Topics covered include:&lt;/p></description></item><item><title>Part 3. Uncovering Themes in Recipes with Topic Modelling.</title><link>http://localhost:1313/projects/project2/project2_3/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_3/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Recipes are more than just lists of ingredients and instructions—they encapsulate cultural, dietary, and thematic patterns waiting to be uncovered. In the ever-growing realm of textual data, topic modelling serves as a powerful tool to discover hidden themes and insights.&lt;/p>
&lt;p>In this blog, I’ll explore how topic modeling techniques, such as Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorisation (NMF), help us extract meaningful themes from recipes.&lt;/p></description></item><item><title>PART 3. Choosing the Right Model: Training and Evaluating an AI Recipe Difficulty Classifier</title><link>http://localhost:1313/projects/project1/project1_3/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_3/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the previous post, I explored how feature engineering transforms raw recipe data into valuable insights for predicting recipe difficulty. With features like ingredient complexity, technique identification, and step count, my dataset is now ready for the next stage: selecting, training, and evaluating a machine learning model that can classify recipes by difficulty level. Model selection is a crucial step in building a successful classifier. In this post, I’ll walk you through the models I tested, the training process, and the metrics I used to evaluate performance.&lt;/p></description></item><item><title>Part 4. Evaluating CNN Models for Pneumonia Detection.</title><link>http://localhost:1313/projects/project5/project5_4/</link><pubDate>Fri, 20 Sep 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
&lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Evaluating the performance of deep learning models in medical imaging projects requires more than just accuracy.&lt;/p>
&lt;p>Metrics like &lt;strong>precision&lt;/strong>, &lt;strong>recall&lt;/strong>, and &lt;strong>F1-score&lt;/strong> provide deeper insights, especially when minimising false negatives is critical, as in pneumonia detection.&lt;/p>
&lt;p>This blog explores how our models—&lt;strong>Manual CNN&lt;/strong> and &lt;strong>VGG16&lt;/strong>—were evaluated and highlights the role of confusion matrices in understanding their performance.&lt;/p>
&lt;h3 id="metrics-for-evaluation">Metrics for Evaluation&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Accuracy&lt;/strong>: The percentage of correctly classified samples.&lt;/p></description></item><item><title>Part 4. Word Clouds in Action: Decoding Public Opinion on NASDAQ Companies.</title><link>http://localhost:1313/projects/project4/project4_4/</link><pubDate>Sun, 25 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>While numerical analysis reveals overarching trends, visual representations like &lt;strong>word clouds&lt;/strong> provide an intuitive way to explore the most frequently used terms in a dataset.&lt;/p>
&lt;p>For this project, word clouds were generated to uncover qualitative insights from positive, neutral, and negative tweets about NASDAQ companies. These insights complemented the sentiment analysis, offering a richer understanding of public opinion.&lt;/p>
&lt;p>This blog covers how we created sentiment-specific word clouds, complete with Python code and examples of the insights they provided.&lt;/p></description></item><item><title>Part 4. Boosting AI Performance: Data Augmentation for Histopathological Imaging.</title><link>http://localhost:1313/projects/project13/project13_4/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;h3 id="boosting-ai-performance-data-augmentation-for-histopathological-imaging">&lt;strong>Boosting AI Performance: Data Augmentation for Histopathological Imaging&lt;/strong>&lt;/h3>
&lt;p>In medical imaging, especially in histopathology, deep learning models often face challenges such as limited datasets and class imbalances. These limitations can hinder the performance of models and their generalization to new data. A powerful technique to overcome these issues is &lt;strong>data augmentation&lt;/strong>—synthetically increasing the size and diversity of the training data. In this blog, we’ll dive into how data augmentation techniques like flipping, rotation, and scaling can enhance deep learning models for medical imaging.&lt;/p></description></item><item><title>Part 4. Hyperparameter Tuning for Neural Networks: The Fashion MNIST Approach.</title><link>http://localhost:1313/projects/project12/project12_4/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>When training neural networks (NNs), every parameter matters. Hyperparameters like learning rate and batch size aren’t learned by the model—they’re chosen by you. These settings can make or break your model’s performance. But how do you find the right combination?&lt;/p>
&lt;p>In this blog, I’ll take you through my experience fine-tuning hyperparameters for a NN trained on the Fashion MNIST dataset. We’ll cover:&lt;/p></description></item><item><title>Part 4. Tackling Overfitting in Deep Learning Models.</title><link>http://localhost:1313/projects/project11/project11_4/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning models have revolutionised machine learning, enabling breakthroughs in image recognition, natural language processing, and more.&lt;/p>
&lt;p>However, one common challenge that haunts even the most skilled practitioners is overfitting. Overfitting occurs when a model learns the training data too well, including its noise and irrelevant patterns, at the cost of generalising to new, unseen data.&lt;/p>
&lt;p>Imagine training a model to classify histopathological images of cancer 9as in my case). If the model overfits, it might memorise specific features of the training examples rather than learning the general structure of benign and malignant cases. The result? Stellar performance on the training data but poor results on validation or test data.&lt;/p></description></item><item><title>PART 4. Neural Networks in Environmental Data Analysis</title><link>http://localhost:1313/projects/project9/project9_4/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>When it comes to predicting air pollution, traditional regression models can only go so far. They’re great at identifying linear relationships but fall short when faced with the complex, non-linear patterns that often define real-world data. This is where neural networks (NNs) shine.&lt;/p>
&lt;p>In this blog, we’ll explore how NNs were leveraged to predict PM10 levels in Auckland, how they addressed the limitations of regression models, and why they became a critical tool in this project.&lt;/p></description></item><item><title>Part 4. Advanced Machine Learning for PM10 Prediction: Random Forest, XGBoost, and More.</title><link>http://localhost:1313/projects/project8/project8_4/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Regression models laid a solid foundation for PM10 prediction, but air pollution is a complex phenomenon influenced by nonlinear and time-dependent factors.&lt;/p>
&lt;p>To capture these intricacies, advanced machine learning models like neural networks (NNs) and ensemble methods come into play. These models are capable of uncovering patterns and relationships that simpler models might overlook.&lt;/p></description></item><item><title>Part 4. Multidimensional Data Analysis: Practical Insights.</title><link>http://localhost:1313/projects/project3/project3_4/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>The ultimate purpose of building a data warehouse is to be able to perform powerful and meaningful analyses to derive actionable business insights.&lt;/p>
&lt;p>For the &amp;lsquo;OfficeProducts&amp;rsquo; project, we carried out several forms of multidimensional analysis to answer key business questions related to product sales, customer trends, and promotional effectiveness.&lt;/p>
&lt;p>In this post, I&amp;rsquo;ll explore the practical insights gained through these analyses and how tools like &lt;strong>ROLLUP&lt;/strong> and &lt;strong>CUBE&lt;/strong> helped extract deeper trends from the data.&lt;/p></description></item><item><title>PART 4. Addressing Class Imbalance in Medical Image Datasets. Tackling Class Imbalance in Histopathology: Strategies and Insights</title><link>http://localhost:1313/projects/project10/project10_4/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In medical imaging datasets like histopathology, class imbalance is a common and critical challenge. For instance, datasets may contain significantly more benign samples than malignant ones, making it harder for models to learn to detect the minority class accurately. This can lead to poor sensitivity (recall), which is especially problematic in healthcare where identifying true positives is critical.&lt;/p>
&lt;p>In this blog, we explore:&lt;/p></description></item><item><title>Part 4. Clustering Recipes Based on Similarity: An Overview of Techniques and Challenges.</title><link>http://localhost:1313/projects/project2/project2_4/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction-clustering-recipes-based-on-similarity-an-overview-of-techniques-and-challenges">Introduction. Clustering Recipes Based on Similarity: An Overview of Techniques and Challenges&lt;/h3>
&lt;p>Clustering is a powerful unsupervised learning technique that organises data points into groups based on shared features.&lt;/p>
&lt;p>When applied to recipes, clustering can reveal hidden patterns, such as regional cuisines, ingredient pairings, or common preparation techniques.&lt;/p>
&lt;p>In this blog, we’ll explore:&lt;/p>
&lt;ol>
&lt;li>Clustering methods like K-Means and Hierarchical Clustering.&lt;/li>
&lt;li>Pre-processing and feature selection for recipe data.&lt;/li>
&lt;li>Evaluating clusters for meaningfulness.&lt;/li>
&lt;li>Challenges and lessons learned during clustering experiments.&lt;/li>
&lt;/ol>
&lt;h3 id="why-clustering-recipes">Why Clustering Recipes?&lt;/h3>
&lt;p>Clustering allows us to group recipes into meaningful categories based on similarity. For example:&lt;/p></description></item><item><title>Part 4. Tackling Overfitting in Recipe Difficulty Classification: Lessons Learned and Solutions.</title><link>http://localhost:1313/projects/project1/project1_4/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_4/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>As I progressed with training my AI-powered recipe classifier, I noticed a common issue creeping in: &lt;em>overfitting&lt;/em>, which happens when a model performs well on the training data but struggles to generalise to new, unseen data. In ML, this can result in poor accuracy on validation or test data. In this blog, I’ll walk you through how I identified overfitting in my model and the steps I took to address it. I’ll also explain the visual clues from training and validation loss/accuracy graphs that helped me recognise this issue.&lt;/p></description></item><item><title>Part 5. Insights from Sensitivity and Specificity Analysis in Pneumonia Detection.</title><link>http://localhost:1313/projects/project5/project5_5/</link><pubDate>Fri, 20 Sep 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
&lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>When evaluating AI models for medical diagnostics, metrics like &lt;strong>sensitivity&lt;/strong> and &lt;strong>specificity&lt;/strong> are crucial.&lt;/p>
&lt;p>Unlike general-purpose accuracy, these metrics provide deeper insights into how well a model distinguishes between true positive and true negative cases.&lt;/p>
&lt;p>For pneumonia detection, where false negatives can have severe consequences, understanding these metrics is essential.&lt;/p>
&lt;p>In this blog, I break down sensitivity and specificity, demonstrate their importance in model evaluation, and analyse how they influenced our choice between the Manual CNN and VGG16 models.&lt;/p></description></item><item><title>Part 5. Latent Themes in Tweets: Topic Modelling with LDA.</title><link>http://localhost:1313/projects/project4/project4_5/</link><pubDate>Sun, 25 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Social media conversations often revolve around recurring themes, making it essential to identify hidden patterns in large datasets. &lt;strong>Latent Dirichlet Allocation (LDA)&lt;/strong>, a popular topic modelling technique, enables us to uncover such latent themes by clustering similar words within documents.&lt;/p>
&lt;p>In this project, LDA helped reveal key topics in tweets about NASDAQ companies, such as product launches, stock performance, and CEO-driven discussions.&lt;/p></description></item><item><title>Part 5. Why AI Calibration is Critical for Reliable Breast Cancer Diagnosis.</title><link>http://localhost:1313/projects/project13/project13_5/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;h3 id="why-ai-calibration-is-critical-for-reliable-breast-cancer-diagnosis">&lt;strong>Why AI Calibration is Critical for Reliable Breast Cancer Diagnosis&lt;/strong>&lt;/h3>
&lt;p>AI-powered tools are revolutionizing healthcare by providing fast, accurate, and scalable diagnostic solutions. In breast cancer diagnosis, deep learning models, particularly Convolutional Neural Networks (CNNs), have shown remarkable promise. However, a highly accurate model is not necessarily a reliable one. This is where &lt;strong>AI calibration&lt;/strong> plays a critical role—ensuring that a model’s predicted probabilities align closely with the actual likelihood of events, making predictions more interpretable and trustworthy.&lt;/p></description></item><item><title>Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines.</title><link>http://localhost:1313/projects/project12/project12_5/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Machine learning models are only as good as the data they’re trained on. But even the best dataset won’t save you if your data splits are flawed. Splitting data into training, validation, and test sets seems straightforward, but small mistakes can lead to big problems like overfitting, underfitting, or unreliable performance metrics.&lt;/p>
&lt;p>You have a perfectly balanced dataset. Every class is equally represented, and it seems like splitting the data into training, testing, and validation sets should be a no-brainer. But even with balanced datasets like Fashion MNIST, thoughtful splitting is critical to ensure fair evaluation, reproducibility, and proper generalisation.&lt;/p></description></item><item><title>PART 5. Exploring Long Short-Term Memory (LSTM) for Time-Series Data</title><link>http://localhost:1313/projects/project9/project9_5/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Time-series data presents unique challenges and opportunities. The sequential nature of the data requires models capable of capturing dependencies over time—something traditional machine learning (ML)models often struggle with.&lt;/p>
&lt;p>In this blog, we delve into the use of Long Short-Term Memory (LSTM) networks, a type of recurrent neural network (RNN), to predict PM10 pollution levels in Auckland.&lt;/p></description></item><item><title>Part 5. Evaluating and Selecting the Best Models for PM10 Prediction.</title><link>http://localhost:1313/projects/project8/project8_5/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>After building and testing various machine learning models, the next critical step is evaluating their performance and selecting the best ones for deployment.&lt;/p>
&lt;p>In this blog, we’ll compare models using rigorous metrics like RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error).&lt;/p>
&lt;p>We’ll also explore hyperparameter tuning for neural networks, leveraging GridSearchCV for optimal performance.&lt;/p></description></item><item><title>Part 5. Sales Channels Performance and Promotions: Lessons Learned.</title><link>http://localhost:1313/projects/project3/project3_5/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Sales channel performance and promotional effectiveness are key aspects that drive business success.&lt;/p>
&lt;p>For &amp;lsquo;OfficeProducts&amp;rsquo;, understanding which channels contributed the most to overall sales and evaluating how promotions influenced sales trends were crucial.&lt;/p>
&lt;p>This blog post will walk you through how we analysed sales performance across various channels and promotions using the data warehouse, and the insights we gained from these analyses.&lt;/p></description></item><item><title>PART 5. Evaluation and Calibration: Building Trust in Medical AI Models</title><link>http://localhost:1313/projects/project10/project10_5/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning models are increasingly used in critical domains like healthcare. However, high accuracy alone doesn’t guarantee a model’s reliability.&lt;/p>
&lt;p>For medical AI systems, evaluation and calibration are key to building trust, ensuring fair predictions, and avoiding costly mistakes.&lt;/p>
&lt;p>In this blog, we’ll explore:&lt;/p>
&lt;ul>
&lt;li>The importance of model calibration.&lt;/li>
&lt;li>Key metrics: &lt;strong>F1-score&lt;/strong>, &lt;strong>Brier score loss&lt;/strong>, &lt;strong>ROC-AUC&lt;/strong>, and &lt;strong>confusion matrices&lt;/strong>.&lt;/li>
&lt;li>How to visualise and measure calibration using calibration curves.&lt;/li>
&lt;/ul>
&lt;h3 id="why-model-calibration-and-evaluation-matter">Why Model Calibration and Evaluation Matter&lt;/h3>
&lt;p>Medical imaging models often predict probabilities (e.g., &amp;ldquo;90% chance of malignancy&amp;rdquo;). But probability alone isn’t useful unless it reflects reality. For instance:&lt;/p></description></item><item><title>Part 5. Evaluating and Interpreting Recipe Clusters for Meaningful Insights.</title><link>http://localhost:1313/projects/project2/project2_5/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction-evaluating-and-interpreting-recipe-clusters-for-meaningful-insights">Introduction. Evaluating and Interpreting Recipe Clusters for Meaningful Insights&lt;/h3>
&lt;p>Clustering recipes can reveal fascinating patterns, but identifying meaningful clusters is only half the battle.&lt;/p>
&lt;p>The real challenge lies in evaluating their quality and interpreting their results effectively. Without rigorous evaluation, clusters might lack utility, leading to misleading conclusions.&lt;/p>
&lt;p>In this blog, we’ll cover:&lt;/p>
&lt;ol>
&lt;li>Techniques for evaluating cluster quality.&lt;/li>
&lt;li>Methods for interpreting recipe clusters.&lt;/li>
&lt;li>Challenges and insights gained from real-world clustering projects.&lt;/li>
&lt;/ol>
&lt;h3 id="why-does-cluster-evaluation-matter">Why Does Cluster Evaluation Matter?&lt;/h3>
&lt;p>Creating clusters is straightforward, but ensuring they represent meaningful groupings requires evaluation. For recipes, this means asking:&lt;/p></description></item><item><title>Part 5. Interpreting the AI Recipe Classifier with LIME: Making ML Transparent.</title><link>http://localhost:1313/projects/project1/project1_5/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In building a recipe difficulty classifier, I wanted to make sure the model&amp;rsquo;s predictions weren’t just accurate but also understandable. For anyone working with ML, especially in fields where transparency is key, model interpretability is crucial. This is where LIME (Local Interpretable Model-Agnostic Explanations) comes in.
In this blog post, I’ll walk you through how I used LIME to make sense of my classifier’s decisions, ensuring that its predictions are grounded and explainable.&lt;/p></description></item><item><title>Part 6. Future Directions for AI-Assisted Medical Imaging.</title><link>http://localhost:1313/projects/project5/project5_6/</link><pubDate>Fri, 20 Sep 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project5/project5_6/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project5_images/pr5.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
&lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>AI made significant strides in medical imaging, as demonstrated by our pneumonia detection project.&lt;/p>
&lt;p>However, the journey is far from complete. Future advancements in deep learning, real-world deployment, and ethical considerations will shape the role of AI in diagnostics and healthcare delivery.&lt;/p>
&lt;p>In this blog, we explore the potential and challenges of AI in medical imaging, including future directions for improving model performance, ensuring ethical deployment, and scaling solutions for global healthcare.&lt;/p></description></item><item><title>Part 6. Sentiment Trends: Insights by Company and Year.</title><link>http://localhost:1313/projects/project4/project4_6/</link><pubDate>Sun, 25 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_6/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Understanding how public sentiment evolves over time provides critical insights into the factors shaping market perceptions. In this blog, we analyze sentiment trends for NASDAQ companies, exploring how significant events—such as product launches, earnings calls, or controversies—impacted public opinion. Using time-series analysis, we visualized longitudinal sentiment patterns, highlighting their value for investors and analysts.&lt;/p>
&lt;hr>
&lt;h3 id="step-1-aggregating-sentiment-by-date">&lt;strong>Step 1: Aggregating Sentiment by Date&lt;/strong>&lt;/h3>
&lt;p>The first step was to aggregate sentiment counts for each company by date. This created a time-series dataset that allowed us to track changes in sentiment over time.&lt;/p></description></item><item><title>Part 6. Making AI Transparent: Grad-CAM and LIME in Medical Image Analysis.</title><link>http://localhost:1313/projects/project13/project13_6/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the ever-evolving field of AI, deep learning (DL) has emerged as a transformative force, reshaping industries and driving innovation. In medical imaging, where precision and interpretability are critical, advanced techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) and LIME (Local Interpretable Model-agnostic Explanations) are becoming essential tools for understanding how models make decisions.&lt;/p>
&lt;p>This project leverages Grad-CAM and LIME to explain predictions made by cutting-edge deep learning models like &lt;code>ResNet50&lt;/code>, &lt;code>EfficientNetB0&lt;/code>, and &lt;code>DenseNet201&lt;/code> for breast cancer diagnosis. By visualising what a model &amp;ldquo;sees&amp;rdquo; and validating its decision-making process, we bridge the gap between AI&amp;rsquo;s technical prowess and the human trust required for adoption in critical healthcare settings.&lt;/p></description></item><item><title>Part 6. Mastering Ensembling Techniques: Boosting Model Performance with Stacking and Voting.</title><link>http://localhost:1313/projects/project11/project11_6/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>No single model is perfect, and each has its own strengths and weaknesses. Ensembling techniques address this by combining predictions from multiple models to create a stronger, more robust model. Whether you’re using bagging, boosting, stacking, or voting, ensembling is a powerful strategy to achieve higher accuracy and better generalization.&lt;/p>
&lt;p>In this blog, we’ll focus on:&lt;/p>
&lt;p>The fundamentals of stacking and soft voting.
Implementing stacking with a meta-model.
Using soft voting for combined predictions.
Evaluating ensemble models with metrics like ROC-AUC.
By the end, you’ll be able to implement and evaluate ensemble methods for your own machine learning projects.&lt;/p></description></item><item><title>Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks.</title><link>http://localhost:1313/projects/project12/project12_6/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Training a neural network requires more than just a good dataset or an effective architecture—it requires the right optimiser. Stochastic Gradient Descent (SGD) is a staple of deep learning.&lt;/p>
&lt;p>In my Fashion MNIST project, I used Stochastic Gradient Descent (SGD) to optimise a dense neural network. Why? Because simplicity doesn’t just work—it excels, especially when resources are limited or interpretability is key.&lt;/p></description></item><item><title>PART 6. Comparing Models and Real-World Implications</title><link>http://localhost:1313/projects/project9/project9_6/</link><pubDate>Sat, 03 Feb 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project9/project9_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project9_images/pr9.jpg"
 alt="Photo by Markus Distelrath on Pexels">&lt;figcaption>
 &lt;p>Photo by Markus Distelrath on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Artificial intelligence (AI) and machine learning (ML) are not just tools for academic research—they hold transformative potential for real-world applications.&lt;/p>
&lt;p>In this final blog, we focus on translating our findings into actionable insights:&lt;/p>
&lt;ul>
&lt;li>How can the models and predictions generated in this project help policymakers, urban planners, and individuals?&lt;/li>
&lt;li>What are the future possibilities for AI in environmental health?&lt;/li>
&lt;/ul>
&lt;h3 id="1-the-power-of-predictions">1. The Power of Predictions&lt;/h3>
&lt;p>&lt;strong>Turning Numbers Into Actions&lt;/strong>&lt;/p></description></item><item><title>Part 6. From Data to Action: What Pollution Data and AI Teach Us About Cleaner Air.</title><link>http://localhost:1313/projects/project8/project8_6/</link><pubDate>Mon, 20 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project8/project8_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project8_images/pr8.jpg"
 alt="Photo by Dom J on Pexels">&lt;figcaption>
 &lt;p>Photo by Dom J on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Predicting PM10 and other pollutants is not just about building models or visualising data—it&amp;rsquo;s about understanding the invisible threats in the air we breathe and finding ways to address them.&lt;/p>
&lt;p>After exploring data pre-processing, modelling, and evaluation, this final piece reflects on the insights gained and their real-world implications.&lt;/p>
&lt;p>I’ll discuss how the results from this project—built on advanced AI/ML techniques—can guide better decision-making for policymakers, businesses, and citizens alike.&lt;/p></description></item><item><title>Part 6. Recommendations for Management and Business Insights.</title><link>http://localhost:1313/projects/project3/project3_6/</link><pubDate>Thu, 09 Nov 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project3/project3_6/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project3_images/pr3.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/warehouse-management-system" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Data-driven insights play a crucial role in helping businesses make informed decisions that align with market dynamics.&lt;/p>
&lt;p>For &amp;lsquo;OfficeProducts&amp;rsquo;, the analysis conducted using the data warehouse provided several valuable insights that translated into actionable recommendations for management.&lt;/p>
&lt;p>In this post, I&amp;rsquo;ll cover key strategies derived from the data, including &lt;strong>sales trends, promotion effectiveness&lt;/strong>, and &lt;strong>bundling opportunities&lt;/strong>, which were designed to help &amp;lsquo;OfficeProducts&amp;rsquo; optimise its business performance.&lt;/p></description></item><item><title>Part 6. Applications of Recipe Topic Modelling and Clustering.</title><link>http://localhost:1313/projects/project2/project2_6/</link><pubDate>Sun, 09 Apr 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project2/project2_6/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project2_images/pr2.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction-applications-of-recipe-topic-modelling-and-clustering-in-real-life">Introduction. Applications of Recipe Topic Modelling and Clustering in Real Life&lt;/h3>
&lt;p>Recipes are more than just instructions—they are cultural artifacts, sources of inspiration, and solutions to everyday problems. But as the number of recipes available online grows exponentially, finding the right one can feel overwhelming. That’s where the power of topic modelling and clustering comes into play.&lt;/p>
&lt;p>These techniques unlock the potential of recipe data, helping users discover, search, and explore recipes in ways that feel intuitive and tailored. In this blog, we’ll explore:&lt;/p></description></item><item><title>Part 6. Deploying an AI Model for Recipe Classification: Bringing the Classifier to Life.</title><link>http://localhost:1313/projects/project1/project1_6/</link><pubDate>Sat, 09 Apr 2022 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project1/project1_6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Once a model that classifies recipes by difficulty level is built and trained , the next challenge is deploying it into a real-world environment. In this blog, we’ll cover the process of moving our trained model from a development setting to a production environment. Deployment enables the model to make predictions and serve users in real-time, opening up possibilities for applications like recipe recommendation engines, cooking assistant apps, or culinary content platforms.&lt;/p></description></item><item><title>Part 7. Overcoming Challenges in Sentiment Analysis.</title><link>http://localhost:1313/projects/project4/project4_7/</link><pubDate>Sun, 25 Aug 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project4/project4_7/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project4_images/pr4.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/sentiment-analysis-NASDAQ-companies-Tweets" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Sentiment analysis offers a window into public opinion but comes with its own set of challenges. Sarcasm, evolving language, and biased data can lead to misclassification, impacting the reliability of results.&lt;/p>
&lt;p>In this blog, we dive into the hurdles encountered during sentiment analysis on over 4 million tweets about NASDAQ companies and explore solutions to address them.&lt;/p>
&lt;h3 id="key-challenges-in-sentiment-analysis">Key Challenges in Sentiment Analysis&lt;/h3>
&lt;h4 id="1-sarcasm-and-context-dependency">1. Sarcasm and Context Dependency&lt;/h4>
&lt;ul>
&lt;li>Tweets like &lt;em>&amp;ldquo;Oh great, another Tesla delay. Just what we needed!&amp;rdquo;&lt;/em> express negative sentiment despite containing positive words like &amp;ldquo;great.&amp;rdquo;&lt;/li>
&lt;li>Contextual understanding is essential for accurate classification.&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Solution&lt;/em>:&lt;/p></description></item><item><title>Part 7. Evaluating AI Models for Healthcare: Beyond Accuracy.</title><link>http://localhost:1313/projects/project13/project13_7/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_7/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;h3 id="evaluating-ai-models-for-healthcare-beyond-accuracy">&lt;strong>Evaluating AI Models for Healthcare: Beyond Accuracy&lt;/strong>&lt;/h3>
&lt;p>In healthcare, the stakes are higher than in most other fields. A seemingly high-performing AI model that achieves 95% accuracy may still fail to detect critical cases, leading to life-threatening consequences. For clinical applications, performance metrics like &lt;strong>sensitivity&lt;/strong>, &lt;strong>specificity&lt;/strong>, and &lt;strong>Area Under the Curve (AUC)&lt;/strong> provide a more nuanced evaluation, ensuring AI models align with real-world needs.&lt;/p></description></item><item><title>Part 7. Building Robust End-to-End Image Classification Pipelines.</title><link>http://localhost:1313/projects/project11/project11_7/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_7/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the world of machine learning, image classification is one of the most common and impactful applications.&lt;/p>
&lt;p>From detecting diseases in medical imaging to identifying products in e-commerce, the ability to categorise images accurately has transformed industries.&lt;/p>
&lt;p>However, building an effective image classification model requires more than just training a neural network—it demands a robust, end-to-end pipeline that can handle the entire process, from raw data to deployment.&lt;/p></description></item><item><title>Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification.</title><link>http://localhost:1313/projects/project12/project12_7/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_7/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Choosing the right loss function is one of the most critical decisions when building a neural network. For multi-class classification tasks, like predicting clothing categories in Fashion MNIST, the sparse categorical crossentropy (SCC) loss function is often the go-to solution. But what makes it so effective?&lt;/p>
&lt;p>This blog dives into:&lt;/p>
&lt;ul>
&lt;li>What sparse categorical crossentropy is and how it works.&lt;/li>
&lt;li>Why it’s the ideal choice for tasks involving multiple classes.&lt;/li>
&lt;li>How to implement it efficiently in TensorFlow/Keras.&lt;/li>
&lt;/ul>
&lt;p>By the end, you’ll have a solid understanding of this loss function and when to use it in your own projects.&lt;/p></description></item><item><title>Part 8. Unveiling Hidden Patterns: Feature Extraction with Pre-Trained CNNs.</title><link>http://localhost:1313/projects/project13/project13_8/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_8/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;p>Got it! Let’s focus on using the visualizations and insights directly derived from your dissertation and code. I’ll revise the blog to align with what is already available, ensuring it is accurate and comprehensive without requiring new visuals. Here’s the updated write-up:&lt;/p>
&lt;hr>
&lt;h3 id="introduction">&lt;strong>Introduction&lt;/strong>&lt;/h3>
&lt;p>Medical imaging has revolutionized diagnostics, offering clinicians unprecedented insight into human health. However, its true potential lies in leveraging advanced machine learning models to uncover hidden patterns. This blog explores how three cutting-edge deep learning models—&lt;strong>ResNet50&lt;/strong>, &lt;strong>EfficientNetB0&lt;/strong>, and &lt;strong>DenseNet201&lt;/strong>—extract meaningful features from medical images. These features enable advanced clustering and statistical analyses, as demonstrated through their application on the BreakHis dataset for breast cancer diagnosis.&lt;/p></description></item><item><title>Part 8. Trial and Error in Neural Network Training: Lessons from Fashion MNIST</title><link>http://localhost:1313/projects/project12/project12_8/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project12/project12_8/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project12_images/pr12.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Training neural networks (NNs) is a lot like navigating uncharted waters. No matter how much preparation or theoretical knowledge you have, it’s the experiments—and the inevitable mistakes—that shape your skills.&lt;/p>
&lt;p>As a data scientist working on Fashion MNIST, a dataset of 28x28 grayscale images representing 10 clothing categories, I realised that building effective models requires more than just writing code; it demands iteration, debugging, and adaptability.&lt;/p></description></item><item><title>PART 9. Clustering Breast Cancer Features: An Innovative Approach Using CNNs.</title><link>http://localhost:1313/projects/project13/project13_9/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_9/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">&lt;strong>Introduction&lt;/strong>&lt;/h3>
&lt;p>Breast cancer imaging datasets provide invaluable data for early detection and diagnostics. However, even within the same diagnostic category, significant variations can exist. These intra-class variations often hold critical information about tumor subtypes, aggressiveness, and treatment response. By employing &lt;strong>hierarchical clustering&lt;/strong> and &lt;strong>statistical analysis&lt;/strong>, researchers can uncover these subtle differences, enabling personalized diagnostics and treatment strategies. In this blog, we’ll explore how these techniques can be applied to breast cancer imaging datasets to drive precision medicine.&lt;/p></description></item><item><title>Part 10. Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data.</title><link>http://localhost:1313/projects/project13/project13_10/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_10/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">&lt;strong>Introduction&lt;/strong>&lt;/h3>
&lt;p>Deep learning (DL) models, particularly Convolutional Neural Networks (CNNs), are powerful tools for analysing medical imaging data. However, their &amp;ldquo;black-box&amp;rdquo; nature often limits their utility in sensitive applications like breast cancer diagnostics, where interpretability is paramount. By combining CNN feature extraction with statistical analysis, we can enhance model interpretability, revealing meaningful patterns and offering deeper insights into the data.&lt;/p></description></item><item><title>Part 11. Deploying AI Models for Breast Cancer Diagnosis: Challenges and Solutions Description.</title><link>http://localhost:1313/projects/project13/project13_11/</link><pubDate>Wed, 17 Jul 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project13/project13_11/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project13_images/pr13.jpg">&lt;figcaption>
 &lt;h4>Photo by Ben Hershey on Unsplash&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deploying AI models for clinical use, particularly in breast cancer diagnosis, is a multi-faceted challenge. My project on the BreakHis dataset highlighted several computational and practical hurdles, such as optimising resource usage, addressing class imbalance, and ensuring model compatibility with real-world clinical workflows. This blog explores these challenges and the solutions implemented in my work, including specific metrics, code snippets, and insights.&lt;/p></description></item><item><title>AI-Powered Recipe Assistant: Smart Cooking with AI</title><link>http://localhost:1313/projects/project14/project14/</link><pubDate>Wed, 20 Mar 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project14/project14/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project14_images/project14.png">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/AI-Assistant-Recipe" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>The &lt;strong>AI-Powered Recipe Assistant&lt;/strong> is an interactive tool designed to help users &lt;strong>generate, customise, and save recipes&lt;/strong> using AI. It leverages &lt;strong>large language models (LLMs)&lt;/strong> to dynamically create recipes based on user-provided ingredients and preferences.&lt;/p>
&lt;p>Built with &lt;strong>Streamlit&lt;/strong>, the assistant offers a clean UI where users can:&lt;/p>
&lt;ul>
&lt;li>Find recipes from a curated database.&lt;/li>
&lt;li>Generate AI-created recipes when no match is found.&lt;/li>
&lt;li>Save and manage favourite recipes.&lt;/li>
&lt;/ul>
&lt;p>This project demonstrates the &lt;strong>fusion of AI, NLP, and data-driven decision-making&lt;/strong>, transforming how users interact with recipe recommendations.&lt;/p></description></item><item><title>Part 5. Advanced Regularisation Techniques for CNNs.</title><link>http://localhost:1313/projects/project11/project11_5/</link><pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project11/project11_5/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project11_images/pr11.jpg">
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Convolutional Neural Networks (CNNs) have transformed machine learning, excelling in fields like image recognition, object detection, and medical imaging.&lt;/p>
&lt;p>However, like all machine learning models, CNNs are prone to overfitting, where the model performs well on the training data but struggles to generalise to unseen data. This is where regularisation comes into play.&lt;/p>
&lt;p>Regularisation techniques are designed to prevent overfitting and improve generalisation, making your CNN robust and reliable.&lt;/p></description></item><item><title>Solving a 4x4 Letter Placement Puzzle Using Genetic Algorithms.</title><link>http://localhost:1313/projects/project7/project7/</link><pubDate>Tue, 20 Jun 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project7/project7/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project7_images/pr7.jpg"
 alt="Photo by Magda Ehlers on Pexels">&lt;figcaption>
 &lt;p>Photo by Magda Ehlers on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Solving-a-4x4-Letter-Placement-Puzzle-Using-Genetic-Algorithms" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Combinatorial optimisation problems, like puzzles and scheduling tasks, often have large solution spaces that make them challenging to solve using traditional methods.&lt;/p>
&lt;p>This project leverages &lt;strong>Genetic Algorithms (GAs)&lt;/strong>, a nature-inspired optimisation technique, to solve a &lt;strong>4x4 letter placement puzzle&lt;/strong>. The puzzle requires arranging letters in a grid while meeting specific constraints.&lt;/p></description></item><item><title>BLA BLA BLA.</title><link>http://localhost:1313/projects/project15/project15/</link><pubDate>Mon, 20 Mar 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project15/project15/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project1_images/pr1.jpg"
 alt="Image by Brett Sayles on Pexels">&lt;figcaption>
 &lt;p>Image by Brett Sayles on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Client-Server-Network-Socket-Programming" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the world of distributed systems, client-server architecture forms the backbone of communication between applications.&lt;/p>
&lt;p>This project demonstrates the development of a &lt;strong>secure client-server system&lt;/strong> using Python, incorporating features like &lt;strong>serialisation&lt;/strong>, &lt;strong>file encryption&lt;/strong>, and &lt;strong>multi-threading&lt;/strong> to ensure reliability and security.&lt;/p>
&lt;p>This blog walks through the design, implementation, and key features of this system, offering insights into how these concepts can be applied in real-world scenarios.&lt;/p></description></item><item><title>Building a Secure Client-Server System with Python.</title><link>http://localhost:1313/projects/project6/project6/</link><pubDate>Mon, 20 Mar 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project6/project6/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/project6_images/pr6.jpg"
 alt="Image by Brett Sayles on Pexels">&lt;figcaption>
 &lt;p>Image by Brett Sayles on Pexels&lt;/p>
 &lt;/figcaption>
&lt;/figure>

&lt;p>&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/Client-Server-Network-Socket-Programming" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In the world of distributed systems, client-server architecture forms the backbone of communication between applications.&lt;/p>
&lt;p>This project demonstrates the development of a &lt;strong>secure client-server system&lt;/strong> using Python, incorporating features like &lt;strong>serialisation&lt;/strong>, &lt;strong>file encryption&lt;/strong>, and &lt;strong>multi-threading&lt;/strong> to ensure reliability and security.&lt;/p>
&lt;p>This blog walks through the design, implementation, and key features of this system, offering insights into how these concepts can be applied in real-world scenarios.&lt;/p></description></item><item><title>About Me</title><link>http://localhost:1313/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/about/</guid><description>&lt;figure>&lt;img src="http://localhost:1313/images/1.jpg">&lt;figcaption>
 &lt;h4>Hi 👋, I&amp;#39;m Natasha (Dr. Smith) — AI Data Scientist | Business Strategist&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>I’m most in my element where &lt;strong>data, AI, and business strategy&lt;/strong> converge, tackling complex challenges that drive real-world impact. With 20+ years of hands-on data experience, I blend deep technical expertise with domain knowledge in &lt;strong>business economics, management, and innovation&lt;/strong>.&lt;/p>
&lt;p>As a &lt;strong>quant and applied statistician&lt;/strong>, I specialise in &lt;strong>AI engineering and machine learning&lt;/strong>, uncovering insights, optimising decision-making, and building AI-powered solutions that move the needle. But my strength goes beyond developing models—I translate complex technical findings into actionable business intelligence.&lt;/p></description></item><item><title>Contact</title><link>http://localhost:1313/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/contact/</guid><description>&lt;p>If you&amp;rsquo;d like to reach out, feel free to contact me via email or connect on social media.&lt;/p>
&lt;p>I’m building a community of AI and data professionals and enthusiasts— and would love for you to &lt;strong>&lt;a href="https://www.linkedin.com/in/drnsmith/">connect with me on LinkedIn&lt;/a>&lt;/strong>!&lt;/p>
&lt;p>If my content resonates with you, &lt;strong>&lt;a href="https://medium.com/@NeverOblivious">follow me on Medium&lt;/a>&lt;/strong> and &lt;strong>&lt;a href="https://substack.com/@neveroblivious">subscribe to my Substack&lt;/a>&lt;/strong>.&lt;/p>
&lt;div style="display: flex; gap: 20px; justify-content: center; margin-top: 20px;">
 &lt;a href="mailto:drnatashasth@gmail.com" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/gmail.png" alt="Gmail" style="width: 30px; height: 30px; vertical-align: middle;">
 Email
 &lt;/a>
 &lt;a href="https://www.linkedin.com/in/drnsmith/" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/linkedin.png" alt="LinkedIn" style="width: 30px; height: 30px; vertical-align: middle;">
 LinkedIn
 &lt;/a>
 &lt;a href="https://twitter.com/yourtwitterhandle" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/twitter.png" alt="Twitter" style="width: 30px; height: 30px; vertical-align: middle;">
 Twitter
 &lt;/a>
 &lt;a href="https://github.com/drnsmith" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width: 30px; height: 30px; vertical-align: middle;">
 GitHub
 &lt;/a>
 &lt;a href="https://substack.com/@neveroblivious" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/substack.png" alt="Substack" style="width: 30px; height: 30px; vertical-align: middle;">
 Substack
 &lt;/a>
 &lt;a href="https://medium.com/@NeverOblivious" target="_blank" style="text-decoration: none; color: black;">
 &lt;img src="http://localhost:1313/images/medium.png" alt="Medium" style="width: 30px; height: 30px; vertical-align: middle;">
 Medium
 &lt;/a>
&lt;/div></description></item></channel></rss>