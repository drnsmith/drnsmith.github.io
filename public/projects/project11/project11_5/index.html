<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 5. Advanced Regularisation Techniques for CNNs. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Regularisation prevents overfitting and improves model generalisation. This blog covers advanced techniques like L1/L2 regularisation, batch normalisation, and data-driven regularisation methods.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project11_images/pr11.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 5. Advanced Regularisation Techniques for CNNs.
          </h1>
          
            <p class="f5">Regularisation prevents overfitting and improves model generalisation. This blog covers advanced techniques like L1/L2 regularisation, batch normalisation, and data-driven regularisation methods.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project11_images/pr11.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Convolutional Neural Networks (CNNs) have transformed machine learning, excelling in fields like image recognition, object detection, and medical imaging.</p>
<p>However, like all machine learning models, CNNs are prone to overfitting, where the model performs well on the training data but struggles to generalise to unseen data. This is where regularisation comes into play.</p>
<p>Regularisation techniques are designed to prevent overfitting and improve generalisation, making your CNN robust and reliable.</p>
<p>Beyond the basics, advanced regularisation methods like L1/L2 regularisation, batch normalisation, and data-driven regularisation techniques offer powerful ways to fine-tune your model.</p>
<p>In this blog, we will:</p>
<ul>
<li>Understand why regularisation is crucial for CNNs.</li>
<li>Explore advanced regularisation techniques, including their mathematical foundations and practical implementation.</li>
<li>Discuss real-world applications of these techniques to enhance CNN performance.</li>
</ul>
<h3 id="technical-explanation">Technical Explanation</h3>
<h4 id="why-do-cnns-need-regularisation">Why Do CNNs Need Regularisation?</h4>
<p>CNNs often have millions of parameters due to their complex architectures, making them susceptible to overfitting. Regularisation combats this by introducing constraints or additional information to the learning process. This ensures the model focuses on essential patterns rather than noise in the data.</p>
<h3 id="advanced-regularisation-techniques">Advanced Regularisation Techniques</h3>
<h4 id="1-l1-and-l2-regularisation">1. L1 and L2 Regularisation</h4>
<p><strong>L1 Regularization (Lasso)</strong></p>
<p>L1 regularisation penalises the sum of the absolute values of the weights:</p>
<p>[
\text{Loss}<em>{\text{L1}} = \text{Loss}</em>{\text{Original}} + \lambda \sum_{i} |w_i|
]</p>
<ul>
<li>Encourages sparsity by driving less important weights to zero.</li>
<li>Useful for feature selection in CNNs.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Dense
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.regularizers <span style="color:#f92672">import</span> l1, l2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, kernel_regularizer<span style="color:#f92672">=</span>l2(<span style="color:#ae81ff">0.01</span>)),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, kernel_regularizer<span style="color:#f92672">=</span>l1(<span style="color:#ae81ff">0.01</span>)),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>])
</span></span></code></pre></div><p><strong>L2 Regularisation (Ridge)</strong>
L2 regularisation penalises the sum of the squared weights:</p>
<p>[
\text{Loss}<em>{\text{L2}} = \text{Loss}</em>{\text{Original}} + \lambda \sum_{i} w_i^2
]</p>
<ul>
<li>Encourages smaller weights, reducing the model’s sensitivity to individual parameters.</li>
</ul>
<h4 id="2-batch-normalisation">2. Batch Normalisation</h4>
<p>Batch normalisation normalises the inputs of each layer during training, stabilising learning and reducing the dependence on initialisation. It also acts as an implicit regularzer by reducing internal covariate shift.</p>
<p>[
\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
]</p>
<p>Where:</p>
<ul>
<li>
<p>( \mu ): Mean of the current mini-batch.</p>
</li>
<li>
<p>( \sigma^2 ): Variance of the current mini-batch.</p>
</li>
<li>
<p>( \epsilon ): Small constant to avoid division by zero.</p>
</li>
<li>
<p>Accelerates training by allowing higher learning rates.</p>
</li>
<li>
<p>Reduces the need for dropout in some cases.</p>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> BatchNormalization
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>    BatchNormalization(),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),
</span></span><span style="display:flex;"><span>    BatchNormalization(),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)
</span></span><span style="display:flex;"><span>])
</span></span></code></pre></div><h4 id="learning-rate-schedulling">Learning Rate Schedulling</h4>
<p>Learning rate schedulling dynamically adjusts the learning rate during training to improve convergence and prevent overfitting.</p>
<h3 id="inverse-time-decay">Inverse Time Decay</h3>
<p>The inverse time decay schedule reduces the learning rate as training progresses:</p>
<p>[
\text{Learning Rate} = \frac{\text{Initial Rate}}{1 + \text{Decay Rate} \cdot \text{Epochs}}
]</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.optimizers.schedules <span style="color:#f92672">import</span> InverseTimeDecay
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr_schedule <span style="color:#f92672">=</span> InverseTimeDecay(
</span></span><span style="display:flex;"><span>    initial_learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>,
</span></span><span style="display:flex;"><span>    decay_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>    decay_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    staircase<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This schedule starts with a learning rate of <code>0.001</code> and decreases it over time for finer adjustments during training.</p>
<h3 id="real-world-applications">Real-World Applications</h3>
<h4 id="medical-imaging">Medical Imaging</h4>
<p>Regularisation techniques like dropout and batch normalisation are crucial in medical imaging tasks, where datasets are often small. These methods ensure the CNN generalises well and avoids overfitting, enabling accurate diagnoses.</p>
<p>For example,</p>
<ul>
<li>Histopathological image classification of cancer cells using L2 regularisation and dropout.</li>
</ul>
<h4 id="autonomous-vehicles">Autonomous Vehicles</h4>
<p>CNNs used in autonomous vehicles must generalise across varied lighting and weather conditions. Data augmentation plays a critical role in creating robust models capable of handling real-world variability.</p>
<p>For example,</p>
<ul>
<li>Augmenting road scene datasets with brightness shifts, rotations, and flips.</li>
</ul>
<h4 id="retail-image-analysis">Retail Image Analysis</h4>
<p>In tasks like product categorisation or shelf analysis, CNNs must handle high intra-class variability. Techniques like learning rate scheduling and L1 regularisation ensure the models are both accurate and efficient.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Advanced regularisation techniques like L1/L2 regularisation, batch normalisation, dropout, and data-driven methods such as data augmentation are powerful tools to combat overfitting and enhance model generalisation.</p>
<p>These techniques ensure your CNNs remain robust, scalable, and reliable in real-world scenarios. By applying these methods in your projects, you can build models that balance learning and generalisation, unlocking the full potential of deep learning.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and stay healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://drnsmith.github.io/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


