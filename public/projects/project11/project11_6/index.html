<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 6. Mastering Ensembling Techniques: Boosting Model Performance with Stacking and Voting. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="This blog explores ensembling techniques that combine the strengths of multiple models to improve predictive performance. Focusing on stacking, I provide code examples, visualisations, and practical tips to implement these methods. Learn how ensembling can enhance the robustness and accuracy of your machine learning models.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project11_images/pr11.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 6. Mastering Ensembling Techniques: Boosting Model Performance with Stacking and Voting.
          </h1>
          
            <p class="f5">This blog explores ensembling techniques that combine the strengths of multiple models to improve predictive performance. Focusing on stacking, I provide code examples, visualisations, and practical tips to implement these methods. Learn how ensembling can enhance the robustness and accuracy of your machine learning models.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project11_images/pr11.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Custom-CNNs-Histopathology-Classification" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>No single model is perfect, and each has its own strengths and weaknesses. Ensembling techniques address this by combining predictions from multiple models to create a stronger, more robust model. Whether you’re using bagging, boosting, stacking, or voting, ensembling is a powerful strategy to achieve higher accuracy and better generalization.</p>
<p>In this blog, we’ll focus on:</p>
<p>The fundamentals of stacking and soft voting.
Implementing stacking with a meta-model.
Using soft voting for combined predictions.
Evaluating ensemble models with metrics like ROC-AUC.
By the end, you’ll be able to implement and evaluate ensemble methods for your own machine learning projects.</p>
<p>Technical Explanation
Why Use Ensembling?
Ensembling reduces overfitting and variance by leveraging the strengths of multiple models. It improves generalization, making predictions more reliable, especially for complex datasets.</p>
<ol>
<li>Stacking
Stacking combines predictions from base models (e.g., neural networks, decision trees) into a feature matrix, which is then used as input for a meta-model. The meta-model learns how to combine these predictions optimally.</li>
</ol>
<p>Code for Stacking:
Step 1: Combine Base Model Predictions</p>
<p>import numpy as np</p>
<h1 id="assuming-predictions-from-base-models">Assuming predictions from base models</h1>
<p>ldam_predictions = np.random.rand(100)  # Example predictions from model 1
cw_predictions = np.random.rand(100)    # Example predictions from model 2
smote_predictions = np.random.rand(100) # Example predictions from model 3
custom_loss_predictions = np.random.rand(100) # Example predictions from model 4</p>
<h1 id="combine-predictions-into-a-feature-matrix">Combine predictions into a feature matrix</h1>
<p>ensemble_features = np.column_stack((ldam_predictions, cw_predictions, smote_predictions, custom_loss_predictions))
Step 2: Train-Test Split for Ensemble Features</p>
<p>from sklearn.model_selection import train_test_split</p>
<h1 id="assuming-val_labels-are-the-true-labels">Assuming val_labels are the true labels</h1>
<p>ensemble_features_train, ensemble_features_test, val_labels_train, val_labels_test = train_test_split(
ensemble_features, val_labels, test_size=0.2, random_state=42
)
Step 3: Train a Meta-Model</p>
<p>from sklearn.linear_model import LogisticRegression</p>
<h1 id="initialize-and-train-the-meta-model">Initialize and train the meta-model</h1>
<p>meta_model = LogisticRegression()
meta_model.fit(ensemble_features_train, val_labels_train)</p>
<p>Step 4: Evaluate the Meta-Model</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_auc_score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict probabilities for ROC-AUC calculation</span>
</span></span><span style="display:flex;"><span>meta_probabilities <span style="color:#f92672">=</span> meta_model<span style="color:#f92672">.</span>predict_proba(ensemble_features_test)[:, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>roc_auc <span style="color:#f92672">=</span> roc_auc_score(val_labels_test, meta_probabilities)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;ROC AUC Score:&#34;</span>, roc_auc)
</span></span></code></pre></div><figure><img src="/images/project11_images/pr11_roc.png">
</figure>

<p>Accuracy: 94.41% Precision: 95.10% Recall: 97% F1 Score: 96.04% It appears that the meta-model has performed exceptionally well, suggesting that the stacking ensemble approach effectively combined the strengths of your base models to achieve high performance across all key metrics. This is a strong result, especially in fields requiring high sensitivity and precision, such as medical image analysis or other critical applications.</p>
<p>The high recall (97%) is particularly noteworthy, as it indicates that the meta-model is very effective at identifying the positive class, which could be crucial for applications like disease detection where missing a positive case could have serious consequences. The balance between precision and recall, reflected in the high F1 score (96.04%), suggests that your meta-model manages to maintain a low rate of false positives while still correctly identifying most of the true positives, which is often a challenging balance to achieve.</p>
<p>These results validate the efficacy of using a stacking ensemble method in scenarios where you have multiple predictive models, each with its own approach to handling class imbalances or other dataset-specific challenges. It demonstrates the power of combining these models to leverage their individual strengths and mitigate their weaknesses.</p>
<p>Real-World Applications
Medical Diagnostics:</p>
<p>Ensemble models can combine predictions from CNNs trained on different features of medical images, improving diagnostic accuracy.
Fraud Detection:</p>
<p>Stacking meta-models can combine predictions from various algorithms (e.g., decision trees, SVMs) to identify fraudulent transactions more effectively.
Customer Segmentation:</p>
<p>Soft voting ensembles improve segmentation by leveraging multiple clustering or classification algorithms.
Conclusion
Key Takeaways:</p>
<p>Ensembling techniques like stacking and voting improve model performance by leveraging the strengths of multiple models.
Stacking combines predictions with a meta-model, while voting averages predictions for a consensus.
Evaluation metrics like ROC-AUC provide insights into the ensemble&rsquo;s effectiveness.
Ensembling is a powerful addition to your machine learning toolkit. Experiment with these techniques to improve your models&rsquo; robustness and performance!</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and stay healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


