<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>AI and Air Quality: Predicting Pollution for a Cleaner Future. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="This project leverages machine learning to predict PM10 pollution levels, tackling challenges like missing data, feature engineering, and model selection. Through a structured pipeline—from data pre-processing and exploratory analysis to advanced regression and ensemble models—it identifies key pollution drivers and enhances forecasting accuracy. The final phase transforms model predictions into actionable insights, supporting data-driven environmental policy and urban air quality improvements.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project8_images/pr8.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            AI and Air Quality: Predicting Pollution for a Cleaner Future.
          </h1>
          
            <p class="f5">This project leverages machine learning to predict PM10 pollution levels, tackling challenges like missing data, feature engineering, and model selection. Through a structured pipeline—from data pre-processing and exploratory analysis to advanced regression and ensemble models—it identifies key pollution drivers and enhances forecasting accuracy. The final phase transforms model predictions into actionable insights, supporting data-driven environmental policy and urban air quality improvements.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project8_images/pr8.jpg"
    alt="Photo by Dom J on Pexels"><figcaption>
      <p>Photo by Dom J on Pexels</p>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h1 id="part-1-cleaning-the-air-data-pre-processing-for-pm10-prediction">Part 1. Cleaning the Air: Data Pre-processing for PM10 Prediction</h1>
<p>Have you ever stopped to think about the data behind environmental predictions? We hear a lot about air pollution and its devastating effects on our health, but what’s often overlooked is the behind-the-scenes work required to make accurate predictions. The first step in any data-driven environmental project is cleaning the data—and let me tell you, it’s not as simple as it sounds.</p>
<p>For those of us who work with environmental datasets, we know that real-world data is never perfect. It’s often messy, inconsistent, and incomplete. This is especially true when working with air quality data, where we’re dealing with thousands of readings, irregular patterns, missing values, and numerous variables that impact pollution levels.</p>
<p>In this part, I’ll walk you through the challenges I faced while cleaning environmental data for a PM10 prediction project. I’ll demonstrate how effective pre-processing laid the foundation for accurate predictions of air quality levels and ultimately, how machine learning (ML) models could be used to tackle air pollution.</p>
<p>So, let’s dive in and clean the air—starting with cleaning the data.</p>
<h3 id="whats-the-problem-with-environmental-data">What’s the Problem with Environmental Data?</h3>
<p>When I first started working with environmental data, I was amased by how much information was available. Hourly measurements, pollution concentrations, temperature readings, wind speed, and more—data coming from different sources, like air quality monitoring stations, weather reports, and traffic records. But as exciting as this data was, it was also messy. And not just a little bit messy—extremely messy.</p>
<h4 id="missing-data">Missing Data</h4>
<p>One of the biggest challenges I faced was dealing with missing values. Imagine trying to predict the pollution level for a city based on incomplete data—missing temperature readings, unrecorded pollutant levels, or even entire days without data. In some cases, I could find gaps of several hours or days in the data. These gaps needed to be handled with care to avoid distorting the predictions.</p>
<p>So, how do we deal with missing data? The approach I took was a combination of:</p>
<ul>
<li><strong>Interpolation</strong>: Estimating the missing values based on surrounding data points.</li>
<li><strong>Exclusion</strong>: In cases where gaps were too large or could distort the overall trends, I excluded that data.</li>
</ul>
<p>While it’s not perfect, it’s a compromise that ensures the model remains accurate enough to make useful predictions.</p>
<h4 id="outliers">Outliers</h4>
<p>Outliers are another problem in environmental datasets.</p>
<p>An outlier in air quality data could be a sudden spike in pollution levels due to a sensor malfunction, or it could represent a real pollution event like a nearby fire or industrial accident. The challenge is figuring out which is which. In some cases, I used statistical methods, like Interquartile Range (IQR), to detect and remove outliers that were too extreme to be real.  But I also made judgment calls. Some spikes might be significant enough to keep in the dataset, while others were obvious sensor errors that needed to be discarded.</p>
<h4 id="irregularities-in-the-data">Irregularities in the Data</h4>
<p>Environmental data is also inconsistent.</p>
<p>Different air quality stations report data at different times, or even use different methods to record measurements. This means that some of the data might not align correctly, making it difficult to perform meaningful analysis.</p>
<p>For example, one station might measure PM10 levels every 15 minutes, while another station might do so every hour. To handle this, I had to standardise the time intervals and make sure the data was aligned across different stations.</p>
<h3 id="steps-in-data-pre-processing-for-pm10-prediction">Steps in Data Pre-processing for PM10 Prediction</h3>
<h4 id="data-import-and-inspection">Data Import and Inspection</h4>
<p>The first step in cleaning the data was importing the various datasets, which were in multiple formats. I used <code>Pandas</code> to load data from <code>CSVs</code>, <code>Excel</code> files, and other formats into <code>DataFrames</code> for easier manipulation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load data into a DataFrame</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_excel(<span style="color:#e6db74">&#39;PM10_final_updated_with_lags.xlsx&#39;</span>)
</span></span></code></pre></div><p>Once the data was loaded, I inspected it to understand its structure. I used commands like <code>.head()</code>, <code>.info()</code>, and <code>.describe()</code> to get a glimpse of the first few rows, check the column data types, and get summary statistics.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Inspecting the first few rows</span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>head()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Checking for missing values and datatypes</span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>info()
</span></span></code></pre></div><h4 id="handling-missing-values">Handling Missing Values</h4>
<p>Next, I tackled the missing values. Some were easy to handle with interpolation, while others required filling with a placeholder value or removing entire rows.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Filling missing values using forward-fill method</span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>fillna(method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ffill&#39;</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Or interpolate using linear interpolation for numerical columns</span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>interpolate(method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;linear&#39;</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><h4 id="outlier-detection">Outlier Detection</h4>
<p>For outliers, I used the <strong>IQR method</strong> to identify and remove extreme values.</p>
<p>The IQR is a measure of statistical dispersion, and outliers can be defined as any values outside the range of [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR] (where Q1 and Q3 are the first and third quartiles).</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculating the IQR for detecting outliers</span>
</span></span><span style="display:flex;"><span>Q1 <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>quantile(<span style="color:#ae81ff">0.25</span>)
</span></span><span style="display:flex;"><span>Q3 <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>quantile(<span style="color:#ae81ff">0.75</span>)
</span></span><span style="display:flex;"><span>IQR <span style="color:#f92672">=</span> Q3 <span style="color:#f92672">-</span> Q1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Identifying outliers</span>
</span></span><span style="display:flex;"><span>outliers <span style="color:#f92672">=</span> (data[<span style="color:#e6db74">&#39;PM10&#39;</span>] <span style="color:#f92672">&lt;</span> (Q1 <span style="color:#f92672">-</span> <span style="color:#ae81ff">1.5</span> <span style="color:#f92672">*</span> IQR)) <span style="color:#f92672">|</span> (data[<span style="color:#e6db74">&#39;PM10&#39;</span>] <span style="color:#f92672">&gt;</span> (Q3 <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.5</span> <span style="color:#f92672">*</span> IQR))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Removing outliers</span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> data[<span style="color:#f92672">~</span>outliers]
</span></span></code></pre></div><h4 id="feature-engineering-and-transformation">Feature Engineering and Transformation</h4>
<p>In this step, I created new features that would improve the model’s ability to predict pollution levels. For example, lagged variables were created to account for the fact that pollution from one hour might affect the next. This transformation is crucial for time-series data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Creating lagged features for PM10 levels</span>
</span></span><span style="display:flex;"><span>data[<span style="color:#e6db74">&#39;PM10_lag_1&#39;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>shift(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>data[<span style="color:#e6db74">&#39;PM10_lag_2&#39;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>shift(<span style="color:#ae81ff">2</span>)
</span></span></code></pre></div><p>I also scaled the data, since many  ML models work better when numerical features are normalised to a similar scale. For this, I used <code>StandardScaler</code> from <code>scikit-learn</code>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scaling numerical columns</span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>data[[<span style="color:#e6db74">&#39;PM10&#39;</span>, <span style="color:#e6db74">&#39;Temperature&#39;</span>, <span style="color:#e6db74">&#39;WindSpeed&#39;</span>]] <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(data[[<span style="color:#e6db74">&#39;PM10&#39;</span>, <span style="color:#e6db74">&#39;Temperature&#39;</span>, <span style="color:#e6db74">&#39;WindSpeed&#39;</span>]])
</span></span></code></pre></div><h3 id="why-data-preprocessing-matters">Why Data Preprocessing Matters</h3>
<p>Effective data pre-processing is a critical step in any ML project, but it’s particularly important when dealing with environmental data. If you don&rsquo;t clean your data, the models you build might fail to capture important patterns, or worse, they might produce inaccurate predictions that could mislead decision-makers.</p>
<p>In our case, cleaning the air (the data) was essential to making accurate predictions about pollution levels. By dealing with missing data, outliers, and inconsistencies, I ensured that the models would receive high-quality data, which ultimately led to better predictions and more actionable insights.</p>
<h3 id="summary-data-is-the-foundation-for-clean-air">Summary: Data Is the Foundation for Clean Air</h3>
<p>As we’ve seen, cleaning data isn’t a glamorous task, but it’s one of the most important steps in any ML project.</p>
<p>By properly handling messy environmental data, we can build robust models that predict PM10 levels with greater accuracy, providing decision-makers with the insights they need to improve air quality and public health.</p>
<p>So, next time you breathe in a breath of fresh air, remember—it’s not just the air you’re breathing, but the data behind it that helps us make it cleaner.</p>
<h1 id="part-2-exploring-the-data-understanding-pm10-and-its-impact-through-eda">Part 2. Exploring the Data: Understanding PM10 and Its Impact Through EDA</h1>
<p>Behind every successful ML project is a stage that is equal parts science and art: <strong>Exploratory Data Analysis (EDA)</strong>. This step is where we uncover the hidden stories in the data, identify patterns, and gain insights that inform the model-building process.</p>
<p>When working with air pollution data, EDA plays a vital role in answering key questions:</p>
<ul>
<li><em>What are the main factors influencing PM10 levels?</em></li>
<li><em>Are there seasonal or daily trends in air pollution?</em></li>
<li><em>How do weather and traffic impact PM10 concentrations?</em></li>
</ul>
<p>In this part, I’ll take you through the EDA process for the PM10 prediction project. We’ll explore the patterns and correlations in the data, visualise trends, and prepare our dataset for ML models.</p>
<h4 id="why-pm10-understanding-the-choice-of-focus">Why PM10? Understanding the Choice of Focus</h4>
<p>In air quality studies, both PM10 (particles ≤10 micrometers) and PM2.5 (particles ≤2.5 micrometers) are widely analysed. Each has unique health implications and sources. In this project, I focused on PM10 for several reasons, as shown in the descriptive statistics below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load your dataset containing PM10 and PM2.5 data</span>
</span></span><span style="display:flex;"><span>file_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;/Users/.../PM2.5_PM10_merged.xlsx&#39;</span>
</span></span><span style="display:flex;"><span>merged_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_excel(file_path)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Descriptive Statistics</span>
</span></span><span style="display:flex;"><span>desc_stats <span style="color:#f92672">=</span> merged_df[[<span style="color:#e6db74">&#39;PM2.5&#39;</span>, <span style="color:#e6db74">&#39;PM10&#39;</span>]]<span style="color:#f92672">.</span>describe()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Adding Standard Deviation to the table</span>
</span></span><span style="display:flex;"><span>std_dev <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;PM2.5&#39;</span>: [merged_df[<span style="color:#e6db74">&#39;PM2.5&#39;</span>]<span style="color:#f92672">.</span>std()],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;PM10&#39;</span>: [merged_df[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>std()]
</span></span><span style="display:flex;"><span>}, index<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;std_dev&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Concatenating the descriptive statistics and standard deviation</span>
</span></span><span style="display:flex;"><span>result_table <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat([desc_stats, std_dev])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Printing the result table</span>
</span></span><span style="display:flex;"><span>print(result_table)
</span></span></code></pre></div><p><strong>Key Insights</strong>:</p>
<ul>
<li>The average value of PM10 was higher than that of PM2.5, indicating that PM10 generally has higher concentrations in the dataset.</li>
<li>PM10 also exhibited a higher standard deviation, suggesting more variability in its concentrations compared to PM2.5.</li>
<li>The maximum PM10 value far exceeded that of PM2.5, showing that PM10 has a wider range of concentrations.</li>
<li>The 25th, 50th (median), and 75th percentiles of PM10 were consistently higher than those of PM2.5, confirming higher concentrations across the dataset.</li>
</ul>
<p><strong>Why PM10 Matters More in This Case</strong>:</p>
<ul>
<li>Given PM10&rsquo;s greater variability and concentration, it serves as a better target for ML models aiming to predict pollution spikes.</li>
<li>Additionally, PM10’s larger particles originate from diverse sources, including construction, vehicle emissions, and natural phenomena like dust storms, making it an important metric for urban air quality monitoring.</li>
</ul>
<p>By analySing PM10, therefore, I address a broader range of pollution sources, providing actionable insights for mitigating air quality issues in urban areas.</p>
<h3 id="1-starting-with-the-basics">1. Starting with the Basics</h3>
<p>The first step in EDA is getting a sense of the data. After cleaning the dataset in the pre-processing phase, I began by summarising the key statistics and visualising the distributions of variables.</p>
<h4 id="summary-statistics">Summary Statistics</h4>
<p>Using <code>Pandas</code>, I calculated the mean, median, standard deviation, and other basic metrics for PM10 and other relevant features like temperature, wind speed, and traffic volume.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Display summary statistics</span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>describe()
</span></span></code></pre></div><p>This revealed a lot about the data:</p>
<ul>
<li>PM10 levels varied significantly, with occasional spikes that hinted at outliers or pollution events.</li>
<li>Weather variables like temperature and wind speed showed consistent ranges, confirming the reliability of the sensors.</li>
</ul>
<h3 id="2-visualising-pm10-levels">2. Visualising PM10 Levels</h3>
<ul>
<li><em>Histogram of PM10 Levels</em></li>
</ul>
<p>To understand the distribution of PM10 levels, I plotted a histogram. This helped identify whether the data was skewed or normally distributed.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting histogram of PM10 levels</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hist(data[<span style="color:#e6db74">&#39;PM10&#39;</span>], bins<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, edgecolor<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Distribution of PM10 Levels&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;PM10&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Frequency&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>The histogram revealed a right-skewed distribution, meaning that while most pollution levels were moderate, there were occasional high pollution events. These spikes required further investigation to determine their causes.</p>
<p><em>Time-Series Plot</em></p>
<p>Next, I plotted PM10 levels over time to identify any trends or recurring patterns.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting PM10 over time</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(data[<span style="color:#e6db74">&#39;Timestamp&#39;</span>], data[<span style="color:#e6db74">&#39;PM10&#39;</span>])
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;PM10 Levels Over Time&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Time&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;PM10&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>This visualisation highlighted some clear trends:</p>
<ul>
<li><em>Seasonal variations</em>: PM10 levels tended to rise during the winter months, likely due to heating systems and stagnant air.</li>
<li><em>Daily fluctuations</em>: There were spikes in the morning and evening, coinciding with rush hour traffic.</li>
</ul>
<h3 id="3-correlation-analysis-the-key-to-pm10-insights">3. Correlation Analysis: The Key to PM10 Insights</h3>
<p>A major highlight of the EDA process was the correlation heatmap, which provided a comprehensive look at how PM10 is related to other variables. The heatmap below shows the correlations among pollutants, weather variables, and PM10 levels.</p>
<figure><img src="/images/heatmap.png">
</figure>

<h4 id="interpreting-the-heatmap">Interpreting the Heatmap</h4>
<p><strong>Strong Correlations with PM10</strong>:</p>
<ul>
<li>
<p>Sulfur dioxide emissions (SO2)strongly correlate with PM10, likely due to shared sources like industrial activities.</p>
</li>
<li>
<p>Traffic-Related Pollutants (NO2, CO): Nitrogen dioxide and carbon monoxide showed moderate positive correlations, reflecting their role in traffic-related emissions.</p>
</li>
</ul>
<p><strong>Negative Correlations</strong>:</p>
<ul>
<li><em>Wind Speed</em>: As expected, wind speed negatively correlates with PM10. High winds disperse pollutants, lowering concentrations.</li>
<li><em>Seasonality</em>: Certain gases like methane (CH4) showed variability that indirectly affected PM10 patterns.</li>
</ul>
<p><strong>Multicollinearity</strong>:</p>
<ul>
<li>Some variables, like NH3 and N2O, are highly correlated with each other, suggesting they may represent similar sources or processes.</li>
</ul>
<p>The heatmap also helped identify which variables might be redundant or less informative, guiding the feature selection process for modelling.</p>
<h3 id="4-uncovering-patterns-and-trends">4. Uncovering Patterns and Trends</h3>
<h4 id="daily-and-seasonal-trends">Daily and Seasonal Trends</h4>
<p>To dive deeper into how PM10 levels varied over time, I broke the data down into daily and monthly averages.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Grouping data by month and day</span>
</span></span><span style="display:flex;"><span>data[<span style="color:#e6db74">&#39;Month&#39;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;Timestamp&#39;</span>]<span style="color:#f92672">.</span>dt<span style="color:#f92672">.</span>month
</span></span><span style="display:flex;"><span>data[<span style="color:#e6db74">&#39;DayOfWeek&#39;</span>] <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;Timestamp&#39;</span>]<span style="color:#f92672">.</span>dt<span style="color:#f92672">.</span>dayofweek
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Calculate monthly and daily averages</span>
</span></span><span style="display:flex;"><span>monthly_avg <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>groupby(<span style="color:#e6db74">&#39;Month&#39;</span>)[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>daily_avg <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>groupby(<span style="color:#e6db74">&#39;DayOfWeek&#39;</span>)[<span style="color:#e6db74">&#39;PM10&#39;</span>]<span style="color:#f92672">.</span>mean()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plotting</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(monthly_avg, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Monthly Average PM10 Levels&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Month&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;PM10&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>This revealed two important trends:</p>
<ul>
<li><em>Higher pollution in winter months</em>: Likely due to heating emissions and stable atmospheric conditions that trap pollutants near the ground.</li>
<li><em>Weekly patterns</em>: PM10 levels were higher on weekdays compared to weekends, reflecting traffic-related emissions.</li>
</ul>
<h4 id="scatter-plots-for-key-relationships">Scatter Plots for Key Relationships</h4>
<p>Scatter plots helped visualise relationships between PM10 and other variables.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scatter plot of PM10 vs Traffic Volume</span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(data[<span style="color:#e6db74">&#39;TrafficVolume&#39;</span>], data[<span style="color:#e6db74">&#39;PM10&#39;</span>], alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;PM10 vs Traffic Volume&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Traffic Volume&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;PM10&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>The scatter plot showed a clear upward trend, confirming that traffic is a major contributor to pollution levels.</p>
<h3 id="5-feature-selection-insights">5. Feature Selection Insights</h3>
<p>EDA isn’t just about understanding the data; it also informs which features to include in the model. From my analysis, the following features stood out as critical for predicting PM10 levels:</p>
<ul>
<li><em>Traffic Volume</em>: A strong direct correlation with PM10.</li>
<li><em>Temperature</em>: Indirectly affects pollution by influencing atmospheric conditions.</li>
<li><em>Wind Speed</em>: Disperses pollutants, reducing PM10 levels.</li>
<li><em>Time-Based Features</em>: Seasonal and daily trends are essential for capturing recurring patterns.</li>
</ul>
<h3 id="6-challenges-encountered-during-eda">6. Challenges Encountered During EDA</h3>
<p>While EDA is a powerful tool, it’s not without challenges:</p>
<ul>
<li><em>Handling High Variability</em>: Pollution levels can vary widely based on external factors like geography or sudden weather changes, making it difficult to generalise trends.</li>
<li><em>Balancing Signal and Noise</em>: Some patterns in the data may be statistical noise, not meaningful trends.</li>
<li><em>Data Gaps</em>: Despite cleaning efforts, some gaps remained, particularly for certain monitoring stations.</li>
</ul>
<h4 id="why-eda-matters">Why EDA Matters</h4>
<p>EDA isn’t just a box to tick off before modelling—it’s where you understand your data’s story.
For this PM10 prediction project, EDA uncovered the key drivers of air pollution, highlighted patterns worth modeling, and ensured the dataset was ready for machine learning algorithms.</p>
<p>By the end of the EDA phase, I had a clear roadmap for the next steps. With the insights gained, I could confidently move forward to build models that predict PM10 levels with accuracy and reliability.</p>
<h3 id="summary-from-data-to-insights">Summary: From Data to Insights</h3>
<p>EDA bridges the gap between raw data and actionable insights. For this project, it transformed thousands of rows of PM10 measurements into meaningful patterns, showing us how pollution levels change over time and what factors contribute most to poor air quality.</p>
<h1 id="part-3-regression-models-for-air-quality-prediction-from-simplicity-to-accuracy">Part 3. Regression Models for Air Quality Prediction: From Simplicity to Accuracy</h1>
<p>Predicting air pollution isn’t just about crunching numbers—it’s about finding patterns, building models, and learning how different variables interact with one another.</p>
<p>In this part, I take the first step toward accurate PM10 predictions by exploring regression models. These models form the backbone of many machine learning (ML) projects, providing interpretable results and insights into the relationships between variables.</p>
<p>Regression models are a great starting point for predicting PM10 levels because they are straightforward, efficient, and capable of capturing linear and moderately nonlinear relationships. In this part, I’ll dive into how regression models were used, discuss their performance, and highlight the insights they revealed about air quality patterns.</p>
<h3 id="1-the-case-for-regression-models">1. The Case for Regression Models</h3>
<p>Why start with regression? The answer lies in their simplicity and interpretability. Regression models:</p>
<ul>
<li><em>Identify Relationships</em>: They reveal which features (e.g., wind speed, temperature) most strongly affect PM10 levels.</li>
<li><em>Set Baseline Performance</em>: They establish a baseline to compare more complex models like neural networks.</li>
<li><em>Handle Complexity Well</em>: With techniques like regularisation, they manage multicollinearity and over-fitting effectively.</li>
</ul>
<h3 id="2-preparing-the-dataset">2. Preparing the Dataset</h3>
<p>Before diving into model building, the dataset underwent additional preparation:</p>
<ul>
<li><em>Feature Selection</em>: From EDA, we selected the most influential variables, such as traffic volume, temperature, wind speed, and time-based features (hour, day, and season).</li>
<li><em>Train-Test Split</em>: To evaluate the models, we split the data into 80% training and 20% testing sets.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define features and target</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> data[[<span style="color:#e6db74">&#39;TrafficVolume&#39;</span>, <span style="color:#e6db74">&#39;Temperature&#39;</span>, <span style="color:#e6db74">&#39;WindSpeed&#39;</span>, <span style="color:#e6db74">&#39;Hour&#39;</span>, <span style="color:#e6db74">&#39;Day&#39;</span>, <span style="color:#e6db74">&#39;Month&#39;</span>]]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split the dataset</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span></code></pre></div><h3 id="3-linear-regression-a-starting-point">3. Linear Regression: A Starting Point</h3>
<p>The first model we built was a linear regression model. This model assumes a linear relationship between the features and the target variable (PM10). While simplistic, it provides a clear picture of how each variable contributes to pollution levels.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_absolute_error, mean_squared_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train the model</span>
</span></span><span style="display:flex;"><span>lr_model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>lr_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict on test data</span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> lr_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Evaluate performance</span>
</span></span><span style="display:flex;"><span>mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, y_pred)
</span></span><span style="display:flex;"><span>mse <span style="color:#f92672">=</span> mean_squared_error(y_test, y_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Linear Regression - MAE: </span><span style="color:#e6db74">{</span>mae<span style="color:#e6db74">}</span><span style="color:#e6db74">, MSE: </span><span style="color:#e6db74">{</span>mse<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Linear Regression</strong>:</p>
<ul>
<li>Traffic volume emerged as the strongest predictor, with PM10 levels spiking during rush hours.</li>
<li>Wind speed had a negative coefficient, confirming its role in dispersing pollutants.</li>
</ul>
<p>While the model performed well for general trends, it struggled with extreme values, highlighting the need for more sophisticated methods.</p>
<h3 id="4-ridge-and-lasso-regression-tackling-multicollinearity">4. Ridge and Lasso Regression: Tackling Multicollinearity</h3>
<p>When dealing with real-world data, features are often correlated. This can lead to multicollinearity, where the model struggles to differentiate the effect of closely related variables. <strong>Ridge</strong> and <strong>Lasso</strong> regression address this issue by adding regularisation.</p>
<p><code>Ridge</code> penalises large coefficients, helping the model generaliSe better.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> Ridge
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Ridge regression</span>
</span></span><span style="display:flex;"><span>ridge_model <span style="color:#f92672">=</span> Ridge(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>ridge_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>ridge_pred <span style="color:#f92672">=</span> ridge_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>ridge_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, ridge_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Ridge Regression - MAE: </span><span style="color:#e6db74">{</span>ridge_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><code>Lasso</code> goes a step further by shrinking some coefficients to zero, effectively performing feature selection.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> Lasso
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Lasso regression</span>
</span></span><span style="display:flex;"><span>lasso_model <span style="color:#f92672">=</span> Lasso(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span><span style="display:flex;"><span>lasso_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>lasso_pred <span style="color:#f92672">=</span> lasso_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>lasso_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, lasso_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Lasso Regression - MAE: </span><span style="color:#e6db74">{</span>lasso_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Regularised Models</strong>:</p>
<ul>
<li>Both models reduced over-fitting, improving generalisation on the test data.</li>
<li><code>Lasso</code> identified traffic volume and hour of the day as the most influential features, while <code>Ridge</code> retained all features but reduced their impact.</li>
</ul>
<h3 id="5-decision-tree-regression-adding-nonlinearity">5. Decision Tree Regression: Adding Nonlinearity</h3>
<p>To capture more complex relationships, we implemented a <strong>Decision Tree</strong> regressor. Unlike linear models, <code>Decision Trees</code> split the data into regions and make predictions based on the average value in each region.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Decision Tree</span>
</span></span><span style="display:flex;"><span>dt_model <span style="color:#f92672">=</span> DecisionTreeRegressor(max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>dt_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>dt_pred <span style="color:#f92672">=</span> dt_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>dt_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, dt_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Decision Tree Regression - MAE: </span><span style="color:#e6db74">{</span>dt_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Decision Trees</strong>:</p>
<p>The model captured nonlinear patterns, such as sudden pollution spikes during low-wind conditions.
However, the tree’s performance depended heavily on its depth—too shallow, and it missed patterns; too deep, and it overfit the training data.</p>
<h3 id="6-model-comparison-and-evaluation">6. Model Comparison and Evaluation</h3>
<p>To compare the models, we used Mean Absolute Error (MAE) as the primary metric. Lower MAE indicates better performance.
<figure><img src="/images/eval.png">
</figure>
</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Linear regression provided a strong baseline but struggled with nonlinear patterns.</li>
<li>Ridge and Lasso improved generalisation by reducing over-fitting.</li>
<li>Decision trees excelled at capturing complex relationships but required careful tuning to avoid overfitting.</li>
</ul>
<h4 id="challenges-and-lessons-learned">Challenges and Lessons Learned</h4>
<p>Building regression models was not without its challenges:</p>
<ul>
<li><em>Feature Selection</em>: Including too many correlated features led to multicollinearity, which required regularisation techniques to resolve.</li>
<li>Nonlinear Patterns: Linear models couldn’t fully capture pollution spikes, motivating the use of decision trees and later more advanced models.</li>
<li>Over-fitting: Decision trees, while powerful, required hyperparameter tuning to strike a balance between performance and generalisation.</li>
</ul>
<h3 id="summary">Summary</h3>
<p>Regression models provided valuable insights into the factors driving PM10 levels and set the stage for more advanced machine learning approaches. From identifying the key contributors like traffic and weather to tackling challenges like multicollinearity, this phase laid the groundwork for accurate air quality predictions.</p>
<h1 id="part-4-advanced-machine-learning-for-pm10-prediction-random-forest-xgboost-and-more">Part 4. Advanced Machine Learning for PM10 Prediction: Random Forest, XGBoost, and More</h1>
<p>Regression models laid a solid foundation for PM10 prediction, but air pollution is a complex phenomenon influenced by nonlinear and time-dependent factors. To capture these intricacies, advanced machine learning models like neural networks (NNs) and ensemble methods come into play.</p>
<p>These models are capable of uncovering patterns and relationships that simpler models might overlook. In this part, I’ll explore how advanced methods such as <strong>Random Forest</strong>, <strong>Gradient Boosting</strong>, and <strong>Long Short-Term Memory (LSTM)</strong> networks were employed to predict PM10 levels with greater accuracy.</p>
<p>I’ll also discuss their strengths, limitations, and the unique insights they offered into the dynamics of air pollution.</p>
<h3 id="1-ensemble-methods-random-forest-and-gradient-boosting">1. Ensemble Methods: Random Forest and Gradient Boosting</h3>
<h4 id="random-forest">Random Forest</h4>
<p><code>Random Forest</code> is an ensemble method that builds multiple decision trees and averages their predictions. It reduces over-fitting and improves accuracy by leveraging the wisdom of the crowd.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_absolute_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Random Forest</span>
</span></span><span style="display:flex;"><span>rf_model <span style="color:#f92672">=</span> RandomForestRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>rf_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>rf_pred <span style="color:#f92672">=</span> rf_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>rf_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, rf_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Random Forest - MAE: </span><span style="color:#e6db74">{</span>rf_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h4 id="gradient-boosting">Gradient Boosting</h4>
<p><code>Gradient Boosting</code> builds trees sequentially, with each tree correcting the errors of the previous one. It excels at capturing subtle patterns in the data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> GradientBoostingRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Gradient Boosting</span>
</span></span><span style="display:flex;"><span>gb_model <span style="color:#f92672">=</span> GradientBoostingRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>gb_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>gb_pred <span style="color:#f92672">=</span> gb_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>gb_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, gb_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Gradient Boosting - MAE: </span><span style="color:#e6db74">{</span>gb_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Ensemble Models</strong>:</p>
<ul>
<li><code>Random Forest</code> provided robust predictions by averaging over many decision trees, making it less prone to overfitting.</li>
<li><code>Gradient Boosting</code> excelled at capturing complex patterns but required careful tuning of hyperparameters like learning rate and number of trees.</li>
<li>Both models outperformed simpler regression techniques, particularly in predicting pollution spikes.</li>
</ul>
<h3 id="2-neural-networks-a-deep-dive">2. Neural Networks: A Deep Dive</h3>
<h4 id="the-need-for-neural-networks">The Need for Neural Networks</h4>
<p>While ensemble methods are powerful, they struggle with time-series data, where patterns evolve over time. Enter NNs, particularly <code>Long Short-Term Memory (LSTM)</code> networks, which are designed to handle sequential data.</p>
<h4 id="implementing-lstm-for-pm10-prediction">Implementing LSTM for PM10 Prediction</h4>
<p>LSTM networks, a type of recurrent neural network (RNN), can &ldquo;remember&rdquo; patterns across long sequences, making them ideal for predicting hourly or daily PM10 levels.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reshape data for LSTM (samples, timesteps, features)</span>
</span></span><span style="display:flex;"><span>X_train_lstm <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape((X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>X_test_lstm <span style="color:#f92672">=</span> X_test<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape((X_test<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, X_test<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build the LSTM model</span>
</span></span><span style="display:flex;"><span>lstm_model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    LSTM(<span style="color:#ae81ff">50</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(X_train_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], X_train_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>])),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile and train the model</span>
</span></span><span style="display:flex;"><span>lstm_model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean_absolute_error&#39;</span>)
</span></span><span style="display:flex;"><span>lstm_model<span style="color:#f92672">.</span>fit(X_train_lstm, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>lstm_pred <span style="color:#f92672">=</span> lstm_model<span style="color:#f92672">.</span>predict(X_test_lstm)
</span></span><span style="display:flex;"><span>lstm_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, lstm_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;LSTM - MAE: </span><span style="color:#e6db74">{</span>lstm_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h4 id="comparing-model-performances">Comparing Model Performances</h4>
<p>To evaluate the effectiveness of the models, we compared their <code>Mean Absolute Error (MAE)</code>:
<figure><img src="/images/eval.png">
</figure>
</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li><code>Random Forest</code> and <code>Gradient Boosting</code>: Excellent at capturing feature interactions and nonlinear patterns.</li>
<li><code>LSTM</code>: Outperformed all other models by leveraging time-series data, capturing daily and seasonal trends effectively.</li>
</ul>
<h3 id="challenges-of-advanced-models">Challenges of Advanced Models</h3>
<p>While advanced models offer superior performance, they come with their own set of challenges:</p>
<ul>
<li><em>Computational Intensity</em>: Training <code>LSTM</code> networks required significant time and computational resources.</li>
<li><em>Hyperparameter Tuning</em>: Models like <code>Gradient Boosting</code> and <code>LSTM</code> are sensitive to hyperparameters, requiring extensive experimentation to optimize.</li>
<li><em>Interpretability</em>: Unlike regression models, NNs operate as black boxes, making it harder to explain their predictions.</li>
</ul>
<h3 id="lessons-learned">Lessons Learned</h3>
<p>Working with advanced models highlighted the importance of:</p>
<ul>
<li><em>Feature Engineering</em>: Creating time-based features (e.g., hour of the day) significantly improved model performance.</li>
<li><em>Model Stacking</em>: Combining the strengths of different models (e.g., Random Forest + LSTM) could further enhance predictions.</li>
<li><em>Domain Knowledg</em>*: Understanding the environmental factors affecting PM10 helped guide feature selection and model interpretation.</li>
</ul>
<h3 id="summary-1">Summary</h3>
<p>Advanced models like Random Forest, Gradient Boosting, and LSTM pushed the boundaries of what we could achieve in predicting PM10 levels. By leveraging these techniques, we not only improved accuracy but also gained deeper insights into the factors driving air pollution.</p>
<h1 id="part-5-evaluating-and-selecting-the-best-models-for-pm10-prediction">Part 5. Evaluating and Selecting the Best Models for PM10 Prediction</h1>
<p>After building and testing various ML models, the next critical step is evaluating their performance and selecting the best ones for deployment.</p>
<p>In this part, I’ll compare models using rigorous metrics like RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error). We’ll also explore hyperparameter tuning for neural networks, leveraging <strong>GridSearchCV</strong> for optimal performance.</p>
<h3 id="the-need-for-systematic-evaluation-and-evaluating-multiple-models">The Need for Systematic Evaluation and Evaluating Multiple Models</h3>
<p>With several models—Linear <strong>Regression, Random Forest, Gradient Boosting, XGBoost, Ridge, Lasso, and Neural Networks</strong> — it’s essential to evaluate them fairly. I used:</p>
<ul>
<li><em>Cross-validation</em>: To ensure models perform consistently across different data splits.</li>
<li><em>Scoring metrics</em>: RMSE for penalising large errors and MAE for measuring average error magnitude.</li>
</ul>
<p>I evaluated six models initially using cross-validation and computed <code>RMSE</code> and <code>MAE</code> for each:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> cross_val_score
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_squared_error, mean_absolute_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise the models</span>
</span></span><span style="display:flex;"><span>models <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Linear Regression&#34;</span>: LinearRegression(),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Random Forest&#34;</span>: RandomForestRegressor(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Gradient Boosting&#34;</span>: GradientBoostingRegressor(random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Ridge&#34;</span>: Ridge(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Lasso&#34;</span>: Lasso(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>),
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;XGBoost&#34;</span>: xgb<span style="color:#f92672">.</span>XGBRegressor(objective<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;reg:squarederror&#39;</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise results DataFrame</span>
</span></span><span style="display:flex;"><span>results_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;Model&#39;</span>, <span style="color:#e6db74">&#39;RMSE&#39;</span>, <span style="color:#e6db74">&#39;MAE&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform cross-validation and store results</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> model_name, model <span style="color:#f92672">in</span> models<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate cross-validated RMSE</span>
</span></span><span style="display:flex;"><span>    neg_mse_scores <span style="color:#f92672">=</span> cross_val_score(model, X, y, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;neg_mean_squared_error&#39;</span>)
</span></span><span style="display:flex;"><span>    rmse_scores <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>sqrt(<span style="color:#f92672">-</span>neg_mse_scores)
</span></span><span style="display:flex;"><span>    avg_rmse <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(rmse_scores)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Calculate cross-validated MAE</span>
</span></span><span style="display:flex;"><span>    mae_scores <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>cross_val_score(model, X, y, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;neg_mean_absolute_error&#39;</span>)
</span></span><span style="display:flex;"><span>    avg_mae <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(mae_scores)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Append results to the DataFrame</span>
</span></span><span style="display:flex;"><span>    results_df <span style="color:#f92672">=</span> results_df<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#39;Model&#39;</span>: model_name, <span style="color:#e6db74">&#39;RMSE&#39;</span>: avg_rmse, <span style="color:#e6db74">&#39;MAE&#39;</span>: avg_mae}, ignore_index<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Print the results table</span>
</span></span><span style="display:flex;"><span>print(results_df)
</span></span></code></pre></div><p><strong>Prelimenary Results</strong>
The cross-validation results revealed clear differences in model performance. While simpler models like <code>Linear Regression</code> were fast, they struggled to capture complex patterns.</p>
<p>Ensemble methods like <code>Random Forest</code> and <code>Gradient Boosting</code> performed better, and <code>XGBoost</code> emerged as a strong contender.</p>
<p><strong>Fine-Tuning NNs</strong>: I extended the evaluation to include a NN Regressor, focusing on optimising its architecture and hyperparameters.</p>
<p><strong>Hyperparameter Tuning with GridSearchCV</strong>: Using a grid search, I tested different configurations for hidden layers, activation functions, and learning rates.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neural_network <span style="color:#f92672">import</span> MLPRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define hyperparameter grid</span>
</span></span><span style="display:flex;"><span>param_grid <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;hidden_layer_sizes&#39;</span>: [(<span style="color:#ae81ff">100</span>,), (<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">50</span>)],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;activation&#39;</span>: [<span style="color:#e6db74">&#39;relu&#39;</span>, <span style="color:#e6db74">&#39;tanh&#39;</span>],
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;learning_rate_init&#39;</span>: [<span style="color:#ae81ff">0.001</span>, <span style="color:#ae81ff">0.01</span>],
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scale the features</span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>X_scaled <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform Grid Search</span>
</span></span><span style="display:flex;"><span>nn_model <span style="color:#f92672">=</span> MLPRegressor(max_iter<span style="color:#f92672">=</span><span style="color:#ae81ff">2000</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>grid_search <span style="color:#f92672">=</span> GridSearchCV(nn_model, param_grid, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;neg_mean_squared_error&#39;</span>)
</span></span><span style="display:flex;"><span>grid_search<span style="color:#f92672">.</span>fit(X_scaled, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Extract the best model</span>
</span></span><span style="display:flex;"><span>best_nn_model <span style="color:#f92672">=</span> grid_search<span style="color:#f92672">.</span>best_estimator_
</span></span></code></pre></div><p><strong>NN Results:</strong> The best NN configuration achieved significant improvements, particularly for RMSE, but required more computational resources and careful scaling.</p>
<h3 id="model-comparison">Model Comparison</h3>
<figure><img src="/images/eval3.png">
</figure>

<p><strong>Key Takeaways</strong>:</p>
<ul>
<li><em>XGBoost and Neural Networks</em>: Consistently outperformed other models, capturing both linear and nonlinear patterns effectively.</li>
<li><em>Ensemble Methods</em>: <code>Random Forest</code> and <code>Gradient Boosting</code> offered a balance of accuracy and interpretability.</li>
<li>8Linear Models*: Useful for insights but struggled with complex relationships.</li>
</ul>
<h4 id="lessons-learned-1">Lessons Learned</h4>
<ul>
<li><em>Importance of Cross-Validation</em>: Ensures the models generalise well and avoid overfitting.</li>
<li><em>Scalability of NNs</em>: Requires careful tuning and pre-processing but offers unmatched accuracy for complex datasets.</li>
<li><em>XGBoost’s Efficiency</em>: Emerged as a strong contender for both accuracy and speed, making it ideal for large-scale deployments.</li>
</ul>
<h3 id="summary-2">Summary</h3>
<p>After evaluating multiple models, NNs and XGBoost emerged as the top performers for PM10 prediction. While NNs offered the highest accuracy, XGBoost provided a competitive alternative with faster training times and interpretability.</p>
<h1 id="part-6-from-data-to-action-what-pollution-data-and-ai-teach-us-about-cleaner-air">Part 6. From Data to Action: What Pollution Data and AI Teach Us About Cleaner Air</h1>
<p>Predicting PM10 and other pollutants is not just about building models or visualising data—it&rsquo;s about understanding the invisible threats in the air we breathe and finding ways to address them.
After exploring data pre-processing, modelling, and evaluation, this final piece reflects on the insights gained and their real-world implications.</p>
<p>I’ll discuss how the results from this project—built on advanced AI/ML techniques—can guide better decision-making for policymakers, businesses, and citizens alike. By connecting technical outputs to actionable insights, we close the loop on how AI can make our cities healthier and our air cleaner.</p>
<h3 id="key-insights-from-data">Key Insights from Data</h3>
<ol>
<li><strong>Air Quality Trends Over Time</strong></li>
</ol>
<p>One of the most valuable outputs of this project was uncovering how PM10 levels fluctuated over time:</p>
<ul>
<li><em>Daily Trends</em>: Pollution spiked during rush hours, particularly in urban areas near highways.</li>
<li><em>Seasonal Variations</em>: Winter months consistently showed higher pollution levels, likely due to heating systems and stagnant air conditions.</li>
<li><em>Industrial Impact</em>: Specific pollutants (e.g., SO2) correlated strongly with PM10, highlighting industrial contributions to air pollution.</li>
</ul>
<ol start="2">
<li><strong>Correlations Between Pollutants</strong></li>
</ol>
<p>Through correlation analysis and feature engineering, I discovered:</p>
<ul>
<li><em>Traffic as a Major Contributor</em>: Traffic-related pollutants like NO2 and CO had strong positive correlations with PM10 levels.</li>
<li><em>Role of Weather</em>: Wind speed showed a significant negative correlation, demonstrating its dispersive effect on airborne pollutants.</li>
<li><em>Lag Effects</em>: Time-lagged variables for temperature and humidity provided additional predictive power, revealing delayed environmental impacts.</li>
</ul>
<p>These insights not only improved model accuracy but also provided tangible evidence for targeted interventions.</p>
<h3 id="how-ai-adds-value">How AI Adds Value</h3>
<ol>
<li><strong>Predictive Modelling for Early Warnings</strong>: Advanced models like XGBoost and Neural Networks offered high-accuracy predictions of PM10 levels, enabling proactive measures:</li>
</ol>
<ul>
<li><em>Alerts for High-Risk Periods</em>: Predictions can warn vulnerable populations (e.g., children, elderly, asthmatics) to avoid outdoor exposure during high-pollution hours.</li>
<li><em>Real-Time Monitoring</em>: When integrated with IoT devices and sensors, these models can deliver continuous air quality updates.</li>
</ul>
<ol start="2">
<li><strong>Hotspot Identification</strong>: By analysing spatial data and model outputs, we identified pollution hotspots—areas with consistently high PM10 levels. These insights can guide:</li>
</ol>
<ul>
<li><em>Urban Planning</em>: Planting trees or creating green zones in high-pollution areas.</li>
<li><em>Traffic Management</em>: Implementing congestion pricing or re-routing traffic during peak pollution hours.</li>
<li><em>Industrial Regulations</em>: Strengthening emission controls in industrial zones.
<strong>Actionable Insights for Policy</strong></li>
</ul>
<p>AI models provided more than just numbers—they offered insights for crafting evidence-based policies:</p>
<ul>
<li>Traffic reductions during peak hours could cut PM10 spikes by up to 30%.</li>
<li>Seasonal pollution mitigation strategies (e.g., subsidised public heating alternatives) could reduce winter PM10 levels.</li>
<li>Targeted public awareness campaigns could encourage behavioral changes like carpooling or using public transport.</li>
</ul>
<h3 id="challenges-highlighted-by-the-project">Challenges Highlighted by the Project</h3>
<ol>
<li><strong>Data Quality</strong></li>
</ol>
<ul>
<li><em>Missing Data</em>: Even with sophisticated interpolation methods, gaps in data affected predictions, particularly during critical pollution events.</li>
<li><em>Inconsistencies Across Monitoring Stations</em>: Variations in sensor quality and reporting frequency complicated data integration.</li>
</ul>
<ol start="2">
<li><strong>Model Limitations</strong></li>
</ol>
<ul>
<li><em>Interpretability</em>: While models like XGBoost excelled in accuracy, they lacked transparency compared to simpler models like Linear Regression.</li>
<li><em>Generalisability</em>: Predictions were most accurate for recurring patterns (e.g., daily traffic cycles) but struggled with outlier events like wildfires or sudden industrial discharges.</li>
</ul>
<ol start="3">
<li><strong>Stakeholder Engagement</strong></li>
</ol>
<p>Bridging the gap between technical outputs and actionable policies required clear communication. Explaining AI predictions to non-technical stakeholders (e.g., policymakers, community leaders) remained a challenge.</p>
<h3 id="real-world-applications">Real-World Applications</h3>
<ol>
<li><strong>Empowering Policymakers</strong>: Policymakers can use AI-driven insights to prioritise interventions:</li>
</ol>
<ul>
<li><em>Long-Term Plans</em>: Develop urban green spaces in high-risk zones.</li>
<li><em>Short-Term Measures</em>: Issue temporary traffic restrictions during high-pollution days.</li>
</ul>
<ol start="2">
<li><strong>Protecting Public Health</strong>:</li>
</ol>
<ul>
<li><em>Personalised Alerts</em>: Apps could notify individuals of poor air quality and suggest indoor activities on bad air days.</li>
<li><em>Health Cost Reductions</em>: Early warnings and targeted interventions could reduce hospitalisations related to asthma and cardiovascular diseases.</li>
</ul>
<ol start="3">
<li><strong>Shaping Sustainable Cities</strong>: The integration of AI models with smart city frameworks could</li>
</ol>
<ul>
<li>Optimise traffic flow to minimise emissions.</li>
<li>Inform renewable energy policies by correlating pollution patterns with energy usage.</li>
</ul>
<h3 id="lessons-learned-and-future-directions">Lessons Learned and Future Directions</h3>
<p><strong>Lessons Learned</strong>:</p>
<ul>
<li><em>Data Is the Foundation</em>: Clean, consistent, and high-quality data is non-negotiable for effective AI models.</li>
<li><em>Interdisciplinary Collaboration Matters</em>: Environmental scientists, data scientists, and policymakers must work together to turn predictions into action.</li>
<li><em>AI Needs Human Oversight</em>: While AI models are powerful, human judgment remains essential for interpreting outputs and deciding on interventions.</li>
</ul>
<p><strong>Future Directions</strong>:</p>
<ul>
<li><em>Integrating IoT and AI</em>: Combining AI models with real-time sensor networks for dynamic, adaptive monitoring.</li>
<li><em>Expanding Metrics</em>: Incorporating additional pollutants (e.g., PM2.5, NO2) for a more comprehensive analysis.</li>
<li><em>Scaling Globally</em>: Applying similar methodologies to other cities or regions facing air quality challenges.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>This project demonstrated how AI and ML can bridge the gap between data and decision-making. From identifying pollution hotspots to predicting high-risk periods, the insights generated have far-reaching implications for public health, urban planning, and environmental policy.</p>
<p>But technology alone isn’t the answer.</p>
<p>Real change requires collaboration between scientists, governments, businesses, and individuals. AI gives us the tools to understand and predict pollution, but it’s up to us to act on these insights.
Every breath matters. Let’s make each one cleaner.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and let&rsquo;s keep our planet healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


