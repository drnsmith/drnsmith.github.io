<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Part 1. Building an AI-Powered Recipe Difficulty Classifier: A Journey Through NLP and ML. | Natasha Smith Portfolio</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="In the first blog, I dive into the development of a recipe difficulty classifier. I share the initial steps of the project, including how I approached data pre-processing and natural language processing to prepare recipe data for machine learning models."><meta name=generator content="Hugo 0.142.0"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=stylesheet href=/css/custom.css></head><body class="ma0 avenir bg-near-white"><nav class="pa3 pa4-ns flex justify-end items-center"><ul class="list flex ma0 pa0"><li class=ml3><a class="link dim dark-gray f5" href=/>Home</a></li><li class=ml3><a class="link dim dark-gray f5" href=/about/>About</a></li><li class=ml3><a class="link dim dark-gray f5" href=/projects/>Projects</a></li><li class=ml3><a class="link dim dark-gray f5" href=/contact/>Contact</a></li></ul></nav><header class=page-header style=background-image:url(/images/project1_images/pr1.jpg);background-size:cover;background-position:50%;height:400px;display:flex;align-items:center;justify-content:center;color:#fff;text-align:center><div style=background-color:rgba(0,0,0,.4);padding:1rem;border-radius:4px><h1 class="f1 athelas mt3 mb1">Part 1. Building an AI-Powered Recipe Difficulty Classifier: A Journey Through NLP and ML.</h1><p class=f5>In the first blog, I dive into the development of a recipe difficulty classifier. I share the initial steps of the project, including how I approached data pre-processing and natural language processing to prepare recipe data for machine learning models.</p></div></header><main class=pb7 role=main><article class="mw8 center ph3"><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src=/images/project1_images/pr1.jpg></figure><div style=display:flex;align-items:center;gap:10px><a href=https://github.com/drnsmith/AI-Recipe-Classifier target=_blank style=text-decoration:none><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle>
</a><a href=https://github.com/drnsmith/AI-Recipe-Classifier target=_blank style=font-weight:700;color:#000>View Project on GitHub</a></div><h2 id=introduction><strong>Introduction</strong></h2><p>Cooking varies in complexity. Some recipes are straightforward, while others demand precision, technique, and skill. The challenge was to develop a ML model that classifies recipes into four difficulty levels—<strong>Easy, Medium, Hard, and Very Hard</strong>—using <strong>Natural Language Processing (NLP)</strong> and <strong>Machine Learning (ML)</strong>. In this post, I focus on <strong>data collection, cleaning, and pre-processing</strong>, which lay the foundation for training a robust ML model.</p><h2 id=why-pre-process-recipe-data><strong>Why Pre-process Recipe Data?</strong></h2><p>Raw textual data in recipes is often noisy, containing <strong>special characters, punctuation, HTML tags, and non-standard formatting</strong>. If left untreated, these irregularities can reduce the performance of NLP models. To ensure high-quality inputs for machine learning, I applied a <strong>multi-step text cleaning and transformation process</strong>:</p><ol><li><strong>Remove non-ASCII characters</strong> to eliminate unwanted symbols.</li><li><strong>Convert text to lowercase</strong> for uniformity.</li><li><strong>Remove non-contextual words</strong>, including newlines and HTML tags.</li><li><strong>Remove numbers</strong> as they don’t contribute to textual understanding.</li><li><strong>Remove punctuation</strong> to standardise input format.</li><li><strong>Apply lemmatization and stemming</strong> to normalise words.</li><li><strong>Remove stopwords</strong> to retain only meaningful content.</li></ol><h2 id=1-loading-and-cleaning-data><strong>1. Loading and Cleaning Data</strong></h2><p>First, I loaded the dataset into a <code>Pandas DataFrame</code> and defined various text-cleaning functions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Import necessary libraries</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> nltk
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.tokenize <span style=color:#f92672>import</span> word_tokenize
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.stem <span style=color:#f92672>import</span> WordNetLemmatizer, PorterStemmer
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> re
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> string
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load dataset into a DataFrame</span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;/path/to/recipes_data.csv&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialise NLTK resources</span>
</span></span><span style=display:flex><span>nltk<span style=color:#f92672>.</span>download(<span style=color:#e6db74>&#39;punkt&#39;</span>)
</span></span><span style=display:flex><span>nltk<span style=color:#f92672>.</span>download(<span style=color:#e6db74>&#39;stopwords&#39;</span>)
</span></span><span style=display:flex><span>nltk<span style=color:#f92672>.</span>download(<span style=color:#e6db74>&#39;wordnet&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Define cleaning utilities</span>
</span></span><span style=display:flex><span>lemmatizer <span style=color:#f92672>=</span> WordNetLemmatizer()
</span></span><span style=display:flex><span>stop_words <span style=color:#f92672>=</span> set(stopwords<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>))
</span></span></code></pre></div><h3 id=explanation><strong>Explanation:</strong></h3><ul><li>I used <strong>Pandas</strong> to load the recipe dataset.</li><li><strong>NLTK</strong> was used for tokenisation, stopword removal, and lemmatisation.</li><li>The stopwords list was initialised to filter out non-essential words (e.g., &ldquo;the&rdquo;, &ldquo;and&rdquo;, &ldquo;is&rdquo;).</li></ul><h2 id=2-text-cleaning-functions><strong>2. Text Cleaning Functions</strong></h2><p>To ensure consistency, I created several text-cleaning functions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Function to remove non-ASCII characters</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remove_non_ascii</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;[^\x00-\x7F]&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to convert text to lowercase</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>convert_to_lowercase</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>lower()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to remove unnecessary symbols, HTML tags, and extra spaces</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remove_noncontext_words</span>(text):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> text<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>)<span style=color:#f92672>.</span>replace(<span style=color:#e6db74>&#39;&amp;nbsp&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\[.*?\]&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\s+&#39;</span>, <span style=color:#e6db74>&#39; &#39;</span>, text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;http\S+|www\.\S+&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>strip()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to remove numbers</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remove_numbers</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> re<span style=color:#f92672>.</span>sub(<span style=color:#e6db74>r</span><span style=color:#e6db74>&#39;\d+&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to remove punctuation</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>remove_punctuation</span>(text):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> text<span style=color:#f92672>.</span>translate(str<span style=color:#f92672>.</span>maketrans(<span style=color:#e6db74>&#39;&#39;</span>, <span style=color:#e6db74>&#39;&#39;</span>, string<span style=color:#f92672>.</span>punctuation))
</span></span></code></pre></div><h3 id=explanation-1><strong>Explanation:</strong></h3><ul><li>These functions <strong>clean raw text</strong>, removing unwanted characters that could interfere with NLP processing.</li><li>URLs and unnecessary symbols are stripped out.</li><li>The text is <strong>lowercased</strong> to ensure uniform processing.</li></ul><h2 id=3-text-normalisation-with-nlp><strong>3. Text Normalisation with NLP</strong></h2><p>Lemmatization and stemming help normalise words by reducing them to their base forms.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Function to lemmatise text</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>lemmatize_text</span>(text):
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> word_tokenize(text)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join([lemmatizer<span style=color:#f92672>.</span>lemmatize(token) <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokens])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Function to stem words</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>stem_text</span>(text):
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> word_tokenize(text)
</span></span><span style=display:flex><span>    stemmer <span style=color:#f92672>=</span> PorterStemmer()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join([stemmer<span style=color:#f92672>.</span>stem(token) <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokens])
</span></span></code></pre></div><h3 id=explanation-2><strong>Explanation:</strong></h3><ul><li><strong>Lemmatization</strong> reduces words to their dictionary form (e.g., &ldquo;running&rdquo; → &ldquo;run&rdquo;).</li><li><strong>Stemming</strong> removes suffixes to simplify words (e.g., &ldquo;cooking&rdquo; → &ldquo;cook&rdquo;).</li></ul><h2 id=4-applying-pre-processing-to-recipe-data><strong>4. Applying Pre-processing to Recipe Data</strong></h2><p>I applied all cleaning steps to the dataset, ensuring that recipe data was properly structured before feeding it into the ML model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Comprehensive text pre-processing function</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preprocess_text</span>(text):
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> str(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> remove_non_ascii(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> convert_to_lowercase(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> remove_noncontext_words(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> remove_numbers(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> remove_punctuation(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> lemmatize_text(text)
</span></span><span style=display:flex><span>    text <span style=color:#f92672>=</span> stem_text(text)
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> word_tokenize(text)
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> [lemmatizer<span style=color:#f92672>.</span>lemmatize(token) <span style=color:#66d9ef>for</span> token <span style=color:#f92672>in</span> tokens <span style=color:#66d9ef>if</span> token <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> stop_words]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(set(tokens))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Apply pre-processing to relevant columns</span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;preprocessed_ingredients&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;ingredients&#39;</span>]<span style=color:#f92672>.</span>apply(preprocess_text)
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;preprocessed_directions&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;directions&#39;</span>]<span style=color:#f92672>.</span>apply(preprocess_text)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Combine columns for full recipe representation</span>
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;preprocessed_full_recipe&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;preprocessed_ingredients&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39; &#39;</span> <span style=color:#f92672>+</span> df[<span style=color:#e6db74>&#39;preprocessed_directions&#39;</span>]
</span></span></code></pre></div><h3 id=explanation-3><strong>Explanation:</strong></h3><ul><li>Each recipe’s <strong>ingredients and directions</strong> were pre-processed separately.</li><li>A <strong>combined column</strong> (<code>preprocessed_full_recipe</code>) was created to represent the entire recipe.</li></ul><h2 id=conclusion><strong>Conclusion</strong></h2><p>Data pre-processing is a <strong>crucial first step</strong> in any NLP project. By cleaning and structuring text, I ensured the <strong>ML model receives high-quality inputs</strong> for training.</p><p><strong>Key Takeaways:</strong></p><ul><li>Cleaning text data removes noise and enhances NLP model performance.</li><li>Lemmatisation, stemming, and stopword removal improve text standardisation.</li><li>Pre-processed text is <strong>structured, compact, and informative</strong> for ML.</li></ul><p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and happy cooking!</em></p></div></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Natasha Smith Portfolio 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>