<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 5. Interpreting the AI Recipe Classifier with LIME: Making ML Transparent. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Next, I generate and analyse LIME visualisations to understand the model’s decision-making process. By examining highlighted words and phrases in recipes, I uncover the reasoning behind specific predictions and assess the model&#39;s biases. This step brings transparency to the classifier, revealing how AI interprets recipe complexity.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project1_images/pr1.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 5. Interpreting the AI Recipe Classifier with LIME: Making ML Transparent.
          </h1>
          
            <p class="f5">Next, I generate and analyse LIME visualisations to understand the model’s decision-making process. By examining highlighted words and phrases in recipes, I uncover the reasoning behind specific predictions and assess the model&#39;s biases. This step brings transparency to the classifier, revealing how AI interprets recipe complexity.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project1_images/pr1.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>In building a recipe difficulty classifier, I wanted to make sure the model&rsquo;s predictions weren’t just accurate but also understandable. For anyone working with ML, especially in fields where transparency is key, model interpretability is crucial. This is where LIME (Local Interpretable Model-Agnostic Explanations) comes in.
In this blog post, I’ll walk you through how I used LIME to make sense of my classifier’s decisions, ensuring that its predictions are grounded and explainable.</p>
<h3 id="why-model-interpretability-matters">Why Model Interpretability Matters</h3>
<p>ML models, particularly neural networks, are often referred to as &ldquo;black boxes.&rdquo; They can make accurate predictions, but understanding why they make those predictions can be difficult. This lack of transparency can be problematic, especially when models are used in real-world applications where trust and accountability are essential. For my recipe classifier, understanding the model’s reasoning process was essential. I wanted to know:</p>
<ul>
<li><em>Which ingredients or techniques contribute to the model classifying a recipe as &ldquo;Easy&rdquo; or &ldquo;Hard&rdquo;?</em></li>
<li><em>How does the model weigh different aspects of a recipe to arrive at a difficulty level?</em></li>
</ul>
<p>To answer these questions, I turned to LIME.</p>
<h3 id="what-is-lime">What is LIME?</h3>
<p>LIME stands for Local Interpretable Model-Agnostic Explanations. It’s a tool designed to explain the predictions of any ML model by creating an interpretable approximation of the model’s behaviour in the local region around a specific prediction.</p>
<p>LIME doesn’t explain the entire model. Instead, it explains individual predictions by perturbing input data and observing how the model’s output changes. By focusing on small regions around a prediction, LIME can help us understand what factors most influence that specific prediction.</p>
<p>In this project, LIME was ideal because it allowed me to interpret each individual prediction made by my recipe classifier, without needing to delve into the inner workings of the model itself.</p>
<h3 id="how-i-used-lime-in-the-recipe-classifier-project">How I Used LIME in the Recipe Classifier Project</h3>
<p>I chose a range of recipes across different difficulty levels (Easy, Medium, Hard, Very Hard) to see if the model’s predictions were consistent and explainable. I used the LIME library in <code>Python</code> to generate explanations for individual predictions. LIME works by creating slightly modified versions of a data point (in this case, a recipe) and observing how these changes impact the model’s output. For each recipe, LIME provided insights into the features (ingredients, techniques, etc.) that contributed to the model’s prediction. This allowed me to see which elements of a recipe were driving its difficulty classification.</p>
<p><img src="/images/3.png" alt="Training and Validation Loss and Accuracy"></p>
<p>My expectation was this. For a recipe classified as &ldquo;Hard,&rdquo; LIME might highlight features like &ldquo;multiple steps&rdquo; or &ldquo;specialised techniques&rdquo; as important contributors to the prediction. For an &ldquo;Easy&rdquo; recipe, it might show that &ldquo;basic ingredients&rdquo; and &ldquo;few steps&rdquo; were key factors. This way, LIME helped me verify that the model was focusing on the right aspects of the recipes when making its predictions.</p>
<p>To use LIME, I first needed to install the LIME package in Python:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Importing the necessary packages</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> lime.lime_text <span style="color:#f92672">import</span> LimeTextExplainer
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Creating an instance of LimeTextExplainer</span>
</span></span><span style="display:flex;"><span>explainer <span style="color:#f92672">=</span> LimeTextExplainer(class_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;Easy&#39;</span>, <span style="color:#e6db74">&#39;Medium&#39;</span>, <span style="color:#e6db74">&#39;Hard&#39;</span>, <span style="color:#e6db74">&#39;Very Hard&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Selecting a recipe to explain</span>
</span></span><span style="display:flex;"><span>recipe_text <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;This recipe involves multiple steps including sautéing, baking, and requires specific equipment.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Generating an explanation for the prediction</span>
</span></span><span style="display:flex;"><span>explanation <span style="color:#f92672">=</span> explainer<span style="color:#f92672">.</span>explain_instance(recipe_text, model<span style="color:#f92672">.</span>predict_proba, num_features<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Displaying the explanation</span>
</span></span><span style="display:flex;"><span>explanation<span style="color:#f92672">.</span>show_in_notebook()
</span></span></code></pre></div><ul>
<li><em>LimeTextExplainer</em>: Since my classifier takes recipe descriptions as input, I used LimeTextExplainer, which is designed for text data.</li>
<li><em>explain_instance</em>: This function generates an explanation for a single instance (in this case, a recipe) by examining how slight modifications to the input affect the prediction.</li>
</ul>
<h3 id="observations-and-interpretations">Observations and Interpretations</h3>
<ul>
<li><strong>Prediction Confidence</strong>:</li>
</ul>
<p>The model assigned a high confidence score of 0.80 for class 0 (which represent a difficulty level &ldquo;Easy&rdquo;). Lower confidence scores are observed for the other classes, with the next highest probability at 0.20, indicating that the model is fairly certain about this classification.</p>
<ul>
<li><em>Word Contribution</em>:</li>
</ul>
<p>LIME highlights specific words within the recipe text that significantly influenced the model’s prediction. Words such as &ldquo;mixture,&rdquo; &ldquo;crumb,&rdquo; &ldquo;side,&rdquo; &ldquo;ingredient,&rdquo; and &ldquo;tomato&rdquo; are highlighted, suggesting they contributed notably to the classification decision.</p>
<ul>
<li><em>Importance of Ingredients and Terminology</em>:</li>
</ul>
<p>The highlighted words indicate that certain ingredients and cooking-related terminology play an essential role in the model’s decision-making process. For instance, terms like &ldquo;mixture&rdquo; and &ldquo;crumb&rdquo; could be associated with easier preparation techniques, influencing the model towards a lower difficulty classification.</p>
<ul>
<li><em>Possible Model Bias or Heuristics</em>:</li>
</ul>
<p>The words selected by LIME might suggest that the model has learned certain heuristics, linking specific ingredients or preparation methods to particular difficulty levels.</p>
<p>If &ldquo;tomato&rdquo; and &ldquo;crumb&rdquo; consistently appear in easier recipes, the model may have learned to associate these words with simpler classifications. This can sometimes reveal biases in the dataset, where certain words are overrepresented in specific difficulty categories.</p>
<ul>
<li><em>Interpretability and Transparency</em>:</li>
</ul>
<p>The use of LIME here provides transparency by breaking down the &ldquo;black box&rdquo; of the model, showing users which elements of the recipe text had the most influence on the predicted difficulty. This insight allows to evaluate if the model’s reasoning aligns with human intuition and if adjustments are needed to improve fairness or reduce bias in the predictions. By using LIME, I better understood which parts of the recipe text the model relies on, providing a clear path for refining the classifier or further tailoring it to match real-world perceptions of recipe difficulty.</p>
<h3 id="the-value-of-using-lime">The Value of Using LIME</h3>
<p>LIME proved invaluable in my project for several reasons:</p>
<ul>
<li>Trust: By understanding the model’s reasoning, I could trust its predictions more fully.</li>
<li>Debugging: LIME helped me spot any potential issues where the model might be focusing on irrelevant details.</li>
<li>User-Friendly Explanations: For anyone looking to use this model in a real-world application, LIME explanations provide a way to communicate model behavior clearly and effectively.</li>
</ul>
<h3 id="limitations-and-next-steps">Limitations and Next Steps</h3>
<p>While LIME was incredibly helpful, it does have limitations:</p>
<ul>
<li>Local Interpretations Only: LIME only explains individual predictions rather than providing a global view of the model.</li>
<li>Approximation Errors: Since LIME creates a simplified model to approximate the main model’s behavior, there can be minor errors in interpretation.</li>
</ul>
<p>In future iterations of this project, it would be beneficial to explore other interpretability methods, such as SHAP (SHapley Additive exPlanations), which offers a more holistic view of feature importance across all predictions.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Interpreting ML models is essential, especially in fields where transparency and accountability matter. By using LIME, I was able to open up the &ldquo;black box&rdquo; of my recipe difficulty classifier, ensuring that its predictions were not only accurate but also explainable. For anyone looking to build or use ML models responsibly, tools like LIME offer a powerful way to understand and trust the predictions that models make. If you&rsquo;re building your own classifiers or predictive models, I highly recommend experimenting with LIME. It’s a valuable tool in making machine learning not just effective, but also transparent and reliable.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and happy cooking!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


