<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 6. Deploying an AI Model for Recipe Classification: Bringing the Classifier to Life. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Finally, I take the recipe difficulty classifier from development to deployment, making it accessible and functional in real-world applications. In this blog, you&#39;ll learn the steps I followed to prepare the model for production, including setting up a deployment environment, creating an API, and ensuring the classifier&#39;s responsiveness and reliability for end-users.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project1_images/pr1.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 6. Deploying an AI Model for Recipe Classification: Bringing the Classifier to Life.
          </h1>
          
            <p class="f5">Finally, I take the recipe difficulty classifier from development to deployment, making it accessible and functional in real-world applications. In this blog, you&#39;ll learn the steps I followed to prepare the model for production, including setting up a deployment environment, creating an API, and ensuring the classifier&#39;s responsiveness and reliability for end-users.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project1_images/pr1.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/AI-Recipe-Classifier" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Once a model that classifies recipes by difficulty level is built and trained , the next challenge is deploying it into a real-world environment. In this blog, we’ll cover the process of moving our trained model from a development setting to a production environment. Deployment enables the model to make predictions and serve users in real-time, opening up possibilities for applications like recipe recommendation engines, cooking assistant apps, or culinary content platforms.</p>
<h3 id="1-preparing-the-model-for-deployment">1. Preparing the Model for Deployment</h3>
<p>Before deploying, it&rsquo;s essential to package the model in a way that allows it to operate independently of the training environment. This preparation includes:</p>
<ul>
<li><em>Saving the Model</em>: Using a format like .h5 (for neural networks in TensorFlow/Keras) or .pkl (for scikit-learn models) allows us to save the model’s parameters and architecture.</li>
<li><em>Version Control</em>: Tracking different versions of the model helps in managing updates and improvements over time, especially when experimenting with new features or hyperparameters.</li>
</ul>
<p>I chose to save the model using the format compatible with my framework (<code>TensorFlow</code> for neural networks or a <code>joblib pickle</code> for Random Forest) and documented the version with metadata, including the date, model type, and main hyperparameters.</p>
<h3 id="2-choosing-a-deployment-platform">2. Choosing a Deployment Platform</h3>
<p>Several platforms allow to serve ML models as an API, each with its own benefits:</p>
<ul>
<li><strong>Cloud Platforms</strong>: Services like AWS SageMaker, Google Cloud AI Platform, and Microsoft Azure provide scalable, managed environments for deploying ML models.</li>
<li><strong>Containerisation with Docker</strong>: Docker allows to create a lightweight, portable container that includes our model and its dependencies.</li>
</ul>
<p>This approach works well for deployments on any cloud provider or on-premises servers.</p>
<ul>
<li><strong>Serverless Options</strong>: Using serverless frameworks like AWS Lambda can reduce costs, especially if the model is only used intermittently.</li>
</ul>
<p>For this project, I decided to deploy using Docker for easy scalability and flexibility, with the potential to transition to a cloud platform as usage grows.</p>
<h3 id="1-building-a-rest-api-for-the-model">1. Building a REST API for the Model</h3>
<p>To allow applications to interact with our model, I set up a REST API. This interface allows to send recipe data to the model and receive predictions on the recipe difficulty.</p>
<ul>
<li><em>Framework</em>: I used Flask, a lightweight Python web framework, to create the API. Flask enables to set up endpoints to receive requests, process data, and return predictions.</li>
<li><em>API Endpoints</em>: I set up the following key endpoints:
<ul>
<li>POST /predict: Takes recipe data (ingredients, cooking steps) and returns the predicted difficulty level.</li>
<li>GET /health: A simple endpoint to check if the model is running correctly.</li>
</ul>
</li>
</ul>
<p>A sample of our Flask code to handle incoming requests:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> flask <span style="color:#f92672">import</span> Flask, request, jsonify
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pickle
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>app <span style="color:#f92672">=</span> Flask(__name__)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#### Load the pre-trained model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;recipe_model.pkl&#39;</span>, <span style="color:#e6db74">&#39;rb&#39;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load(f)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.route</span>(<span style="color:#e6db74">&#39;/predict&#39;</span>, methods<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;POST&#39;</span>])
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">predict</span>():
</span></span><span style="display:flex;"><span>    data <span style="color:#f92672">=</span> request<span style="color:#f92672">.</span>json
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Process data for prediction (e.g., feature extraction)</span>
</span></span><span style="display:flex;"><span>    processed_data <span style="color:#f92672">=</span> preprocess(data)  <span style="color:#75715e"># Custom function</span>
</span></span><span style="display:flex;"><span>    prediction <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict([processed_data])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> jsonify({<span style="color:#e6db74">&#39;difficulty&#39;</span>: prediction[<span style="color:#ae81ff">0</span>]})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@app.route</span>(<span style="color:#e6db74">&#39;/health&#39;</span>, methods<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;GET&#39;</span>])
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">health</span>():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> jsonify({<span style="color:#e6db74">&#39;status&#39;</span>: <span style="color:#e6db74">&#39;healthy&#39;</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    app<span style="color:#f92672">.</span>run()
</span></span></code></pre></div><h3 id="4-testing-and-validating-the-api">4. Testing and Validating the API</h3>
<p>Before releasing the API, I ensured it functions correctly under various conditions. Testing includes:</p>
<ul>
<li><em>Unit Testing</em>: Testing individual components, such as the pre-processing function and prediction generation.</li>
<li><em>Integration Testing</em>: Checking the entire flow, from data submission to receiving a prediction, to ensure everything works in unison.</li>
<li><em>Load Testing</em>: Simulating multiple requests to measure the system&rsquo;s capacity and response time, which is especially important for high-traffic applications.</li>
</ul>
<p>I used Postman for API testing, sending test requests to the /predict endpoint with sample data and confirming the model returned correct predictions.</p>
<h3 id="5-deploying-the-api-with-docker">5. Deploying the API with Docker</h3>
<p>To ensure our model API is portable and scalable, I containerised it with Docker. Docker enables to package the application with all necessary dependencies, making it easier to deploy across different environments. Here’s the Dockerfile that can be used to containerise the Flask API:</p>
<pre tabindex="0"><code>FROM python:3.8-slim

# Set the working directory
WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Copy the rest of the application code
COPY . .

# Expose the port Flask will run on
EXPOSE 5000

# Run the Flask app
CMD [&#34;python&#34;, &#34;app.py&#34;]
</code></pre><p>Once the Docker image is built, it can be deployed on any server or cloud service that supports Docker.</p>
<h3 id="6-monitoring-and-maintaining-the-model">6. Monitoring and Maintaining the Model</h3>
<p>After deployment, it&rsquo;s crucial to monitor the model’s performance and update it as needed. Monitoring helps detecting issues early, such as model drift or performance degradation.</p>
<ul>
<li><strong>Logging</strong>: Log incoming requests, predictions, and any errors that occur. This data helps in diagnosing issues and optimising the model over time.</li>
<li><strong>Metrics</strong>: Track metrics like latency, error rates, and prediction accuracy over time.</li>
<li><strong>Scheduled Retraining</strong>: If model performance decreases, consider retraining with new data to adapt to changing recipe trends or ingredients.</li>
</ul>
<p>I set up basic logging and monitoring, and in the future, automated retraining  - to ensure the model remains effective - can be integrated.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Deploying an AI model is an essential step in bringing ML solutions to end-users. For my AI-powered recipe difficulty classifier, I built a REST API with Flask, containerised it using Docker, and tested it thoroughly to ensure reliability. By monitoring and maintaining the model, my aim was to provide a seamless experience for users seeking recipe insights.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and happy cooking!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://drnsmith.github.io/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


