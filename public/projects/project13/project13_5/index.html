<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Part 5. Why AI Calibration is Critical for Reliable Breast Cancer Diagnosis. | Natasha Smith Portfolio</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Explore model calibration techniques like Platt Scaling and Isotonic Regression to ensure accurate and reliable AI predictions in healthcare."><meta name=generator content="Hugo 0.142.0"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=stylesheet href=/css/custom.css></head><body class="ma0 avenir bg-near-white"><nav class="pa3 pa4-ns flex justify-end items-center"><ul class="list flex ma0 pa0"><li class=ml3><a class="link dim dark-gray f5" href=/>Home</a></li><li class=ml3><a class="link dim dark-gray f5" href=/about/>About</a></li><li class=ml3><a class="link dim dark-gray f5" href=/projects/>Projects</a></li><li class=ml3><a class="link dim dark-gray f5" href=/contact/>Contact</a></li></ul></nav><header class=page-header style=background-image:url(/images/project13_images/pr13.jpg);background-size:cover;background-position:50%;height:400px;display:flex;align-items:center;justify-content:center;color:#fff;text-align:center><div style=background-color:rgba(0,0,0,.4);padding:1rem;border-radius:4px><h1 class="f1 athelas mt3 mb1">Part 5. Why AI Calibration is Critical for Reliable Breast Cancer Diagnosis.</h1><p class=f5>Explore model calibration techniques like Platt Scaling and Isotonic Regression to ensure accurate and reliable AI predictions in healthcare.</p></div></header><main class=pb7 role=main><article class="mw8 center ph3"><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src=/images/project13_images/pr13.jpg><figcaption><h4>Photo by Ben Hershey on Unsplash</h4></figcaption></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith//Histopathology-AI-BreastCancer target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><h3 id=why-ai-calibration-is-critical-for-reliable-breast-cancer-diagnosis><strong>Why AI Calibration is Critical for Reliable Breast Cancer Diagnosis</strong></h3><p>AI-powered tools are revolutionizing healthcare by providing fast, accurate, and scalable diagnostic solutions. In breast cancer diagnosis, deep learning models, particularly Convolutional Neural Networks (CNNs), have shown remarkable promise. However, a highly accurate model is not necessarily a reliable one. This is where <strong>AI calibration</strong> plays a critical role—ensuring that a model’s predicted probabilities align closely with the actual likelihood of events, making predictions more interpretable and trustworthy.</p><p>In this blog, we explore the importance of model calibration in healthcare and delve into techniques like <strong>Platt Scaling</strong> and <strong>Isotonic Regression</strong> to improve the reliability of AI predictions in breast cancer diagnostics.</p><hr><h3 id=what-is-ai-calibration><strong>What is AI Calibration?</strong></h3><p>AI calibration refers to the process of adjusting a model’s predicted probabilities to better reflect real-world likelihoods. For example:</p><ul><li>A perfectly calibrated model predicts a 90% chance of malignancy, and in 90 out of 100 such cases, the outcome is indeed malignant.</li></ul><p>Without proper calibration:</p><ul><li><strong>Overconfidence</strong>: The model predicts probabilities that are too high, overestimating risk.</li><li><strong>Underconfidence</strong>: The model predicts probabilities that are too low, underestimating risk.</li></ul><p>Both scenarios are problematic in healthcare, where decisions often hinge on probability thresholds.</p><hr><h3 id=the-importance-of-calibration-in-breast-cancer-diagnosis><strong>The Importance of Calibration in Breast Cancer Diagnosis</strong></h3><p>In breast cancer diagnostics, calibration ensures:</p><ol><li><strong>Trustworthy Predictions</strong>: Clinicians can rely on the model’s outputs for critical decisions.</li><li><strong>Threshold Sensitivity</strong>: Calibrated probabilities allow more meaningful threshold adjustments for screening and treatment.</li><li><strong>Fairness</strong>: Calibrated models reduce bias, particularly in underrepresented or challenging cases.</li></ol><hr><h3 id=common-calibration-techniques><strong>Common Calibration Techniques</strong></h3><h4 id=1-platt-scaling><strong>1. Platt Scaling</strong></h4><p>Platt Scaling is a post-hoc calibration method that fits a logistic regression model to the outputs of an uncalibrated classifier.</p><p><strong>How It Works</strong>:</p><ol><li>Train the CNN model to output uncalibrated probabilities (e.g., softmax probabilities).</li><li>Fit a logistic regression model using these probabilities and the true labels from a validation set.</li></ol><p><strong>Implementation</strong>:
Using Scikit-learn:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.linear_model <span style=color:#f92672>import</span> LogisticRegression
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.calibration <span style=color:#f92672>import</span> calibration_curve
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Uncalibrated model predictions</span>
</span></span><span style=display:flex><span>y_proba <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(x_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Fit Platt Scaling (logistic regression) for calibration</span>
</span></span><span style=display:flex><span>platt_scaler <span style=color:#f92672>=</span> LogisticRegression()
</span></span><span style=display:flex><span>platt_scaler<span style=color:#f92672>.</span>fit(y_proba, y_val)
</span></span><span style=display:flex><span>y_proba_calibrated <span style=color:#f92672>=</span> platt_scaler<span style=color:#f92672>.</span>predict_proba(y_proba)[:, <span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><p><strong>Advantages</strong>:</p><ul><li>Simple and effective for binary classification problems.</li><li>Works well when the model’s predicted probabilities are roughly sigmoid-shaped.</li></ul><hr><h4 id=2-isotonic-regression><strong>2. Isotonic Regression</strong></h4><p>Isotonic Regression is a non-parametric calibration technique that maps predicted probabilities to true probabilities using a piecewise constant function.</p><p><strong>How It Works</strong>:</p><ol><li>Train the CNN model to output uncalibrated probabilities.</li><li>Fit an isotonic regression model using these probabilities and the true labels.</li></ol><p><strong>Implementation</strong>:
Using Scikit-learn:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.isotonic <span style=color:#f92672>import</span> IsotonicRegression
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Fit Isotonic Regression for calibration</span>
</span></span><span style=display:flex><span>iso_reg <span style=color:#f92672>=</span> IsotonicRegression(out_of_bounds<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;clip&#39;</span>)
</span></span><span style=display:flex><span>y_proba_calibrated <span style=color:#f92672>=</span> iso_reg<span style=color:#f92672>.</span>fit_transform(y_proba, y_val)
</span></span></code></pre></div><p><strong>Advantages</strong>:</p><ul><li>Does not assume a specific form for the relationship between predicted and true probabilities.</li><li>More flexible than Platt Scaling, particularly for datasets with complex probability distributions.</li></ul><hr><h3 id=evaluating-calibration><strong>Evaluating Calibration</strong></h3><p>To assess model calibration, the following tools and metrics are commonly used:</p><ol><li><p><strong>Reliability Diagram</strong>:</p><ul><li>A graphical representation comparing predicted probabilities to observed frequencies.</li><li>A perfectly calibrated model aligns with the diagonal line.</li></ul></li><li><p><strong>Expected Calibration Error (ECE)</strong>:</p><ul><li>Measures the difference between predicted and observed probabilities across probability bins.</li></ul></li></ol><p><strong>Implementation</strong>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.calibration <span style=color:#f92672>import</span> calibration_curve
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Reliability diagram</span>
</span></span><span style=display:flex><span>prob_true, prob_pred <span style=color:#f92672>=</span> calibration_curve(y_val, y_proba, n_bins<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(prob_pred, prob_true, marker<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;o&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Uncalibrated Model&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>], linestyle<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;--&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Perfect Calibration&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Mean Predicted Probability&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Fraction of Positives&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Reliability Diagram&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h3 id=case-study-calibrating-a-breast-cancer-detection-model><strong>Case Study: Calibrating a Breast Cancer Detection Model</strong></h3><p><strong>Dataset</strong>: BreakHis (breast cancer histopathology dataset)</p><ol><li><p><strong>Baseline Model</strong>:</p><ul><li>An uncalibrated CNN achieved high accuracy (96%) but overestimated probabilities for malignant cases, reducing trustworthiness.</li></ul></li><li><p><strong>Calibration with Platt Scaling</strong>:</p><ul><li>Improved probability alignment for malignant cases.</li><li>Reliability diagram showed closer adherence to the diagonal line.</li></ul></li><li><p><strong>Calibration with Isotonic Regression</strong>:</p><ul><li>Further enhanced calibration for rare benign cases.</li><li>Achieved better Expected Calibration Error (ECE) than Platt Scaling.</li></ul></li></ol><p><strong>Results</strong>:</p><table><thead><tr><th><strong>Metric</strong></th><th><strong>Uncalibrated</strong></th><th><strong>Platt Scaling</strong></th><th><strong>Isotonic Regression</strong></th></tr></thead><tbody><tr><td>Accuracy</td><td>96%</td><td>96%</td><td>96%</td></tr><tr><td>Expected Calibration Error (ECE)</td><td>0.15</td><td>0.08</td><td>0.05</td></tr><tr><td>Reliability Diagram Slope</td><td>0.75</td><td>0.95</td><td>0.98</td></tr></tbody></table><hr><h3 id=best-practices-for-calibration><strong>Best Practices for Calibration</strong></h3><ol><li><p><strong>Choose the Right Technique</strong>:</p><ul><li>Use Platt Scaling for simpler problems.</li><li>Opt for Isotonic Regression for more complex datasets.</li></ul></li><li><p><strong>Calibrate on Validation Data</strong>:</p><ul><li>Always reserve a separate validation set for calibration to prevent overfitting.</li></ul></li><li><p><strong>Evaluate with Multiple Metrics</strong>:</p><ul><li>Use both reliability diagrams and numerical metrics like ECE for comprehensive evaluation.</li></ul></li></ol><hr><h3 id=conclusion><strong>Conclusion</strong></h3><p>AI calibration is essential for reliable breast cancer diagnosis, ensuring that predicted probabilities are meaningful and trustworthy. Techniques like Platt Scaling and Isotonic Regression provide practical ways to achieve better calibration, improving the interpretability and safety of AI systems in healthcare. By integrating calibration into model development pipelines, we can build more reliable diagnostic tools that clinicians can trust.</p></div></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Natasha Smith Portfolio 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>