<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 10. Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Dive into methods for improving model interpretability by statistically analysing features extracted from deep learning models.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project13_images/pr13.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 10. Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data.
          </h1>
          
            <p class="f5">Dive into methods for improving model interpretability by statistically analysing features extracted from deep learning models.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project13_images/pr13.jpg"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction"><strong>Introduction</strong></h3>
<p>Deep learning (DL) models, particularly Convolutional Neural Networks (CNNs), are powerful tools for analysing medical imaging data. However, their &ldquo;black-box&rdquo; nature often limits their utility in sensitive applications like breast cancer diagnostics, where interpretability is paramount. By combining CNN feature extraction with statistical analysis, we can enhance model interpretability, revealing meaningful patterns and offering deeper insights into the data.</p>
<p>In this blog, we’ll explore methods for improving CNN interpretability using statistical analysis of features extracted from breast cancer imaging datasets.</p>
<hr>
<p>Based on the dissertation, here’s the tailored write-up for the topic:</p>
<hr>
<h3 id="enhancing-interpretability-in-cnns-statistical-insights-from-breast-cancer-data"><strong>Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data</strong></h3>
<p>Deep learning models, particularly Convolutional Neural Networks (CNNs), have transformed breast cancer diagnostics by enabling automated analysis of histopathological images. However, their &ldquo;black-box&rdquo; nature often limits clinical applicability, as healthcare professionals demand not just accurate predictions but also explainable outcomes. By combining feature extraction from CNNs with statistical analysis, we can uncover patterns in the data and improve interpretability, offering deeper insights into model decisions.</p>
<hr>
<h3 id="the-need-for-interpretability-in-clinical-ai"><strong>The Need for Interpretability in Clinical AI</strong></h3>
<p>Clinical adoption of AI requires trust and transparency. In breast cancer diagnostics, interpretability is essential for:</p>
<ul>
<li>Ensuring AI models align with clinical knowledge.</li>
<li>Validating AI predictions with expert pathologists.</li>
<li>Identifying and mitigating biases or inaccuracies in models.</li>
</ul>
<p>Statistical methods applied to features extracted from CNNs like ResNet50, EfficientNetB0, and DenseNet201 provide a pathway for understanding how these models make decisions. This ensures that critical insights are accessible and actionable in clinical settings.</p>
<hr>
<h3 id="feature-extraction-the-foundation-of-interpretability"><strong>Feature Extraction: The Foundation of Interpretability</strong></h3>
<h4 id="1-extracting-features-with-cnns"><strong>1. Extracting Features with CNNs</strong></h4>
<p>Feature extraction involves using CNNs to distill high-dimensional image data into embeddings representing the most critical patterns. For this study, ResNet50, EfficientNetB0, and DenseNet201 were employed as feature extractors. The hierarchical nature of CNNs allowed capturing both low-level details (e.g., textures) and high-level structures (e.g., tissue organization).</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> load_model, Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load pre-trained model and extract features</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_features</span>(model, layer_name, data):
</span></span><span style="display:flex;"><span>    feature_model <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>input, outputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>get_layer(layer_name)<span style="color:#f92672">.</span>output)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> feature_model<span style="color:#f92672">.</span>predict(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>features_resnet <span style="color:#f92672">=</span> extract_features(resnet_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span><span style="display:flex;"><span>features_densenet <span style="color:#f92672">=</span> extract_features(densenet_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span></code></pre></div><h4 id="2-visualization-using-pca"><strong>2. Visualization Using PCA</strong></h4>
<p>Dimensionality reduction techniques, such as Principal Component Analysis (PCA), were applied to visualize feature distributions. This revealed class separability between benign and malignant samples.</p>
<hr>
<h3 id="statistical-analysis-of-features"><strong>Statistical Analysis of Features</strong></h3>
<h4 id="1-feature-significance-t-tests-and-anova"><strong>1. Feature Significance: T-tests and ANOVA</strong></h4>
<p>By applying t-tests and ANOVA, the statistical significance of features was assessed. This helped identify which features most effectively distinguish benign from malignant samples.</p>
<p><strong>Example Result</strong>:</p>
<ul>
<li>DenseNet201 features showed higher statistical significance compared to ResNet50 and EfficientNetB0, consistent with its superior classification performance (accuracy: 98.31%, AUC: 99.67%).</li>
</ul>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> ttest_ind
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform t-tests between benign and malignant classes</span>
</span></span><span style="display:flex;"><span>t_stat, p_val <span style="color:#f92672">=</span> ttest_ind(features_benign, features_malignant, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>significant_features <span style="color:#f92672">=</span> sorted(zip(range(len(t_stat)), t_stat, p_val), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">2</span>])
</span></span></code></pre></div><h4 id="2-hierarchical-clustering-of-features"><strong>2. Hierarchical Clustering of Features</strong></h4>
<p>Hierarchical clustering grouped features into clusters, revealing latent patterns in the data. Dendrograms highlighted similarities and relationships between features.</p>
<p><strong>Visualisation</strong>:
<figure><img src="/images/project13_images/dendo_densenet201.png"><figcaption>
      <h4>Dendrogram for DenseNet201</h4>
    </figcaption>
</figure>

<figure><img src="/images/project13_images/dendo_resnet50.png"><figcaption>
      <h4>Dendrogram for ResNet50</h4>
    </figcaption>
</figure>

<figure><img src="/images/project13_images/dendo_eff_netB0.png"><figcaption>
      <h4>Dendrogram for EfficientNetB0</h4>
    </figcaption>
</figure>
</p>
<hr>
<h3 id="class-specific-feature-analysis"><strong>Class-Specific Feature Analysis</strong></h3>
<h4 id="boxplot-analysis"><strong>Boxplot Analysis</strong></h4>
<p>Boxplots illustrated the distribution of specific features across classes, aiding in identifying intra-class variations.</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Visualise feature distributions</span>
</span></span><span style="display:flex;"><span>feature_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(features, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Feature_</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(features<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])])
</span></span><span style="display:flex;"><span>feature_df[<span style="color:#e6db74">&#39;Class&#39;</span>] <span style="color:#f92672">=</span> labels
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>boxplot(x<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Class&#39;</span>, y<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Feature_10&#39;</span>, data<span style="color:#f92672">=</span>feature_df)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Distribution of Feature_10 Across Classes&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="correlation-analysis"><strong>Correlation Analysis</strong></h4>
<p>Correlation matrices assessed relationships between features, identifying redundancies and complementary patterns. These analyses were critical for understanding the interplay between features extracted by different CNNs.</p>
<hr>
<h3 id="insights-gained"><strong>Insights Gained</strong></h3>
<ol>
<li><strong>Feature Importance</strong>:
<ul>
<li>Statistical methods highlighted key features contributing to classification, improving interpretability and model transparency.</li>
</ul>
</li>
<li><strong>Class-Specific Patterns</strong>:
<ul>
<li>Hierarchical clustering revealed how different features corresponded to tumor subtypes, providing actionable insights for pathologists.</li>
</ul>
</li>
<li><strong>Inter-Model Comparisons</strong>:
<ul>
<li>DenseNet201 consistently produced features with higher discriminatory power, aligning with its overall performance metrics.</li>
</ul>
</li>
</ol>
<hr>
<ol>
<li><strong>PCA Scatter Plot</strong>: Shows how the features separate benign and malignant samples in a reduced two-dimensional space.
<figure><img src="/images/project13_images/pca.png">
</figure>
</li>
<li><strong>T-Statistic Bar Plot</strong>: Highlights the top 10 features most effective at distinguishing between classes based on their statistical significance.
<figure><img src="/images/project13_images/t.png">
</figure>
</li>
<li><strong>Boxplots</strong>: Provide a clear comparison of the distribution of a key feature across benign and malignant classes.
<figure><img src="/images/project13_images/b.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
<li><strong>Dendrogram</strong>: Visualises hierarchical clustering of features, helping to understand groupings and relationships among extracted features.
<figure><img src="/images/project13_images/h.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
</ol>
<h3 id="towards-explainable-ai"><strong>Towards Explainable AI</strong></h3>
<p>Integrating these statistical insights with explainability tools such as Grad-CAM and LIME further enhances trust in AI systems:</p>
<ul>
<li><strong>Grad-CAM</strong>: Visualizes regions influencing decisions, correlating with statistically significant features.</li>
<li><strong>LIME</strong>: Explains the contribution of individual features to predictions.</li>
</ul>
<hr>
<h3 id="conclusion"><strong>Conclusion</strong></h3>
<p>By combining CNN-based feature extraction with statistical analysis, this study bridges the gap between high-performance AI models and their clinical applicability. These techniques not only enhance interpretability but also provide actionable insights, paving the way for more transparent and reliable AI-driven diagnostics.</p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://drnsmith.github.io/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


