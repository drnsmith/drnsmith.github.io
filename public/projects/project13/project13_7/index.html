<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Part 7. Evaluating AI Models for Healthcare: Beyond Accuracy. | Natasha Smith Portfolio</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Explore key performance metrics like sensitivity, specificity, and AUC to assess and optimise AI models for clinical use."><meta name=generator content="Hugo 0.142.0"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=stylesheet href=/css/custom.css></head><body class="ma0 avenir bg-near-white"><nav class="pa3 pa4-ns flex justify-end items-center"><ul class="list flex ma0 pa0"><li class=ml3><a class="link dim dark-gray f5" href=/>Home</a></li><li class=ml3><a class="link dim dark-gray f5" href=/about/>About</a></li><li class=ml3><a class="link dim dark-gray f5" href=/projects/>Projects</a></li><li class=ml3><a class="link dim dark-gray f5" href=/contact/>Contact</a></li></ul></nav><header class=page-header style=background-image:url(/images/project13_images/pr13.jpg);background-size:cover;background-position:50%;height:400px;display:flex;align-items:center;justify-content:center;color:#fff;text-align:center><div style=background-color:rgba(0,0,0,.4);padding:1rem;border-radius:4px><h1 class="f1 athelas mt3 mb1">Part 7. Evaluating AI Models for Healthcare: Beyond Accuracy.</h1><p class=f5>Explore key performance metrics like sensitivity, specificity, and AUC to assess and optimise AI models for clinical use.</p></div></header><main class=pb7 role=main><article class="mw8 center ph3"><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src=/images/project13_images/pr13.jpg><figcaption><h4>Photo by Ben Hershey on Unsplash</h4></figcaption></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith//Histopathology-AI-BreastCancer target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><h3 id=evaluating-ai-models-for-healthcare-beyond-accuracy><strong>Evaluating AI Models for Healthcare: Beyond Accuracy</strong></h3><p>In healthcare, the stakes are higher than in most other fields. A seemingly high-performing AI model that achieves 95% accuracy may still fail to detect critical cases, leading to life-threatening consequences. For clinical applications, performance metrics like <strong>sensitivity</strong>, <strong>specificity</strong>, and <strong>Area Under the Curve (AUC)</strong> provide a more nuanced evaluation, ensuring AI models align with real-world needs.</p><p>In this blog, we explore these key metrics and their role in assessing and optimizing AI models for healthcare.</p><hr><h3 id=why-accuracy-alone-is-insufficient><strong>Why Accuracy Alone is Insufficient</strong></h3><p>Accuracy measures the proportion of correct predictions over total predictions, but it doesn’t tell the whole story. For example:</p><ul><li>In a dataset with 90% benign cases and 10% malignant cases, a model predicting &ldquo;benign&rdquo; for all samples achieves 90% accuracy—but fails to detect any malignant cases.</li></ul><p>In healthcare, <strong>false negatives</strong> (failing to detect disease) and <strong>false positives</strong> (falsely diagnosing disease) have vastly different implications, requiring metrics that account for this imbalance.</p><hr><h3 id=key-metrics-for-evaluating-ai-models-in-healthcare><strong>Key Metrics for Evaluating AI Models in Healthcare</strong></h3><h4 id=1-sensitivity-recall><strong>1. Sensitivity (Recall)</strong></h4><p><strong>Definition</strong>: The proportion of actual positive cases (e.g., malignant) correctly identified by the model.</p><p>[
\text{Sensitivity} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Negatives (FN)}}
]</p><p><strong>Importance</strong>:</p><ul><li>High sensitivity ensures the model minimizes false negatives, crucial for detecting diseases that require urgent intervention.</li></ul><h4 id=2-specificity><strong>2. Specificity</strong></h4><p><strong>Definition</strong>: The proportion of actual negative cases (e.g., benign) correctly identified by the model.</p><p>[
\text{Specificity} = \frac{\text{True Negatives (TN)}}{\text{True Negatives (TN)} + \text{False Positives (FP)}}
]</p><p><strong>Importance</strong>:</p><ul><li>High specificity reduces false positives, preventing unnecessary anxiety and additional testing for patients.</li></ul><h4 id=3-precision><strong>3. Precision</strong></h4><p><strong>Definition</strong>: The proportion of predicted positive cases that are actually positive.</p><p>[
\text{Precision} = \frac{\text{True Positives (TP)}}{\text{True Positives (TP)} + \text{False Positives (FP)}}
]</p><p><strong>Importance</strong>:</p><ul><li>High precision ensures that positive predictions are reliable, reducing the burden of follow-up testing.</li></ul><h4 id=4-f1-score><strong>4. F1-Score</strong></h4><p><strong>Definition</strong>: The harmonic mean of sensitivity and precision.</p><p>[
\text{F1-Score} = 2 \cdot \frac{\text{Precision} \cdot \text{Sensitivity}}{\text{Precision} + \text{Sensitivity}}
]</p><p><strong>Importance</strong>:</p><ul><li>Useful for imbalanced datasets, balancing false positives and false negatives.</li></ul><h4 id=5-area-under-the-curve-auc><strong>5. Area Under the Curve (AUC)</strong></h4><p><strong>Definition</strong>: The area under the Receiver Operating Characteristic (ROC) curve, which plots the true positive rate (sensitivity) against the false positive rate (1-specificity).</p><p><strong>Importance</strong>:</p><ul><li>AUC evaluates the model&rsquo;s ability to distinguish between classes across various probability thresholds.</li><li>AUC close to 1.0 indicates excellent discrimination, while 0.5 represents random guessing.</li></ul><hr><h3 id=case-study-breakhis-dataset-for-breast-cancer-diagnosis><strong>Case Study: BreakHis Dataset for Breast Cancer Diagnosis</strong></h3><h4 id=dataset><strong>Dataset</strong>:</h4><ul><li>Histopathological dataset with imbalanced benign (31%) and malignant (69%) cases.</li></ul><h4 id=baseline-evaluation><strong>Baseline Evaluation</strong>:</h4><table><thead><tr><th><strong>Metric</strong></th><th><strong>Value</strong></th></tr></thead><tbody><tr><td>Accuracy</td><td>94.5%</td></tr><tr><td>Sensitivity</td><td>88.2%</td></tr><tr><td>Specificity</td><td>72.3%</td></tr><tr><td>Precision</td><td>90.1%</td></tr><tr><td>F1-Score</td><td>89.1%</td></tr><tr><td>AUC</td><td>0.92</td></tr></tbody></table><p><strong>Analysis</strong>:</p><ul><li>While accuracy is high, the relatively low specificity indicates frequent false positives, causing unnecessary interventions.</li></ul><h4 id=optimized-model><strong>Optimized Model</strong>:</h4><p>Using weighted loss functions and data augmentation, sensitivity and specificity were balanced.</p><table><thead><tr><th><strong>Metric</strong></th><th><strong>Value</strong></th></tr></thead><tbody><tr><td>Accuracy</td><td>96.8%</td></tr><tr><td>Sensitivity</td><td>93.7%</td></tr><tr><td>Specificity</td><td>90.5%</td></tr><tr><td>Precision</td><td>94.2%</td></tr><tr><td>F1-Score</td><td>93.9%</td></tr><tr><td>AUC</td><td>0.96</td></tr></tbody></table><p><strong>Outcome</strong>:</p><ul><li>The optimized model achieved a better balance between sensitivity and specificity, improving both diagnostic accuracy and reliability.</li></ul><hr><h3 id=visualizing-model-performance><strong>Visualizing Model Performance</strong></h3><ol><li><strong>ROC Curve</strong>:<ul><li>A graphical representation showing trade-offs between sensitivity and specificity.</li><li>Helps in selecting an optimal probability threshold.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> roc_curve, auc
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Simulated data for ROC curve</span>
</span></span><span style=display:flex><span>fpr, tpr, _ <span style=color:#f92672>=</span> roc_curve(y_test, model_probs)
</span></span><span style=display:flex><span>roc_auc <span style=color:#f92672>=</span> auc(fpr, tpr)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>8</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(fpr, tpr, label<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;ROC Curve (AUC = </span><span style=color:#e6db74>{</span>roc_auc<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot([<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>], [<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>], linestyle<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;--&#39;</span>, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;gray&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;False Positive Rate (1-Specificity)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;True Positive Rate (Sensitivity)&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;ROC Curve&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend(loc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;lower right&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><ol start=2><li><strong>Confusion Matrix</strong>:<ul><li>Summarizes true positives, false positives, true negatives, and false negatives.</li></ul></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics <span style=color:#f92672>import</span> confusion_matrix, ConfusionMatrixDisplay
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>conf_matrix <span style=color:#f92672>=</span> confusion_matrix(y_test, model_preds)
</span></span><span style=display:flex><span>disp <span style=color:#f92672>=</span> ConfusionMatrixDisplay(conf_matrix, display_labels<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;Benign&#34;</span>, <span style=color:#e6db74>&#34;Malignant&#34;</span>])
</span></span><span style=display:flex><span>disp<span style=color:#f92672>.</span>plot(cmap<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Blues&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Confusion Matrix&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><hr><h3 id=best-practices-for-evaluating-healthcare-ai-models><strong>Best Practices for Evaluating Healthcare AI Models</strong></h3><ol><li><strong>Use Multiple Metrics</strong>:<ul><li>Rely on sensitivity, specificity, and AUC instead of accuracy alone.</li></ul></li><li><strong>Consider Clinical Context</strong>:<ul><li>Prioritize metrics like sensitivity for life-threatening conditions.</li><li>Optimize specificity to reduce unnecessary follow-ups for benign cases.</li></ul></li><li><strong>Threshold Tuning</strong>:<ul><li>Adjust probability thresholds to balance sensitivity and specificity based on clinical needs.</li></ul></li></ol><hr><h3 id=conclusion><strong>Conclusion</strong></h3><p>Evaluating AI models for healthcare requires moving beyond accuracy to metrics like sensitivity, specificity, and AUC. These metrics provide a nuanced understanding of model performance, ensuring reliable and clinically meaningful predictions. By adopting this comprehensive evaluation approach, we can develop AI tools that clinicians can trust, ultimately improving patient outcomes.</p></div></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Natasha Smith Portfolio 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>