<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>PART 5. Exploring Long Short-Term Memory (LSTM) for Time-Series Data | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Dive into the use of Long Short-Term Memory (LSTM) networks for time-series forecasting. This blog examines the architecture, training process, and challenges faced in adapting LSTM to environmental data.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project9_images/pr9.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            PART 5. Exploring Long Short-Term Memory (LSTM) for Time-Series Data
          </h1>
          
            <p class="f5">Dive into the use of Long Short-Term Memory (LSTM) networks for time-series forecasting. This blog examines the architecture, training process, and challenges faced in adapting LSTM to environmental data.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project9_images/pr9.jpg"
    alt="Photo by Markus Distelrath on Pexels"><figcaption>
      <p>Photo by Markus Distelrath on Pexels</p>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Pollution-Prediction-Auckland" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Time-series data presents unique challenges and opportunities. The sequential nature of the data requires models capable of capturing dependencies over time—something traditional machine learning (ML)models often struggle with.</p>
<p>In this blog, we delve into the use of Long Short-Term Memory (LSTM) networks, a type of recurrent neural network (RNN), to predict PM10 pollution levels in Auckland.</p>
<p>We’ll explore how LSTM networks work, their application in this project, and the hurdles we faced along the way.</p>
<h3 id="1-why-lstm-for-air-pollution-data">1. Why LSTM for Air Pollution Data?</h3>
<p><strong>Sequential Dependencies</strong>
Unlike regression or Random Forest models, LSTMs are specifically designed to handle sequential data. In air pollution forecasting:</p>
<ul>
<li><em>Lagged Variables</em>: PM10 levels from previous hours directly influence current levels.</li>
<li><em>Temporal Trends</em>: Patterns like rush hours or seasonal changes require a model that &ldquo;remembers&rdquo; past inputs.</li>
</ul>
<p><strong>Capturing Complex Dynamics</strong>
LSTM networks excel at modelling complex, non-linear relationships in time-series data. This is especially valuable for air quality data, where pollution levels are influenced by traffic, weather, and geographic factors.</p>
<h3 id="2-how-lstms-work">2. How LSTMs Work</h3>
<p><strong>The Basics of Recurrent Neural Networks (RNNs)</strong></p>
<p>RNNs are neural networks (NNs) with loops that allow information to persist. However, standard RNNs struggle with long-term dependencies due to the vanishing gradient problem.</p>
<p><strong>Enter LSTM</strong></p>
<p>LSTM networks address this limitation with their unique architecture:</p>
<ul>
<li><strong>Cell State</strong>8**: A &ldquo;memory&rdquo; that flows through the network, carrying relevant information forward.</li>
<li><strong>Gates</strong>: Mechanisms that control what information is added, removed, or retained:</li>
<li><strong>Forget Gate</strong>: Decides what to discard.</li>
<li><strong>Input Gate</strong>: Determines what new information to add.</li>
<li><strong>Output Gate</strong>: Controls what information to output.</li>
</ul>
<p>This structure allows LSTM networks to maintain long-term dependencies, making them ideal for time-series tasks.</p>
<h3 id="3-applying-lstm-to-predict-pm10-levels">3. Applying LSTM to Predict PM10 Levels</h3>
<p><strong>Data Preparation</strong></p>
<ul>
<li><em>Feature Scaling</em>: Scaling the features ensured that the LSTM model could converge efficiently.</li>
<li><em>Reshaping for Time-Series</em>: The input data was reshaped into a 3D format—samples, timesteps, and features.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> MinMaxScaler
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Scaling the data</span>
</span></span><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> MinMaxScaler()
</span></span><span style="display:flex;"><span>X_scaled <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(X)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reshaping for LSTM</span>
</span></span><span style="display:flex;"><span>X_lstm <span style="color:#f92672">=</span> X_scaled<span style="color:#f92672">.</span>reshape((X_scaled<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, X_scaled<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span></code></pre></div><p><strong>Model Architecture and Training</strong></p>
<p>We designed and trained an LSTM network with the following parameters:</p>
<ul>
<li>Hidden Units: 64 neurons in the LSTM layer to capture temporal patterns.</li>
<li>Optimisation: Adam optimiser for efficient learning.</li>
<li>Loss Function: Mean Squared Error (MSE) to minimize prediction errors.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the LSTM model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential()
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(LSTM(<span style="color:#ae81ff">64</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(X_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], X_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>])))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>add(Dense(<span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mse&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model</span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(X_lstm, y, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, validation_split<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><h3 id="4-hyperparameter-tuning">4. Hyperparameter Tuning</h3>
<p>To optimise the model&rsquo;s performance, we tuned key hyperparameters:</p>
<ul>
<li>Batch Size: We tested batch sizes of 4, 8, 16, 32, and 64, with 32 emerging as the optimal size for balancing runtime and stability.</li>
<li>Epochs: Although 30 epochs were used for training, the 10th epoch was identified as a trade-off between cost and accuracy.</li>
<li>Hidden Layer Neurons: 64 neurons provided a balance between accuracy and computational efficiency.</li>
</ul>
<h3 id="5-results-and-insights">5. Results and Insights</h3>
<h4 id="performance-metrics">Performance Metrics</h4>
<p>The LSTM model demonstrated significant improvements over traditional models:</p>
<ul>
<li><strong>RMSE</strong>: 0.97 (compared to 1.21 for Random Forest).</li>
<li><strong>MAE</strong>: 0.73 (compared to 0.85 for Random Forest).</li>
</ul>
<h4 id="capturing-temporal-trends">Capturing Temporal Trends</h4>
<p>The LSTM model successfully captured:</p>
<ul>
<li>Rush Hour Spikes: Morning and evening traffic peaks.</li>
<li>Seasonal Patterns: Higher pollution levels during winter due to stagnant air conditions.</li>
</ul>
<h4 id="feature-contributions">Feature Contributions</h4>
<p>SHAP analysis revealed that lagged PM10 values, wind speed, and traffic volume were the most influential predictors in the LSTM model.</p>
<h4 id="challenges-faced">Challenges Faced</h4>
<ul>
<li>
<p><em>Computational Complexity</em>
Training LSTM networks on large datasets is computationally intensive. Each epoch required significant processing power, and hyperparameter tuning added to the computational burden.</p>
</li>
<li>
<p><em>Data Preparation</em>
Environmental data is inherently messy. Missing values, outliers, and inconsistencies made the pre-processing phase critical. Lagged variables and feature engineering were essential to capture temporal patterns.</p>
</li>
<li>
<p><em>Overfitting</em>
With a limited amount of high-quality data, over-fitting became a concern. We mitigated this by:</p>
</li>
<li>
<p>Using dropout layers to prevent the network from relying too heavily on specific neurons.</p>
</li>
<li>
<p>Regularisation techniques like early stopping.</p>
</li>
</ul>
<p><strong>Lessons Learned</strong></p>
<ul>
<li>The Power of Sequential Models: LSTM networks proved invaluable for capturing temporal dependencies in PM10 data.</li>
<li>Importance of Pre-processing: High-quality data pre-processing laid the foundation for accurate predictions.</li>
</ul>
<p><strong>Challenges to Overcome</strong></p>
<ul>
<li>Resource Intensity: LSTM models require significant computational resources.</li>
<li>Interpretability: Advanced tools like SHAP values are essential for explaining model predictions.</li>
</ul>
<h3 id="conclusion-unlocking-the-potential-of-lstm-for-environmental-data">Conclusion: Unlocking the Potential of LSTM for Environmental Data</h3>
<p>By leveraging LSTM networks, we were able to uncover patterns and trends in air pollution data that traditional models missed.</p>
<p>However, this approach comes with its own set of challenges, from computational demands to interpretability issues.</p>
<p>Despite these hurdles, the insights gained from LSTM models have the potential to inform policies and actions aimed at improving air quality.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and let&rsquo;s keep our planet healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://drnsmith.github.io/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


