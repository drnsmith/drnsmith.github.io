<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 4. Advanced Machine Learning for PM10 Prediction: Random Forest, XGBoost, and More. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Discover how ensemble models like Random Forest and XGBoost outperform traditional regression methods in handling complex pollution datasets.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project8_images/pr8.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 4. Advanced Machine Learning for PM10 Prediction: Random Forest, XGBoost, and More.
          </h1>
          
            <p class="f5">Discover how ensemble models like Random Forest and XGBoost outperform traditional regression methods in handling complex pollution datasets.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project8_images/pr8.jpg"
    alt="Photo by Dom J on Pexels"><figcaption>
      <p>Photo by Dom J on Pexels</p>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Regression models laid a solid foundation for PM10 prediction, but air pollution is a complex phenomenon influenced by nonlinear and time-dependent factors.</p>
<p>To capture these intricacies, advanced machine learning models like neural networks (NNs) and ensemble methods come into play. These models are capable of uncovering patterns and relationships that simpler models might overlook.</p>
<p>In this blog, I’ll explore how advanced methods such as <strong>Random Forest</strong>, <strong>Gradient Boosting</strong>, and <strong>Long Short-Term Memory (LSTM)</strong> networks were employed to predict PM10 levels with greater accuracy.</p>
<p>I’ll also discuss their strengths, limitations, and the unique insights they offered into the dynamics of air pollution.</p>
<h3 id="1-ensemble-methods-random-forest-and-gradient-boosting">1. Ensemble Methods: Random Forest and Gradient Boosting</h3>
<h4 id="random-forest">Random Forest</h4>
<p>Random Forest is an ensemble method that builds multiple decision trees and averages their predictions. It reduces over-fitting and improves accuracy by leveraging the wisdom of the crowd.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> RandomForestRegressor
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_absolute_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Random Forest</span>
</span></span><span style="display:flex;"><span>rf_model <span style="color:#f92672">=</span> RandomForestRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>rf_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>rf_pred <span style="color:#f92672">=</span> rf_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>rf_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, rf_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Random Forest - MAE: </span><span style="color:#e6db74">{</span>rf_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h4 id="gradient-boosting">Gradient Boosting</h4>
<p>Gradient Boosting builds trees sequentially, with each tree correcting the errors of the previous one. It excels at capturing subtle patterns in the data.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.ensemble <span style="color:#f92672">import</span> GradientBoostingRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Gradient Boosting</span>
</span></span><span style="display:flex;"><span>gb_model <span style="color:#f92672">=</span> GradientBoostingRegressor(n_estimators<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>gb_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>gb_pred <span style="color:#f92672">=</span> gb_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>gb_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, gb_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Gradient Boosting - MAE: </span><span style="color:#e6db74">{</span>gb_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Ensemble Models</strong>:</p>
<ul>
<li>Random Forest provided robust predictions by averaging over many decision trees, making it less prone to overfitting.</li>
<li>Gradient Boosting excelled at capturing complex patterns but required careful tuning of hyperparameters like learning rate and number of trees.</li>
<li>Both models outperformed simpler regression techniques, particularly in predicting pollution spikes.</li>
</ul>
<h3 id="2-neural-networks-a-deep-dive">2. Neural Networks: A Deep Dive</h3>
<h4 id="the-need-for-neural-networks">The Need for Neural Networks</h4>
<p>While ensemble methods are powerful, they struggle with time-series data, where patterns evolve over time. Enter NNs, particularly Long Short-Term Memory (LSTM) networks, which are designed to handle sequential data.</p>
<h4 id="implementing-lstm-for-pm10-prediction">Implementing LSTM for PM10 Prediction</h4>
<p>LSTM networks, a type of recurrent neural network (RNN), can &ldquo;remember&rdquo; patterns across long sequences, making them ideal for predicting hourly or daily PM10 levels.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> LSTM, Dense
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reshape data for LSTM (samples, timesteps, features)</span>
</span></span><span style="display:flex;"><span>X_train_lstm <span style="color:#f92672">=</span> X_train<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape((X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, X_train<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>X_test_lstm <span style="color:#f92672">=</span> X_test<span style="color:#f92672">.</span>values<span style="color:#f92672">.</span>reshape((X_test<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], <span style="color:#ae81ff">1</span>, X_test<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build the LSTM model</span>
</span></span><span style="display:flex;"><span>lstm_model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    LSTM(<span style="color:#ae81ff">50</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>, input_shape<span style="color:#f92672">=</span>(X_train_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>], X_train_lstm<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">2</span>])),
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile and train the model</span>
</span></span><span style="display:flex;"><span>lstm_model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;adam&#39;</span>, loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mean_absolute_error&#39;</span>)
</span></span><span style="display:flex;"><span>lstm_model<span style="color:#f92672">.</span>fit(X_train_lstm, y_train, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>lstm_pred <span style="color:#f92672">=</span> lstm_model<span style="color:#f92672">.</span>predict(X_test_lstm)
</span></span><span style="display:flex;"><span>lstm_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, lstm_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;LSTM - MAE: </span><span style="color:#e6db74">{</span>lstm_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h4 id="comparing-model-performances">Comparing Model Performances</h4>
<p>To evaluate the effectiveness of the models, we compared their Mean Absolute Error (MAE):
<figure><img src="/images/project8_images/eval.png">
</figure>
</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Random Forest and Gradient Boosting: Excellent at capturing feature interactions and nonlinear patterns.</li>
<li>LSTM: Outperformed all other models by leveraging time-series data, capturing daily and seasonal trends effectively.</li>
</ul>
<h3 id="challenges-of-advanced-models">Challenges of Advanced Models</h3>
<p>While advanced models offer superiour performance, they come with their own set of challenges:</p>
<ul>
<li><em>Computational Intensity</em>: Training LSTM networks required significant time and computational resources.</li>
<li><em>Hyperparameter Tuning</em>: Models like Gradient Boosting and LSTM are sensitive to hyperparameters, requiring extensive experimentation to optimize.</li>
<li><em>Interpretability</em>: Unlike regression models, NNs operate as black boxes, making it harder to explain their predictions.</li>
</ul>
<h3 id="lessons-learned">Lessons Learned</h3>
<p>Working with advanced models highlighted the importance of:</p>
<ul>
<li><strong>Feature Engineering</strong>: Creating time-based features (e.g., hour of the day) significantly improved model performance.</li>
<li><strong>Model Stacking</strong>: Combining the strengths of different models (e.g., Random Forest + LSTM) could further enhance predictions.</li>
<li><strong>Domain Knowledge</strong>: Understanding the environmental factors affecting PM10 helped guide feature selection and model interpretation.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Advanced models like Random Forest, Gradient Boosting, and LSTM pushed the boundaries of what we could achieve in predicting PM10 levels.</p>
<p>By leveraging these techniques, we not only improved accuracy but also gained deeper insights into the factors driving air pollution.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and let&rsquo;s keep our planet healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


