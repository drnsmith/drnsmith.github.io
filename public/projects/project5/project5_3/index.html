<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 3. Manual CNN vs. Pre-Trained VGG16: A Comparative Analysis. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="This blog compares the performance of a manually designed CNN and the pre-trained VGG16 model for pneumonia detection. The analysis highlights their architectural differences, training strategies, and performance on chest X-ray data.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project5_images/pr5.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 3. Manual CNN vs. Pre-Trained VGG16: A Comparative Analysis.
          </h1>
          
            <p class="f5">This blog compares the performance of a manually designed CNN and the pre-trained VGG16 model for pneumonia detection. The analysis highlights their architectural differences, training strategies, and performance on chest X-ray data.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><p><figure><img src="/images/project5_images/pr5.jpg">
</figure>

<strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/pneumonia-detection-CNN" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
</a>
<h3 id="introduction">Introduction</h3>
<p>Deep learning (DL) provides multiple pathways to solving problems, including designing custom architectures or leveraging pre-trained models.</p>
<p>In this blog, we compare the performance of a <strong>manual CNN</strong> and the <strong>VGG16 pre-trained model</strong> for pneumonia detection.</p>
<p>While the manual CNN was lightweight and tailored to the dataset, VGG16 brought the power of transfer learning with its pre-trained <strong>ImageNet</strong> weights.</p>
<p>This comparative analysis explores their architectures, training strategies, and results.</p>
<h3 id="manual-cnn-tailored-for-the-dataset">Manual CNN: Tailored for the Dataset</h3>
<p>The manually designed CNN aimed to strike a balance between simplicity and performance. It consisted of convolutional layers for feature extraction, pooling layers for down-sampling, and dense layers for classification.</p>
<h4 id="architecture">Architecture</h4>
<ul>
<li><strong>Convolution Layers</strong>: Extract features like edges and textures.</li>
<li><strong>MaxPooling Layers</strong>: Reduce spatial dimensions and computational complexity.</li>
<li><strong>Dense Layers</strong>: Combine extracted features for classification.</li>
</ul>
<h4 id="python-code-manual-cnn-architecture">Python Code: Manual CNN Architecture</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Conv2D, MaxPooling2D, Flatten, Dense, Dropout
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_manual_cnn</span>(input_shape):
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>        Conv2D(<span style="color:#ae81ff">32</span>, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>, input_shape<span style="color:#f92672">=</span>input_shape),
</span></span><span style="display:flex;"><span>        MaxPooling2D((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)),
</span></span><span style="display:flex;"><span>        Conv2D(<span style="color:#ae81ff">64</span>, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>), activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>),
</span></span><span style="display:flex;"><span>        MaxPooling2D((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)),
</span></span><span style="display:flex;"><span>        Flatten(),
</span></span><span style="display:flex;"><span>        Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>),
</span></span><span style="display:flex;"><span>        Dropout(<span style="color:#ae81ff">0.5</span>),
</span></span><span style="display:flex;"><span>        Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialize model</span>
</span></span><span style="display:flex;"><span>manual_cnn <span style="color:#f92672">=</span> build_manual_cnn((<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>manual_cnn<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><h4 id="strengths">Strengths</h4>
<ul>
<li><em>Lightweight</em>: Fewer parameters compared to large pre-trained models.</li>
<li><em>Flexibility</em>: Architecture tailored to chest X-ray data.</li>
</ul>
<h4 id="limitations">Limitations</h4>
<ul>
<li><em>Learning from Scratch</em>: Lacks the knowledge pre-trained on large datasets like <strong>ImageNet</strong>.</li>
<li><em>Longer Training Time</em>: Requires more epochs to converge.</li>
</ul>
<h3 id="vgg16-transfer-learning-in-action">VGG16: Transfer Learning in Action</h3>
<p>VGG16 is a popular pre-trained CNN that has demonstrated strong performance in image classification tasks.</p>
<p>By freezing its convolutional layers, we leveraged its pre-trained weights for feature extraction while fine-tuning the dense layers for pneumonia detection.</p>
<h4 id="architecture-1">Architecture</h4>
<ul>
<li><em>Feature Extraction Layers</em>: Pre-trained convolutional layers from VGG16.</li>
<li><em>Dense Layers</em>: Custom layers added for binary classification.</li>
</ul>
<h4 id="python-code-vgg16-model">Python Code: VGG16 Model</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.applications <span style="color:#f92672">import</span> VGG16
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Flatten, Dense, Dropout
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">build_vgg16</span>(input_shape):
</span></span><span style="display:flex;"><span>    base_model <span style="color:#f92672">=</span> VGG16(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;imagenet&#34;</span>, include_top<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, input_shape<span style="color:#f92672">=</span>input_shape)
</span></span><span style="display:flex;"><span>    model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>        base_model,
</span></span><span style="display:flex;"><span>        Flatten(),
</span></span><span style="display:flex;"><span>        Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;relu&#34;</span>),
</span></span><span style="display:flex;"><span>        Dropout(<span style="color:#ae81ff">0.5</span>),
</span></span><span style="display:flex;"><span>        Dense(<span style="color:#ae81ff">1</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;sigmoid&#34;</span>)
</span></span><span style="display:flex;"><span>    ])
</span></span><span style="display:flex;"><span>    base_model<span style="color:#f92672">.</span>trainable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>  <span style="color:#75715e"># Freeze pre-trained layers</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise model</span>
</span></span><span style="display:flex;"><span>vgg16_model <span style="color:#f92672">=</span> build_vgg16((<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">3</span>))
</span></span><span style="display:flex;"><span>vgg16_model<span style="color:#f92672">.</span>summary()
</span></span></code></pre></div><h4 id="strengths-1">Strengths</h4>
<ul>
<li><em>Transfer Learning</em>: Pre-trained weights accelerate training and improve accuracy.</li>
<li><em>Feature Richness</em>: Extracts high-level features from images.</li>
</ul>
<h4 id="limitations-1">Limitations</h4>
<ul>
<li><em>Heavy Architecture</em>: High computational requirements.</li>
<li><em>Over-fitting Risk</em>: Fine-tuning dense layers requires careful monitoring.</li>
</ul>
<h3 id="training-strategies">Training Strategies</h3>
<p>Both models were trained on the augmented dataset with the same optimiser, learning rate, and number of epochs.</p>
<h4 id="python-code-training">Python Code: Training</h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.optimizers <span style="color:#f92672">import</span> Adam
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.image <span style="color:#f92672">import</span> ImageDataGenerator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Data augmentation</span>
</span></span><span style="display:flex;"><span>datagen <span style="color:#f92672">=</span> ImageDataGenerator(
</span></span><span style="display:flex;"><span>    rescale<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">255</span>,
</span></span><span style="display:flex;"><span>    rotation_range<span style="color:#f92672">=</span><span style="color:#ae81ff">15</span>,
</span></span><span style="display:flex;"><span>    width_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    height_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    zoom_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>    horizontal_flip<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_generator <span style="color:#f92672">=</span> datagen<span style="color:#f92672">.</span>flow_from_directory(<span style="color:#e6db74">&#34;train&#34;</span>, target_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>), batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, class_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary&#34;</span>)
</span></span><span style="display:flex;"><span>val_generator <span style="color:#f92672">=</span> datagen<span style="color:#f92672">.</span>flow_from_directory(<span style="color:#e6db74">&#34;val&#34;</span>, target_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">150</span>, <span style="color:#ae81ff">150</span>), batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>, class_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile and train the model</span>
</span></span><span style="display:flex;"><span>manual_cnn<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>), loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary_crossentropy&#34;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
</span></span><span style="display:flex;"><span>vgg16_model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>Adam(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>), loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;binary_crossentropy&#34;</span>, metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;accuracy&#34;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train models</span>
</span></span><span style="display:flex;"><span>manual_cnn<span style="color:#f92672">.</span>fit(train_generator, validation_data<span style="color:#f92672">=</span>val_generator, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>vgg16_model<span style="color:#f92672">.</span>fit(train_generator, validation_data<span style="color:#f92672">=</span>val_generator, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>)
</span></span></code></pre></div><h3 id="comparison-of-results">Comparison of Results</h3>
<figure><img src="/images/project5_images/performance.png">
</figure>

<h4 id="observations">Observations</h4>
<ul>
<li><em>Training Speed</em>:</li>
</ul>
<p>Manual CNN converged more slowly compared to VGG16.</p>
<ul>
<li><em>Accuracy</em>:</li>
</ul>
<p>VGG16 outperformed the manual CNN by 2% in validation accuracy.</p>
<ul>
<li><em>Recall</em>:</li>
</ul>
<p>VGG16 achieved higher recall, crucial for detecting pneumonia cases with minimal false negatives.</p>
<h3 id="key-takeaways">Key Takeaways</h3>
<p><em>Manual CNN</em>:</p>
<ul>
<li>Lightweight and effective for datasets with limited computational resources.</li>
<li>Requires more training time and careful tuning.</li>
</ul>
<p><em>VGG16</em>:</p>
<ul>
<li>Transfer learning provides a significant performance boost.</li>
<li>Ideal for medical imaging projects with access to powerful hardware.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Both models demonstrated strong performance, but VGG16’s transfer learning capabilities gave it a slight edge in accuracy and recall. However, the manual CNN remains a viable alternative for scenarios with limited computational resources or hardware constraints.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and stay healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://drnsmith.github.io/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


