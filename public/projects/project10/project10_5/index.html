<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>PART 5. Evaluation and Calibration: Building Trust in Medical AI Models | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Learn how to evaluate and calibrate deep learning models for medical imaging. This blog covers calibration curves, F1-score optimization, Brier score loss, ROC-AUC, and confusion matrices, explaining their importance in building trustworthy AI systems for healthcare.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project10_images/pr10.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            PART 5. Evaluation and Calibration: Building Trust in Medical AI Models
          </h1>
          
            <p class="f5">Learn how to evaluate and calibrate deep learning models for medical imaging. This blog covers calibration curves, F1-score optimization, Brier score loss, ROC-AUC, and confusion matrices, explaining their importance in building trustworthy AI systems for healthcare.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><p><figure><img src="/images/project10_images/pr10.jpg">
</figure>

<strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Deep learning models are increasingly used in critical domains like healthcare. However, high accuracy alone doesn’t guarantee a model’s reliability.</p>
<p>For medical AI systems, evaluation and calibration are key to building trust, ensuring fair predictions, and avoiding costly mistakes.</p>
<p>In this blog, we’ll explore:</p>
<ul>
<li>The importance of model calibration.</li>
<li>Key metrics: <strong>F1-score</strong>, <strong>Brier score loss</strong>, <strong>ROC-AUC</strong>, and <strong>confusion matrices</strong>.</li>
<li>How to visualise and measure calibration using calibration curves.</li>
</ul>
<h3 id="why-model-calibration-and-evaluation-matter">Why Model Calibration and Evaluation Matter</h3>
<p>Medical imaging models often predict probabilities (e.g., &ldquo;90% chance of malignancy&rdquo;). But probability alone isn’t useful unless it reflects reality. For instance:</p>
<ul>
<li>If a model predicts &ldquo;90% malignant&rdquo; for 10 images, then approximately 9 of those should indeed be malignant for the model to be calibrated.</li>
<li>Miscalibration can lead to overconfident predictions, causing false positives or negatives—both critical in healthcare.</li>
</ul>
<p>In addition to calibration, evaluating key metrics like F1-score, ROC-AUC, and Brier score loss provides a holistic understanding of model performance.</p>
<h3 id="key-metrics-explained">Key Metrics Explained</h3>
<h4 id="1-calibration-curve"><strong>1. Calibration Curve</strong></h4>
<p>A calibration curve plots predicted probabilities against actual outcomes. Perfectly calibrated models produce a diagonal line. Deviations indicate over- or under-confidence.</p>
<p><strong>Code for Calibration Curve:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.calibration <span style="color:#f92672">import</span> calibration_curve
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_calibration_curve</span>(y_true, y_prob, n_bins<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Plot the calibration curve for model predictions.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    prob_true, prob_pred <span style="color:#f92672">=</span> calibration_curve(y_true, y_prob, n_bins<span style="color:#f92672">=</span>n_bins)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(prob_pred, prob_true, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Model Calibration&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Perfect Calibration&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Mean Predicted Probability&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Fraction of Positives&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Calibration Curve&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="2-f1-score"><strong>2. F1-Score</strong></h4>
<p>The F1-score balances precision (correct positive predictions) and recall (ability to find all positive cases). It’s crucial when classes are imbalanced.</p>
<p><strong>Code for F1-Score Calculation:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> f1_score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_f1_score</span>(y_true, y_pred):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Calculate the F1-score.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> f1_score(y_true, y_pred)
</span></span></code></pre></div><h4 id="3-brier-score-loss"><strong>3. Brier Score Loss</strong></h4>
<p>Brier score measures the accuracy of predicted probabilities. A lower score indicates better calibration.</p>
<p><strong>Code for Brier Score Loss:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> brier_score_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_brier_score</span>(y_true, y_prob):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Calculate the Brier score loss.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> brier_score_loss(y_true, y_prob)
</span></span></code></pre></div><h4 id="4-roc-auc"><strong>4. ROC-AUC</strong></h4>
<p>The Receiver Operating Characteristic - Area Under Curve (ROC-AUC) measures a model&rsquo;s ability to distinguish between classes.</p>
<p><strong>Code for ROC-AUC Calculation:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_auc_score
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculate_roc_auc</span>(y_true, y_prob):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Calculate ROC-AUC score.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> roc_auc_score(y_true, y_prob)
</span></span></code></pre></div><h4 id="5-confusion-matrix"><strong>5. Confusion Matrix</strong></h4>
<p>The confusion matrix summarises true positives, true negatives, false positives, and false negatives, giving a complete view of model errors.</p>
<p><strong>Code for Confusion Matrix Visualisation:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix, ConfusionMatrixDisplay
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_confusion_matrix</span>(y_true, y_pred, class_names):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Plot the confusion matrix.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    cm <span style="color:#f92672">=</span> confusion_matrix(y_true, y_pred)
</span></span><span style="display:flex;"><span>    disp <span style="color:#f92672">=</span> ConfusionMatrixDisplay(confusion_matrix<span style="color:#f92672">=</span>cm, display_labels<span style="color:#f92672">=</span>class_names)
</span></span><span style="display:flex;"><span>    disp<span style="color:#f92672">.</span>plot(cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>Blues)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Confusion Matrix&#39;</span>)
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h3 id="application-to-medical-ai">Application to Medical AI</h3>
<p>When applied to a DenseNet201 model for histopathology, these techniques revealed:</p>
<ul>
<li><strong>Calibration Curve</strong>: The model was slightly overconfident, which we addressed using temperature scaling.</li>
<li><strong>F1-Score</strong>: An optimised F1-score ensured balance between precision and recall, crucial for detecting malignant cases.</li>
<li><strong>Brier Score Loss</strong>: Indicated well-calibrated probabilities after adjustments.</li>
<li><strong>ROC-AUC</strong>: Achieved high separation capability between benign and malignant cases.</li>
<li><strong>Confusion Matrix</strong>: Helped visualise false negatives (missed cancers) and false positives (unnecessary interventions).</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Model evaluation and calibration are not just technical add-ons—they’re essential to deploying trustworthy AI in critical fields like healthcare. By using metrics like F1-score, Brier score loss, and calibration curves, you can ensure your model is both accurate and reliable, paving the way for impactful, ethical AI systems.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and stay healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


