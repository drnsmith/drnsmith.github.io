<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Discover why the Stochastic Gradient Descent (SGD) optimiser remains a popular choice in deep learning. We’ll break down its mechanics, advantages, and trade-offs compared to adaptive optimisers like Adam.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project12_images/pr12.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks.
          </h1>
          
            <p class="f5">Discover why the Stochastic Gradient Descent (SGD) optimiser remains a popular choice in deep learning. We’ll break down its mechanics, advantages, and trade-offs compared to adaptive optimisers like Adam.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project12_images/pr12.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Training a neural network requires more than just a good dataset or an effective architecture—it requires the right optimiser. Stochastic Gradient Descent (SGD) is a staple of deep learning.</p>
<p>In my Fashion MNIST project, I used Stochastic Gradient Descent (SGD) to optimise a dense neural network. Why? Because simplicity doesn’t just work—it excels, especially when resources are limited or interpretability is key.</p>
<p>In this blog, we’ll explore:</p>
<p>How SGD works and its role in neural network training.
Why I chose SGD over more complex optimisers.
Practical lessons learned from using SGD on Fashion MNIST.</p>
<h3 id="technical-explanation">Technical Explanation</h3>
<h4 id="1-what-is-sgd">1. What is SGD?</h4>
<p>SGD, or Stochastic Gradient Descent, is the simplest and most widely used optimisation algorithm for training machine learning models. It works by updating the model&rsquo;s weights to minimise the loss function, one small step at a time.</p>
<p>Here’s the formula:
[
\theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta)
]</p>
<p>Where:</p>
<ul>
<li>( \theta_t ): Current model parameters (weights).</li>
<li>( \eta ): Learning rate, which controls the step size.</li>
<li>( \nabla_\theta J(\theta) ): Gradient of the loss function with respect to the parameters.</li>
</ul>
<h4 id="why-stochastic">Why Stochastic?</h4>
<p>Unlike traditional Gradient Descent, which computes gradients over the entire dataset, SGD updates weights for each mini-batch of data. This speeds up training and adds variability that can help escape local minima.</p>
<h4 id="learning-rate-the-key-to-effective-optimisation">Learning Rate: The Key to Effective Optimisation</h4>
<p>The learning rate is a critical parameter in SGD. It controls how much the model adjusts during each update.</p>
<ul>
<li><strong>Too High</strong>: The model oscillates around the minimum, never converging.</li>
<li><strong>Too Low</strong>: The model converges very slowly, wasting computational resources.</li>
<li></li>
</ul>
<p>In my project, the learning rate of 0.01 ensured steady convergence without overshooting the optimal solution.</p>
<ul>
<li><strong>2. Implementing SGD in Fashion MNIST</strong>
In my project, I chose SGD for its simplicity and interpretability. Here’s how I implemented it:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.optimizers <span style="color:#f92672">import</span> SGD
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile the model with SGD</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>), 
</span></span><span style="display:flex;"><span>              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>, 
</span></span><span style="display:flex;"><span>              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span></code></pre></div><p><strong>Key Details:</strong></p>
<ul>
<li>
<p><strong>Learning Rate <code>(η)</code></strong>: I set it to <code>0.01</code>, a standard starting point that balances stability and convergence speed.</p>
</li>
<li>
<p><strong>Loss Function</strong>: <code>sparse_categorical_crossentropy</code> for multi-class classification tasks.</p>
</li>
<li>
<p><strong>Metrics</strong>: Accuracy to evaluate performance.</p>
</li>
<li>
<p><strong>3. Why Choose SGD?</strong>
SGD may not always be the fastest optimiser, but it offers unique advantages:</p>
</li>
<li>
<p><em>Simplicity and Control</em>:
Unlike adaptive methods like Adam, SGD relies purely on the gradient and a fixed learning rate, making it easier to debug and interpret.</p>
</li>
<li>
<p><em>Generalisation</em>:
The noise introduced by mini-batch updates acts as a natural regulariser, helping the model generalise better to unseen data.</p>
</li>
<li>
<p><em>Resource Efficiency</em>:
Without the additional computation required by adaptive optimisers, SGD is lightweight and resource-efficient, making it ideal for smaller projects like Fashion MNIST.</p>
</li>
</ul>
<p><strong>Drawbacks:</strong></p>
<ul>
<li>
<p><em>Learning Rate Sensitivity</em>:
SGD requires careful tuning of the learning rate. Too high, and the model oscillates; too low, and training stagnates.</p>
</li>
<li>
<p><em>Slow Convergence</em>:
Without momentum or adaptive adjustments, SGD can take longer to converge compared to modern optimisers.</p>
</li>
</ul>
<p><strong>Performance Metrics</strong>
Here’s how SGD performed in my Fashion MNIST project:</p>
<ul>
<li>Training Loss: ~0.68</li>
<li>Validation Loss: ~0.71</li>
<li>Training Accuracy: ~77%</li>
<li>Validation Accuracy: ~76%</li>
</ul>
<p>The results show that SGD achieved strong generalisation, with training and validation metrics closely aligned. This indicates the absence of significant overfitting.</p>
<h3 id="real-world-applications">Real-World Applications</h3>
<h4 id="resource-constrained-environments">Resource-Constrained Environments</h4>
<p>SGD’s simplicity and low computational requirements make it perfect for edge devices, mobile applications, or scenarios with limited hardware resources.</p>
<h4 id="educational-use">Educational Use</h4>
<p>SGD is an excellent teaching tool. Its straightforward mechanism provides a clear understanding of how optimisers work, making it a go-to choice for learning and experimenting with machine learning.</p>
<h4 id="research-and-interpretability">Research and Interpretability</h4>
<p>In research, where interpretability and reproducibility matter, SGD offers a reliable and transparent optimisation method.</p>
<h3 id="conclusion">Conclusion</h3>
<p>SGD may not be the flashiest optimisation algorithm, but its reliability, simplicity, and resource efficiency make it a foundational tool in machine learning.</p>
<p>In my Fashion MNIST project, it provided a robust starting point for training dense NNs, delivering solid results with minimal complexity.</p>
<p>When should you use SGD?</p>
<p>Anytime you want a lightweight, interpretable optimiser for tasks where generalisation and resource constraints matter.</p>
<p>And when you’ve mastered it, you’ll have a deeper appreciation for the fancier optimisers that build upon its principles.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and be trendy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


