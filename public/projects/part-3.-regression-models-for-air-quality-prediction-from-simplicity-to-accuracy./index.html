<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 3. Regression Models for Air Quality Prediction: From Simplicity to Accuracy. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="A deep dive into regression techniques, including Linear Regression, Ridge, and Lasso, to predict PM10 levels and understand key contributing factors.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project8_images/pr8.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 3. Regression Models for Air Quality Prediction: From Simplicity to Accuracy.
          </h1>
          
            <p class="f5">A deep dive into regression techniques, including Linear Regression, Ridge, and Lasso, to predict PM10 levels and understand key contributing factors.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project8_images/pr8.jpg"
    alt="Photo by Dom J on Pexels"><figcaption>
      <p>Photo by Dom J on Pexels</p>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/PM-London-Pollution" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Predicting air pollution isn’t just about crunching numbers—it’s about finding patterns, building models, and learning how different variables interact with one another.</p>
<p>In this blog, I take the first step toward accurate PM10 predictions by exploring regression models. These models form the backbone of many machine learning (ML) projects, providing interpretable results and insights into the relationships between variables.</p>
<p>Regression models are a great starting point for predicting PM10 levels because they are straightforward, efficient, and capable of capturing linear and moderately nonlinear relationships.</p>
<p>In this blog, I’ll dive into how regression models were used, discuss their performance, and highlight the insights they revealed about air quality patterns.</p>
<h3 id="1-the-case-for-regression-models">1. The Case for Regression Models</h3>
<p>Why start with regression? The answer lies in their simplicity and interpretability. Regression models:</p>
<ul>
<li>Identify Relationships: They reveal which features (e.g., wind speed, temperature) most strongly affect PM10 levels.</li>
<li>Set Baseline Performance: They establish a baseline to compare more complex models like neural networks.</li>
<li>Handle Complexity Well: With techniques like regularisation, they manage multicollinearity and over-fitting effectively.</li>
</ul>
<h3 id="2-preparing-the-dataset">2. Preparing the Dataset</h3>
<p>Before diving into model building, the dataset underwent additional preparation:</p>
<ul>
<li><em>Feature Selection</em>: From EDA, we selected the most influential variables, such as traffic volume, temperature, wind speed, and time-based features (hour, day, and season).</li>
<li><em>Train-Test Split</em>: To evaluate the models, we split the data into 80% training and 20% testing sets.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define features and target</span>
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> data[[<span style="color:#e6db74">&#39;TrafficVolume&#39;</span>, <span style="color:#e6db74">&#39;Temperature&#39;</span>, <span style="color:#e6db74">&#39;WindSpeed&#39;</span>, <span style="color:#e6db74">&#39;Hour&#39;</span>, <span style="color:#e6db74">&#39;Day&#39;</span>, <span style="color:#e6db74">&#39;Month&#39;</span>]]
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;PM10&#39;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split the dataset</span>
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span></code></pre></div><h3 id="3-linear-regression-a-starting-point">3. Linear Regression: A Starting Point</h3>
<p>The first model we built was a linear regression model. This model assumes a linear relationship between the features and the target variable (PM10).</p>
<p>While simplistic, it provides a clear picture of how each variable contributes to pollution levels.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_absolute_error, mean_squared_error
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train the model</span>
</span></span><span style="display:flex;"><span>lr_model <span style="color:#f92672">=</span> LinearRegression()
</span></span><span style="display:flex;"><span>lr_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict on test data</span>
</span></span><span style="display:flex;"><span>y_pred <span style="color:#f92672">=</span> lr_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Evaluate performance</span>
</span></span><span style="display:flex;"><span>mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, y_pred)
</span></span><span style="display:flex;"><span>mse <span style="color:#f92672">=</span> mean_squared_error(y_test, y_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Linear Regression - MAE: </span><span style="color:#e6db74">{</span>mae<span style="color:#e6db74">}</span><span style="color:#e6db74">, MSE: </span><span style="color:#e6db74">{</span>mse<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Linear Regression</strong>:</p>
<ul>
<li>Traffic volume emerged as the strongest predictor, with PM10 levels spiking during rush hours.</li>
<li>Wind speed had a negative coefficient, confirming its role in dispersing pollutants.</li>
</ul>
<p>While the model performed well for general trends, it struggled with extreme values, highlighting the need for more sophisticated methods.</p>
<h3 id="4-ridge-and-lasso-regression-tackling-multicollinearity">4. Ridge and Lasso Regression: Tackling Multicollinearity</h3>
<p>When dealing with real-world data, features are often correlated (as seen in the heatmap in PART 2). This can lead to multicollinearity, where the model struggles to differentiate the effect of closely related variables.</p>
<p><strong>Ridge</strong> and <strong>Lasso</strong> regression address this issue by adding regularisation.</p>
<p>Ridge penalises large coefficients, helping the model generaliSe better.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> Ridge
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Ridge regression</span>
</span></span><span style="display:flex;"><span>ridge_model <span style="color:#f92672">=</span> Ridge(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>)
</span></span><span style="display:flex;"><span>ridge_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>ridge_pred <span style="color:#f92672">=</span> ridge_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>ridge_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, ridge_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Ridge Regression - MAE: </span><span style="color:#e6db74">{</span>ridge_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Lasso goes a step further by shrinking some coefficients to zero, effectively performing feature selection.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> Lasso
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Lasso regression</span>
</span></span><span style="display:flex;"><span>lasso_model <span style="color:#f92672">=</span> Lasso(alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>)
</span></span><span style="display:flex;"><span>lasso_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>lasso_pred <span style="color:#f92672">=</span> lasso_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>lasso_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, lasso_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Lasso Regression - MAE: </span><span style="color:#e6db74">{</span>lasso_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Regularised Models</strong>:</p>
<ul>
<li>Both models reduced over-fitting, improving generalisation on the test data.</li>
<li>Lasso identified traffic volume and hour of the day as the most influential features, while Ridge retained all features but reduced their impact.</li>
</ul>
<h3 id="5-decision-tree-regression-adding-nonlinearity">5. Decision Tree Regression: Adding Nonlinearity</h3>
<p>To capture more complex relationships, we implemented a decision tree regressor. Unlike linear models, decision trees split the data into regions and make predictions based on the average value in each region.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeRegressor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Initialise and train Decision Tree</span>
</span></span><span style="display:flex;"><span>dt_model <span style="color:#f92672">=</span> DecisionTreeRegressor(max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>dt_model<span style="color:#f92672">.</span>fit(X_train, y_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Predict and evaluate</span>
</span></span><span style="display:flex;"><span>dt_pred <span style="color:#f92672">=</span> dt_model<span style="color:#f92672">.</span>predict(X_test)
</span></span><span style="display:flex;"><span>dt_mae <span style="color:#f92672">=</span> mean_absolute_error(y_test, dt_pred)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Decision Tree Regression - MAE: </span><span style="color:#e6db74">{</span>dt_mae<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p><strong>Insights from Decision Trees</strong>:</p>
<p>The model captured nonlinear patterns, such as sudden pollution spikes during low-wind conditions.
However, the tree’s performance depended heavily on its depth—too shallow, and it missed patterns; too deep, and it overfit the training data.</p>
<h3 id="6-model-comparison-and-evaluation">6. Model Comparison and Evaluation</h3>
<p>To compare the models, we used Mean Absolute Error (MAE) as the primary metric. Lower MAE indicates better performance.
<figure><img src="/images/project8_images/eval.png">
</figure>
</p>
<p><strong>Key Takeaways</strong>:</p>
<ul>
<li>Linear regression provided a strong baseline but struggled with nonlinear patterns.</li>
<li>Ridge and Lasso improved generalisation by reducing over-fitting.</li>
<li>Decision trees excelled at capturing complex relationships but required careful tuning to avoid overfitting.</li>
</ul>
<h4 id="challenges-and-lessons-learned">Challenges and Lessons Learned</h4>
<p>Building regression models was not without its challenges:</p>
<ul>
<li><em>Feature Selection</em>: Including too many correlated features led to multicollinearity, which required regularisation techniques to resolve.</li>
<li>Nonlinear Patterns: Linear models couldn’t fully capture pollution spikes, motivating the use of decision trees and later more advanced models.</li>
<li>Over-fitting: Decision trees, while powerful, required hyperparameter tuning to strike a balance between performance and generalisation.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Regression models provided valuable insights into the factors driving PM10 levels and set the stage for more advanced machine learning approaches.</p>
<p>From identifying the key contributors like traffic and weather to tackling challenges like multicollinearity, this phase laid the groundwork for accurate air quality predictions.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and let&rsquo;s keep our planet healthy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


