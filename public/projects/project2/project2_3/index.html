<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Part 3. Uncovering Themes in Recipes with Topic Modelling. | Natasha Smith Portfolio</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Discover how topic modelling transforms unstructured recipe data into actionable insights. In this blog, I dive into the techniques of Latent Dirichlet Allocation and Non-Negative Matrix Factorisation, explaining how they uncover hidden themes in recipes—from cuisines and dietary preferences to cooking techniques."><meta name=generator content="Hugo 0.142.0"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=stylesheet href=/css/custom.css></head><body class="ma0 avenir bg-near-white"><nav class="pa3 pa4-ns flex justify-end items-center"><ul class="list flex ma0 pa0"><li class=ml3><a class="link dim dark-gray f5" href=/>Home</a></li><li class=ml3><a class="link dim dark-gray f5" href=/about/>About</a></li><li class=ml3><a class="link dim dark-gray f5" href=/projects/>Projects</a></li><li class=ml3><a class="link dim dark-gray f5" href=/contact/>Contact</a></li></ul></nav><header class=page-header style=background-image:url(/images/project2_images/pr2.jpg);background-size:cover;background-position:50%;height:400px;display:flex;align-items:center;justify-content:center;color:#fff;text-align:center><div style=background-color:rgba(0,0,0,.4);padding:1rem;border-radius:4px><h1 class="f1 athelas mt3 mb1">Part 3. Uncovering Themes in Recipes with Topic Modelling.</h1><p class=f5>Discover how topic modelling transforms unstructured recipe data into actionable insights. In this blog, I dive into the techniques of Latent Dirichlet Allocation and Non-Negative Matrix Factorisation, explaining how they uncover hidden themes in recipes—from cuisines and dietary preferences to cooking techniques.</p></div></header><main class=pb7 role=main><article class="mw8 center ph3"><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><p><figure><img src=/images/project2_images/pr2.jpg></figure><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/RecipeNLG-Topic-Modelling-and-Clustering target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Recipes are more than just lists of ingredients and instructions—they encapsulate cultural, dietary, and thematic patterns waiting to be uncovered. In the ever-growing realm of textual data, topic modelling serves as a powerful tool to discover hidden themes and insights.</p><p>In this blog, I’ll explore how topic modeling techniques, such as Latent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorisation (NMF), help us extract meaningful themes from recipes.</p><p>We’ll delve into the step-by-step process, discuss tuning and evaluation using coherence scores, and demonstrate how these methods bring latent patterns to the surface. All examples and code are drawn from the original analysis in our project.</p><h3 id=what-is-topic-modelling>What Is Topic Modelling?</h3><p>Topic modelling is an unsupervised machine learning technique used to identify themes or topics within a collection of documents.</p><p>For recipes, topics can represent categories like cuisines (e.g., Italian or Indian), dietary preferences (e.g., vegetarian, keto), or cooking methods (e.g., baking, grilling).</p><h3 id=two-of-the-most-commonly-used-methods-are>Two of the most commonly used methods are:</h3><ul><li><em>Latent Dirichlet Allocation (LDA)</em>: A probabilistic model that assumes documents are mixtures of topics and that topics are distributions over words.</li><li><em>Non-Negative Matrix Factorisation (NMF)</em>: A matrix decomposition technique that provides an additive, parts-based representation of data.</li></ul><h3 id=pre-processing-the-data>Pre-processing the Data</h3><p>Before applying topic modelling, the recipe text requires pre-processing. Here’s the sequence of steps followed:</p><ul><li><em>Tokenisation</em>: Splitting the text into individual words.</li><li><em>Removing Stop Words</em>: Filtering out common words (e.g., &ldquo;the,&rdquo; &ldquo;and&rdquo;) that don’t contribute to the analysis.</li><li><em>Lemmatisation</em>: Reducing words to their root forms (e.g., &ldquo;cooking&rdquo; → &ldquo;cook&rdquo;).</li><li><em>TF-IDF Vectorisation</em>: Converting the text into numerical format using Term Frequency-Inverse Document Frequency to weigh important terms more heavily.</li></ul><h3 id=code-example-pre-processing>Code Example: Pre-processing</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.feature_extraction.text <span style=color:#f92672>import</span> TfidfVectorizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.corpus <span style=color:#f92672>import</span> stopwords
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.tokenize <span style=color:#f92672>import</span> word_tokenize
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> nltk.stem <span style=color:#f92672>import</span> WordNetLemmatizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize lemmatizer and stop words</span>
</span></span><span style=display:flex><span>lemmatizer <span style=color:#f92672>=</span> WordNetLemmatizer()
</span></span><span style=display:flex><span>stop_words <span style=color:#f92672>=</span> set(stopwords<span style=color:#f92672>.</span>words(<span style=color:#e6db74>&#39;english&#39;</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Sample preprocessing function</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preprocess</span>(text):
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> word_tokenize(text<span style=color:#f92672>.</span>lower())
</span></span><span style=display:flex><span>    tokens <span style=color:#f92672>=</span> [lemmatizer<span style=color:#f92672>.</span>lemmatize(word) <span style=color:#66d9ef>for</span> word <span style=color:#f92672>in</span> tokens <span style=color:#66d9ef>if</span> word<span style=color:#f92672>.</span>isalnum() <span style=color:#f92672>and</span> word <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> stop_words]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join(tokens)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Apply to recipe data</span>
</span></span><span style=display:flex><span>recipes_cleaned <span style=color:#f92672>=</span> [preprocess(recipe) <span style=color:#66d9ef>for</span> recipe <span style=color:#f92672>in</span> recipes_raw]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># TF-IDF vectorization</span>
</span></span><span style=display:flex><span>vectorizer <span style=color:#f92672>=</span> TfidfVectorizer(max_df<span style=color:#f92672>=</span><span style=color:#ae81ff>0.95</span>, min_df<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, stop_words<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;english&#39;</span>)
</span></span><span style=display:flex><span>tfidf_matrix <span style=color:#f92672>=</span> vectorizer<span style=color:#f92672>.</span>fit_transform(recipes_cleaned)
</span></span></code></pre></div><h3 id=applying-topic-modelling>Applying Topic Modelling</h3><ol><li>Latent Dirichlet Allocation (LDA)
LDA assigns words to topics probabilistically. Each document (recipe) can belong to multiple topics, with a distribution over the identified themes.</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> LatentDirichletAllocation
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize and fit LDA model</span>
</span></span><span style=display:flex><span>lda <span style=color:#f92672>=</span> LatentDirichletAllocation(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>lda<span style=color:#f92672>.</span>fit(tfidf_matrix)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Display top words per topic</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>display_topics</span>(model, feature_names, no_top_words):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> topic_idx, topic <span style=color:#f92672>in</span> enumerate(model<span style=color:#f92672>.</span>components_):
</span></span><span style=display:flex><span>        top_words <span style=color:#f92672>=</span> [feature_names[i] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> topic<span style=color:#f92672>.</span>argsort()[<span style=color:#f92672>-</span>no_top_words:]]
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Topic </span><span style=color:#e6db74>{</span>topic_idx<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span><span style=color:#e6db74>&#39; &#39;</span><span style=color:#f92672>.</span>join(top_words)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>display_topics(lda, vectorizer<span style=color:#f92672>.</span>get_feature_names_out(), <span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><p>Example Output:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>Topic 0</span>: <span style=color:#ae81ff>chicken garlic onion salt pepper bake</span>
</span></span><span style=display:flex><span><span style=color:#f92672>Topic 1</span>: <span style=color:#ae81ff>chocolate sugar butter cake vanilla</span>
</span></span><span style=display:flex><span><span style=color:#f92672>Topic 2</span>: <span style=color:#ae81ff>pasta tomato basil parmesan olive</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><ol start=2><li>Non-Negative Matrix Factorisation (NMF)</li></ol><p>Unlike LDA, NMF relies on matrix decomposition to identify latent topics. It’s particularly useful when speed or interpretability is a priority.</p><h3 id=code-example-nmf-implementation>Code Example: NMF Implementation</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>Copy code
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.decomposition <span style=color:#f92672>import</span> NMF
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Initialize and fit NMF model</span>
</span></span><span style=display:flex><span>nmf <span style=color:#f92672>=</span> NMF(n_components<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>nmf<span style=color:#f92672>.</span>fit(tfidf_matrix)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Display top words per topic</span>
</span></span><span style=display:flex><span>display_topics(nmf, vectorizer<span style=color:#f92672>.</span>get_feature_names_out(), <span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><h3 id=evaluating-and-tuning-the-models>Evaluating and Tuning the Models</h3><p>Topic models require fine-tuning to balance coherence and coverage. Key steps include:</p><ul><li><em>Coherence Score</em>: Measures the interpretability of topics by evaluating the semantic similarity of top words within each topic.</li><li><em>Number of Topics (k)</em>: Experimenting with different values of k (number of topics) to identify the optimal model.</li><li><em>Hyperparameters</em>: Adjusting parameters like learning rate, topic distribution priors (LDA), or regularization (NMF).</li></ul><h3 id=code-example-calculating-coherence>Code Example: Calculating Coherence</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> gensim.models <span style=color:#f92672>import</span> CoherenceModel
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gensim.corpora <span style=color:#f92672>import</span> Dictionary
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> gensim.matutils <span style=color:#f92672>import</span> Sparse2Corpus
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Convert TF-IDF matrix to Gensim corpus</span>
</span></span><span style=display:flex><span>corpus <span style=color:#f92672>=</span> Sparse2Corpus(tfidf_matrix, documents_columns<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>dictionary <span style=color:#f92672>=</span> Dictionary<span style=color:#f92672>.</span>from_corpus(corpus, id2word<span style=color:#f92672>=</span>dict(enumerate(vectorizer<span style=color:#f92672>.</span>get_feature_names_out())))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Calculate coherence for LDA model</span>
</span></span><span style=display:flex><span>lda_coherence <span style=color:#f92672>=</span> CoherenceModel(model<span style=color:#f92672>=</span>lda, texts<span style=color:#f92672>=</span>recipes_cleaned, dictionary<span style=color:#f92672>=</span>dictionary, coherence<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;c_v&#39;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Coherence Score: </span><span style=color:#e6db74>{</span>lda_coherence<span style=color:#f92672>.</span>get_coherence()<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><h3 id=insights-from-topic-modelling>Insights from Topic Modelling</h3><p>Using LDA and NMF, the following insights emerged from the recipe data:</p><ul><li><em>Cuisine Themes</em>: Topics often aligned with distinct cuisines, such as Italian or Mexican.</li><li><em>Dietary Preferences</em>: Certain topics highlighted vegan, keto, or gluten-free recipes.</li><li><em>Cooking Techniques</em>: Methods like baking, grilling, or stir-frying emerged as recurring themes.</li></ul><p>These findings not only validated the relevance of topic modeling but also provided actionable insights for recipe categorisation, recommendation systems, and culinary trend analysis.</p><h3 id=conclusion>Conclusion</h3><p>Topic modelling offers a lens to uncover hidden themes in recipe data, transforming unstructured text into actionable insights.</p><p>Whether it’s using LDA to identify nuanced themes or NMF for faster analysis, the choice of technique depends on the specific requirements of the project.</p><p>By tuning and evaluating models with coherence scores, we ensure meaningful outputs that resonate with real-world applications.</p><p>From enhancing recommendation engines to enabling trend analysis, topic modelling has proven invaluable in understanding the culinary world.</p><p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and happy cooking!</em></p></div></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Natasha Smith Portfolio 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>