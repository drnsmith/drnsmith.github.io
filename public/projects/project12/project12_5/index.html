<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Even with balanced datasets, effective data splitting remains critical for machine learning success. This blog explores how train-test splits and validation strategies ensure reliable performance metrics, guide model optimisation, and prevent overfitting. Follow along with practical examples from the Fashion MNIST dataset.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project12_images/pr12.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines.
          </h1>
          
            <p class="f5">Even with balanced datasets, effective data splitting remains critical for machine learning success. This blog explores how train-test splits and validation strategies ensure reliable performance metrics, guide model optimisation, and prevent overfitting. Follow along with practical examples from the Fashion MNIST dataset.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project12_images/pr12.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Machine learning models are only as good as the data they’re trained on. But even the best dataset won’t save you if your data splits are flawed. Splitting data into training, validation, and test sets seems straightforward, but small mistakes can lead to big problems like overfitting, underfitting, or unreliable performance metrics.</p>
<p>You have a perfectly balanced dataset. Every class is equally represented, and it seems like splitting the data into training, testing, and validation sets should be a no-brainer. But even with balanced datasets like Fashion MNIST, thoughtful splitting is critical to ensure fair evaluation, reproducibility, and proper generalisation.</p>
<p>In this blog, I’ll walk you through my approach to splitting the Fashion MNIST dataset. We’ll cover why train-test and validation splits matter, even for balanced datasets, and how these strategies set the foundation for building reliable models.</p>
<h3 id="technical-explanation">Technical Explanation</h3>
<ul>
<li><strong>1. Train-Test Splits for Balanced Datasets</strong>
Although Fashion MNIST is inherently balanced, splitting the dataset still requires care to maintain equal representation of all classes across training and testing sets. A haphazard split could inadvertently introduce biases or create subtle imbalances due to random sampling.</li>
</ul>
<h4 id="why-train-test-splits-matter">Why Train-Test Splits Matter:</h4>
<p>The test set is your final measure of success. It should represent the dataset&rsquo;s overall distribution as closely as possible to provide reliable evaluation metrics.</p>
<p><strong>Implementation in My Project:</strong></p>
<p>Using <code>train_test_split</code> from <code>scikit-learn</code> ensured that the split maintained the original dataset&rsquo;s balance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.datasets <span style="color:#f92672">import</span> fashion_mnist
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load dataset</span>
</span></span><span style="display:flex;"><span>(images, labels), (test_images, test_labels) <span style="color:#f92672">=</span> fashion_mnist<span style="color:#f92672">.</span>load_data()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Combine the datasets for splitting</span>
</span></span><span style="display:flex;"><span>combined_images <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate((images, test_images), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>combined_labels <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate((labels, test_labels), axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train-test split (stratification not required due to balance)</span>
</span></span><span style="display:flex;"><span>train_images, test_images, train_labels, test_labels <span style="color:#f92672">=</span> train_test_split(
</span></span><span style="display:flex;"><span>    combined_images, combined_labels, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This ensures a balanced and consistent representation of all classes in both the training and testing sets.</p>
<ul>
<li><strong>2. Validation Data Splits</strong>
While the test set evaluates the final model, a validation set helps monitor the model’s generalisation during training. Without a validation split, you risk overfitting, as the model’s performance is only evaluated on training data.</li>
</ul>
<h4 id="how-validation-splits-work">How Validation Splits Work:</h4>
<p>During training, a portion of the training data is reserved for validation. The model never sees this data during training, making it a proxy for how the model generalises to unseen data.</p>
<p><strong>Implementation in My Project:</strong></p>
<p>In my Fashion MNIST pipeline, I used Keras’s <code>validation_split</code> parameter to reserve 20% of the training data for validation:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Validation split during training</span>
</span></span><span style="display:flex;"><span>history <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>fit(
</span></span><span style="display:flex;"><span>    train_images, train_labels,
</span></span><span style="display:flex;"><span>    epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>    batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>,
</span></span><span style="display:flex;"><span>    validation_split<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>This approach ensured I could track training vs validation loss and accuracy over epochs to identify overfitting early.</p>
<p><strong>Key Takeaways from My Splitting Strategy</strong></p>
<ul>
<li><em>Fair Representation Matters</em>: Even in balanced datasets, careful splitting ensures consistent performance evaluation.</li>
<li><em>Validation Guides Training</em>: A validation split helps identify overfitting or underfitting, guiding decisions like adjusting dropout rates or learning rates.</li>
<li><em>Reproducibility is Critical</em>: Consistently using a random seed (<code>random_state=42</code>) ensures reproducible splits, a cornerstone of scientific rigor.</li>
</ul>
<h3 id="real-world-applications">Real-World Applications</h3>
<h4 id="educational-benchmarks">Educational Benchmarks</h4>
<p>Fashion MNIST is often used as a benchmark for teaching machine learning. Proper data splits ensure reproducible experiments, making it easier for learners to compare their results with existing benchmarks.</p>
<h4 id="testing-generalisation-in-production">Testing Generalisation in Production</h4>
<p>In production systems, the final model needs to generalise to unseen data. Train-test splits simulate this process, ensuring the model’s robustness before deployment.</p>
<h4 id="building-reusable-pipelines">Building Reusable Pipelines</h4>
<p>By designing reproducible splits and monitoring validation performance, you create robust pipelines that can be reused across similar datasets or tasks.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Even with balanced datasets like Fashion MNIST, thoughtful data splitting is essential for building reliable machine learning pipelines.</p>
<p>Train-test splits ensure fair and consistent evaluation, while validation splits provide crucial feedback during training to guide model development.</p>
<p>In my project, these strategies helped me build a model that generalised well without overfitting, laying the foundation for robust performance.</p>
<p>When working on your next project, don’t underestimate the power of proper data splits. They might just be the unsung heroes of your machine learning pipeline.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and be trendy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


