<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification. | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Loss functions are the backbone of training neural networks. This blog unpacks the Sparse Categorical Crossentropy loss function, explaining why it’s ideal for multi-class classification tasks.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project12_images/pr12.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification.
          </h1>
          
            <p class="f5">Loss functions are the backbone of training neural networks. This blog unpacks the Sparse Categorical Crossentropy loss function, explaining why it’s ideal for multi-class classification tasks.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project12_images/pr12.jpg">
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h3 id="introduction">Introduction</h3>
<p>Choosing the right loss function is one of the most critical decisions when building a neural network. For multi-class classification tasks, like predicting clothing categories in Fashion MNIST, the sparse categorical crossentropy (SCC) loss function is often the go-to solution. But what makes it so effective?</p>
<p>This blog dives into:</p>
<ul>
<li>What sparse categorical crossentropy is and how it works.</li>
<li>Why it’s the ideal choice for tasks involving multiple classes.</li>
<li>How to implement it efficiently in TensorFlow/Keras.</li>
</ul>
<p>By the end, you’ll have a solid understanding of this loss function and when to use it in your own projects.</p>
<h3 id="technical-explanation">Technical Explanation</h3>
<h4 id="what-is-sparse-categorical-crossentropy">What is Sparse Categorical Crossentropy?</h4>
<p>SCC measures the difference between the true labels and the predicted probabilities across all classes. Unlike standard categorical crossentropy, it assumes labels are provided as integers (e.g., class indices) rather than one-hot encoded vectors.</p>
<p>The loss function is defined as:</p>
<p>[
L(y, \hat{y}) = -\frac{1}{N} \sum_{i=1}^{N} \log(\hat{y}_{i}[y_i])
]</p>
<p>Where:</p>
<ul>
<li>( N ): Number of samples in the batch.</li>
<li>( y_i ): True class index for the ( i^{th} ) sample.</li>
<li>( \hat{y}_{i}[y_i] ): Predicted probability for the true class.</li>
</ul>
<p>In simpler terms:</p>
<p>SCC calculates how far the predicted probabilities deviate from the true class. It penalises incorrect predictions more severely, pushing the model to adjust weights in the right direction.</p>
<p>SCC does not require one-hot encoded labels. Instead, it expects integer class indices, making it more memory-efficient. It’s not a binary classification loss function. For binary tasks, use binary crossentropy instead.</p>
<h4 id="why-use-sparse-categorical-crossentropy">Why Use Sparse Categorical Crossentropy?</h4>
<ul>
<li>Efficient Handling of Class Labels</li>
</ul>
<p>Sparse categorical crossentropy works directly with integer labels, saving the extra computational step of converting them into one-hot encoded vectors.
For example, instead of transforming y = [0, 2, 1] into:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>[[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>], 
</span></span><span style="display:flex;"><span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], 
</span></span><span style="display:flex;"><span> [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]]
</span></span></code></pre></div><p>You can use the original integer labels, simplifying preprocessing.</p>
<ul>
<li>Pairs Seamlessly with Softmax</li>
</ul>
<p>The loss function pairs perfectly with the softmax activation function, which outputs a probability distribution across classes. The function evaluates how well these predicted probabilities align with the true class.</p>
<ul>
<li>Focuses on Correct Class Probabilities</li>
</ul>
<p>SCC focuses only on the predicted probability for the true class, ignoring others. This keeps the training efficient and targeted.</p>
<h3 id="sparse-categorical-crossentropy-in-practice">Sparse Categorical Crossentropy in Practice</h3>
<p>In my Fashion MNIST project, this loss function was an obvious choice. Here’s the implementation in TensorFlow/Keras:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Sequential
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.layers <span style="color:#f92672">import</span> Dense, Flatten
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.optimizers <span style="color:#f92672">import</span> SGD
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define the model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Sequential([
</span></span><span style="display:flex;"><span>    Flatten(input_shape<span style="color:#f92672">=</span>(<span style="color:#ae81ff">28</span><span style="color:#f92672">*</span><span style="color:#ae81ff">28</span>,)),  <span style="color:#75715e"># Flatten the 28x28 input images</span>
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">128</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;relu&#39;</span>),  <span style="color:#75715e"># Hidden layer with ReLU activation</span>
</span></span><span style="display:flex;"><span>    Dense(<span style="color:#ae81ff">10</span>, activation<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;softmax&#39;</span>)  <span style="color:#75715e"># Output layer with softmax activation</span>
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Compile the model with sparse categorical crossentropy</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>compile(optimizer<span style="color:#f92672">=</span>SGD(learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">0.01</span>), 
</span></span><span style="display:flex;"><span>              loss<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;sparse_categorical_crossentropy&#39;</span>, 
</span></span><span style="display:flex;"><span>              metrics<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train the model</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>fit(train_images, train_labels, epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, validation_split<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><h4 id="key-differences-sparse-vs-standard-categorical-crossentropy">Key Differences: Sparse vs. Standard Categorical Crossentropy</h4>
<ul>
<li>Input Format</li>
</ul>
<p>SCC expects integer labels: [0, 1, 2].
Standard categorical crossentropy requires one-hot encoded labels: [[1, 0, 0], [0, 1, 0], [0, 0, 1]].</p>
<ul>
<li>Memory Usage</li>
</ul>
<p>SCC is more memory-efficient, especially for large datasets with many classes.</p>
<p><strong>Use Cases</strong></p>
<ul>
<li>Use SCC for datasets with class indices (like Fashion MNIST).</li>
<li>Use standard categorical crossentropy if your labels are already one-hot encoded.</li>
</ul>
<p><strong>Limitations of Sparse Categorical Crossentropy</strong>
While it’s highly effective for multi-class classification, there are a few scenarios where sparse categorical crossentropy may not be ideal:</p>
<ul>
<li>If your dataset contains highly imbalanced classes, consider adding class weights to address bias.</li>
<li>For binary classification tasks, binary crossentropy is more appropriate.</li>
</ul>
<h3 id="conclusion">Conclusion</h3>
<p>Sparse categorical crossentropy is an elegant and efficient loss function for multi-class classification tasks. Its ability to work directly with integer labels and pair seamlessly with softmax makes it an indispensable tool in any data scientist’s toolkit.</p>
<p>In my Fashion MNIST project, SCC simplified pre-processing, enabled efficient learning, and ensured the model focused on improving predictions for the correct class.</p>
<p>If you’re working on a multi-class classification problem, this loss function should be your starting point. It’s easy to implement, computationally efficient, and perfectly suited for tasks like image classification.</p>
<p><em>Feel free to explore the project on GitHub and contribute if you’re interested. Happy coding and be trendy!</em></p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


