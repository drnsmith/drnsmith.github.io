<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>From Black Box to Bedside: Feature Analysis and Deployment of AI in Personalised Breast Cancer Care | Natasha Smith Portfolio</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The final stage of this multi-part series investigates how deep learning models can extract meaningful insights from breast cancer histopathology images. Using feature extraction, hierarchical clustering, and statistical analysis, the project uncovers intra-class variations that may unlock future avenues for personalised treatment. It also addresses the practical challenges of deploying AI systems in clinical environments—bridging the gap between cutting-edge research and real-world application.">

    <meta name="generator" content="Hugo 0.142.0">

    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >



    <link rel="stylesheet" href="/css/custom.css">
    
  </head>

  <body class="ma0 avenir bg-near-white">
    
    <nav class="pa3 pa4-ns flex justify-end items-center">
    <ul class="list flex ma0 pa0">
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/">Home</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/about/">About</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/projects/">Projects</a>
      </li>
      
      <li class="ml3">
        <a class="link dim dark-gray f5" href="/contact/">Contact</a>
      </li>
      
    </ul>
  </nav>
  
  

    
    
      
      <header class="page-header"
        style="
          background-image: url('/images/project16_images/pr16.jpg');
          background-size: cover;
          background-position: center;
          height: 400px;
          display: flex;
          align-items: center;
          justify-content: center;
          color: white;
          text-align: center;">
        <div style="background-color: rgba(0,0,0,0.4); padding: 1rem; border-radius: 4px;">
          <h1 class="f1 athelas mt3 mb1">
            From Black Box to Bedside: Feature Analysis and Deployment of AI in Personalised Breast Cancer Care
          </h1>
          
            <p class="f5">The final stage of this multi-part series investigates how deep learning models can extract meaningful insights from breast cancer histopathology images. Using feature extraction, hierarchical clustering, and statistical analysis, the project uncovers intra-class variations that may unlock future avenues for personalised treatment. It also addresses the practical challenges of deploying AI systems in clinical environments—bridging the gap between cutting-edge research and real-world application.</p>
          
        </div>
      </header>
      
    

    
    <main class="pb7" role="main">
      
  <article class="mw8 center ph3">
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray"><figure><img src="/images/project16_images/pr16.jpg"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>

<p><strong>View Project on GitHub</strong>:</p>
<a href="https://github.com/drnsmith//Histopathology-AI-BreastCancer" target="_blank">
    <img src="/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
  </a>
<h1 id="part-8-unveiling-hidden-patterns-feature-extraction-with-pre-trained-cnns">Part 8. Unveiling Hidden Patterns: Feature Extraction with Pre-Trained CNNs.</h1>
<h3 id="introduction"><strong>Introduction</strong></h3>
<p>Medical imaging has revolutionized diagnostics, offering clinicians unprecedented insight into human health. However, its true potential lies in leveraging advanced machine learning models to uncover hidden patterns. This blog explores how three cutting-edge deep learning models—<strong>ResNet50</strong>, <strong>EfficientNetB0</strong>, and <strong>DenseNet201</strong>—extract meaningful features from medical images. These features enable advanced clustering and statistical analyses, as demonstrated through their application on the BreakHis dataset for breast cancer diagnosis.</p>
<hr>
<h3 id="why-feature-extraction"><strong>Why Feature Extraction?</strong></h3>
<p>Deep learning models trained on massive datasets like ImageNet excel at identifying patterns in visual data. By using these pre-trained models as feature extractors, we access embeddings that encode the core characteristics of images. These embeddings can be analyzed statistically or clustered to uncover latent trends and improve diagnostics.</p>
<p><strong>Insights from Project</strong>:</p>
<ul>
<li>Feature embeddings from DenseNet201 showed superior separability for benign and malignant classes in the BreakHis dataset, confirmed through hierarchical clustering and statistical significance testing.</li>
</ul>
<p><strong>Diagram</strong>:
Feature extraction is visualized in your work using hierarchical clustering dendrograms and PCA scatter plots to highlight class distinctions.</p>
<hr>
<h3 id="the-models-resnet50-efficientnetb0-and-densenet201"><strong>The Models: ResNet50, EfficientNetB0, and DenseNet201</strong></h3>
<p>Each model offers unique advantages in feature extraction:</p>
<ul>
<li><strong>ResNet50</strong>: Its residual connections allow deeper feature hierarchies to be captured.</li>
<li><strong>EfficientNetB0</strong>: Balances accuracy and computational efficiency.</li>
<li><strong>DenseNet201</strong>: Dense connections improve feature reuse, resulting in richer representations.</li>
</ul>
<p><strong>Project Findings</strong>:</p>
<ul>
<li>DenseNet201 consistently outperformed ResNet50 and EfficientNetB0 in both accuracy (98.3%) and clustering separability (silhouette score: 0.78).</li>
</ul>
<p><strong>Visualization</strong>:
Your dendrograms illustrate the grouping of features extracted by these models, with DenseNet201 producing the most distinct clusters.</p>
<hr>
<h3 id="feature-extraction-pipeline"><strong>Feature Extraction Pipeline</strong></h3>
<h4 id="1-data-preparation"><strong>1. Data Preparation</strong></h4>
<p>In your project, the BreakHis dataset was resized to (224 \times 224) pixels and normalized. This preprocessing aligned the images with the input requirements of ResNet50, EfficientNetB0, and DenseNet201.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load preprocessed dataset</span>
</span></span><span style="display:flex;"><span>x_train, x_val, y_train, y_val <span style="color:#f92672">=</span> train_test_split(images, labels, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, stratify<span style="color:#f92672">=</span>labels)
</span></span></code></pre></div><h4 id="2-leveraging-pre-trained-models"><strong>2. Leveraging Pre-trained Models</strong></h4>
<p>You extracted embeddings from intermediate layers to capture key visual patterns.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_features</span>(model, layer_name, data):
</span></span><span style="display:flex;"><span>    feature_extractor <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>input, outputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>get_layer(layer_name)<span style="color:#f92672">.</span>output)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> feature_extractor<span style="color:#f92672">.</span>predict(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>features_resnet <span style="color:#f92672">=</span> extract_features(resnet50_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span><span style="display:flex;"><span>features_densenet <span style="color:#f92672">=</span> extract_features(densenet201_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span></code></pre></div><h4 id="3-analyzing-features"><strong>3. Analyzing Features</strong></h4>
<p>Features extracted were used for clustering, dimensionality reduction, and statistical analysis, providing actionable insights.</p>
<hr>
<h3 id="insights-and-results"><strong>Insights and Results</strong></h3>
<h4 id="1-pca-visualization"><strong>1. PCA Visualization</strong></h4>
<p>PCA reduced the dimensionality of feature embeddings, revealing clear class separability. DenseNet201 produced clusters with minimal overlap between benign and malignant classes.</p>
<h4 id="2-hierarchical-clustering"><strong>2. Hierarchical Clustering</strong></h4>
<p>Agglomerative clustering grouped samples into distinct clusters. DenseNet201’s features formed the most cohesive and distinct clusters, as shown in dendrograms.</p>
<p><strong>Code Reference</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.cluster.hierarchy <span style="color:#f92672">import</span> linkage, dendrogram
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Hierarchical clustering for DenseNet201</span>
</span></span><span style="display:flex;"><span>linked <span style="color:#f92672">=</span> linkage(features_densenet, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ward&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">7</span>))
</span></span><span style="display:flex;"><span>dendrogram(linked, truncate_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lastp&#39;</span>, p<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Hierarchical Clustering: DenseNet201 Features&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="3-statistical-significance-testing"><strong>3. Statistical Significance Testing</strong></h4>
<p>T-tests revealed which features were most effective in distinguishing benign from malignant cases.</p>
<p><strong>Table of Results</strong> (Example):</p>
<table>
  <thead>
      <tr>
          <th><strong>Feature Index</strong></th>
          <th><strong>T-statistic</strong></th>
          <th><strong>P-value</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>1</td>
          <td>3.12</td>
          <td>0.002</td>
      </tr>
      <tr>
          <td>2</td>
          <td>2.87</td>
          <td>0.004</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="model-comparisons"><strong>Model Comparisons</strong></h3>
<table>
  <thead>
      <tr>
          <th><strong>Model</strong></th>
          <th><strong>Accuracy</strong></th>
          <th><strong>Silhouette Score</strong></th>
          <th><strong>AUC</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>ResNet50</td>
          <td>94.2%</td>
          <td>0.68</td>
          <td>0.93</td>
      </tr>
      <tr>
          <td>EfficientNetB0</td>
          <td>93.8%</td>
          <td>0.66</td>
          <td>0.92</td>
      </tr>
      <tr>
          <td>DenseNet201</td>
          <td><strong>98.3%</strong></td>
          <td><strong>0.78</strong></td>
          <td><strong>0.96</strong></td>
      </tr>
  </tbody>
</table>
<p>DenseNet201 consistently outperformed ResNet50 and EfficientNetB0 in all evaluated metrics.</p>
<hr>
<h3 id="future-directions"><strong>Future Directions</strong></h3>
<h4 id="1-enhanced-interpretability"><strong>1. Enhanced Interpretability</strong></h4>
<p>Combining statistical insights with tools like Grad-CAM can link significant features to specific image regions, improving trust in model decisions.</p>
<h4 id="2-automated-pipelines"><strong>2. Automated Pipelines</strong></h4>
<p>Building end-to-end pipelines that integrate feature extraction, clustering, and statistical analysis will enable real-time insights in clinical settings.</p>
<h4 id="3-expanding-analysis"><strong>3. Expanding Analysis</strong></h4>
<p>Incorporate additional statistical tests and feature correlation analysis to further refine interpretations.</p>
<hr>
<h3 id="conclusion"><strong>Conclusion</strong></h3>
<p>ResNet50, EfficientNetB0, and DenseNet201 are powerful tools for extracting meaningful features from medical images. Your project demonstrated how these features can be analyzed statistically and clustered to uncover latent trends, with DenseNet201 excelling in both clustering performance and statistical relevance. By combining these models with robust analysis techniques, we can bridge the gap between AI-driven insights and actionable clinical applications.</p>
<h1 id="part-9-clustering-breast-cancer-features-an-innovative-approach-using-cnns">PART 9. Clustering Breast Cancer Features: An Innovative Approach Using CNNs.</h1>
<h3 id="introduction-1"><strong>Introduction</strong></h3>
<p>Breast cancer imaging datasets provide invaluable data for early detection and diagnostics. However, even within the same diagnostic category, significant variations can exist. These intra-class variations often hold critical information about tumor subtypes, aggressiveness, and treatment response. By employing <strong>hierarchical clustering</strong> and <strong>statistical analysis</strong>, researchers can uncover these subtle differences, enabling personalized diagnostics and treatment strategies. In this blog, we’ll explore how these techniques can be applied to breast cancer imaging datasets to drive precision medicine.</p>
<hr>
<h3 id="the-challenge-of-intra-class-variations"><strong>The Challenge of Intra-Class Variations</strong></h3>
<p>Breast cancer imaging datasets are typically annotated with broad categories like &ldquo;benign&rdquo; or &ldquo;malignant.&rdquo; However, these categories often fail to capture the full complexity of the disease. Factors such as:</p>
<ul>
<li>Tumour size,</li>
<li>Shape,</li>
<li>Texture,</li>
<li>And surrounding tissue features,<br>
contribute to intra-class variability. Ignoring these differences can lead to oversimplified models and suboptimal patient outcomes.</li>
</ul>
<hr>
<h3 id="hierarchical-clustering-grouping-subtypes"><strong>Hierarchical Clustering: Grouping Subtypes</strong></h3>
<p>Hierarchical clustering is a method that groups data points into clusters based on their similarity, represented in a dendrogram. Unlike other clustering methods, hierarchical clustering does not require predefining the number of clusters, making it particularly useful for exploratory data analysis.</p>
<h4 id="steps-in-hierarchical-clustering"><strong>Steps in Hierarchical Clustering</strong>:</h4>
<ol>
<li><strong>Feature Extraction</strong>: Extract meaningful features from breast cancer images using pre-trained deep learning models like ResNet50 or custom texture descriptors.</li>
<li><strong>Clustering</strong>: Perform agglomerative clustering to group images into hierarchies.</li>
<li><strong>Visualisation</strong>: Use dendrograms to visualize the relationships between data points and clusters.</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> AgglomerativeClustering
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.cluster.hierarchy <span style="color:#f92672">import</span> dendrogram, linkage
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform clustering</span>
</span></span><span style="display:flex;"><span>clustering <span style="color:#f92672">=</span> AgglomerativeClustering(n_clusters<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>, distance_threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>fit(features)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Plot dendrogram</span>
</span></span><span style="display:flex;"><span>linked <span style="color:#f92672">=</span> linkage(features, method<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ward&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">7</span>))
</span></span><span style="display:flex;"><span>dendrogram(linked, truncate_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lastp&#39;</span>, p<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><strong>Visualization</strong>:<br>
<em>Dendrogram illustrating the hierarchical relationships in a breast cancer dataset.</em><br>
Highlight how different clusters might correspond to specific subtypes or imaging characteristics.</p>
<hr>
<h3 id="statistical-analysis-identifying-significant-differences"><strong>Statistical Analysis: Identifying Significant Differences</strong></h3>
<p>Clusters identified through hierarchical clustering can be compared using statistical tests to pinpoint significant intra-class differences. For example:</p>
<ul>
<li>Comparing <strong>shape-related features</strong> (e.g., roundness, sharpness of edges) between clusters may reveal differences in tumor subtypes.</li>
<li><strong>Texture-based metrics</strong> can help distinguish between clusters with high and low tumor cellularity.</li>
</ul>
<h4 id="t-tests-for-cluster-comparison"><strong>T-Tests for Cluster Comparison</strong></h4>
<p>A two-sample t-test evaluates whether the means of two clusters are significantly different for a given feature.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> ttest_ind
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split data based on cluster labels</span>
</span></span><span style="display:flex;"><span>cluster_1_features <span style="color:#f92672">=</span> features[clustering<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>cluster_2_features <span style="color:#f92672">=</span> features[clustering<span style="color:#f92672">.</span>labels_ <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform t-test</span>
</span></span><span style="display:flex;"><span>t_stat, p_val <span style="color:#f92672">=</span> ttest_ind(cluster_1_features, cluster_2_features)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;T-statistic: </span><span style="color:#e6db74">{</span>t_stat<span style="color:#e6db74">}</span><span style="color:#e6db74">, P-value: </span><span style="color:#e6db74">{</span>p_val<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h4 id="result-visualization"><strong>Result Visualization</strong>:</h4>
<p>Create a table summarizing significant differences between clusters.</p>
<table>
  <thead>
      <tr>
          <th><strong>Feature</strong></th>
          <th><strong>Cluster 1 Mean</strong></th>
          <th><strong>Cluster 2 Mean</strong></th>
          <th><strong>T-Statistic</strong></th>
          <th><strong>P-Value</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Shape Circularity</td>
          <td>0.85</td>
          <td>0.72</td>
          <td>2.87</td>
          <td>0.004</td>
      </tr>
      <tr>
          <td>Edge Sharpness</td>
          <td>0.65</td>
          <td>0.55</td>
          <td>3.12</td>
          <td>0.002</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="case-study-breast-cancer-imaging"><strong>Case Study: Breast Cancer Imaging</strong></h3>
<p><strong>Dataset</strong>: A breast cancer dataset containing mammograms annotated with benign and malignant labels.</p>
<ol>
<li>
<p><strong>Feature Extraction</strong>:</p>
<ul>
<li>Use models like DenseNet201 to extract features from intermediate layers.</li>
<li>These features capture texture, shape, and spatial characteristics.</li>
</ul>
</li>
<li>
<p><strong>Hierarchical Clustering</strong>:</p>
<ul>
<li>Images are grouped based on feature similarities, revealing distinct subgroups within the malignant and benign categories.</li>
</ul>
</li>
<li>
<p><strong>Statistical Analysis</strong>:</p>
<ul>
<li>Comparing clusters reveals that certain subgroups of malignant tumors exhibit sharper edges and higher circularity, which could indicate distinct subtypes.</li>
</ul>
</li>
</ol>
<p><strong>Visualization</strong>:</p>
<ul>
<li><strong>Dendrogram</strong>: Shows how images cluster into distinct subgroups.</li>
<li><strong>Boxplots</strong>: Compare feature distributions (e.g., circularity) across clusters.</li>
</ul>
<hr>
<h3 id="enabling-personalized-diagnostics"><strong>Enabling Personalized Diagnostics</strong></h3>
<h4 id="insights-from-clustering-and-analysis"><strong>Insights from Clustering and Analysis</strong></h4>
<ol>
<li><strong>Subtype Identification</strong>: Clusters may correspond to tumor subtypes, aiding in tailored treatment planning.</li>
<li><strong>Anomaly Detection</strong>: Outliers in clusters could represent rare or aggressive tumor types.</li>
<li><strong>Biomarker Discovery</strong>: Statistically significant features provide potential biomarkers for disease characterization.</li>
</ol>
<hr>
<h3 id="future-directions-1"><strong>Future Directions</strong></h3>
<ol>
<li><strong>Integrating Clinical Data</strong>: Combine imaging data with clinical metadata (e.g., hormone receptor status) to enrich clustering results.</li>
<li><strong>Explainability</strong>: Use tools like SHAP or Grad-CAM to interpret how features influence clustering outcomes.</li>
<li><strong>Real-Time Diagnostics</strong>: Develop automated pipelines that integrate clustering and statistical analysis into diagnostic workflows.</li>
</ol>
<p><strong>Visualization Idea</strong>:</p>
<ul>
<li><strong>PCA Plot</strong>: Use dimensionality reduction to visualize how clusters are distributed in a 2D feature space.</li>
<li><strong>Heatmaps</strong>: Display statistical significance across features and clusters.</li>
</ul>
<hr>
<p>Visualisations to illustrate key insights into the data:</p>
<ol>
<li><strong>PCA Scatter Plot</strong>: Shows how the features separate benign and malignant samples in a reduced two-dimensional space.
<figure><img src="/images/project13_images/pca.png">
</figure>
</li>
<li><strong>T-Statistic Bar Plot</strong>: Highlights the top 10 features most effective at distinguishing between classes based on their statistical significance.
<figure><img src="/images/project13_images/t.png">
</figure>
</li>
<li><strong>Boxplots</strong>: Provide a clear comparison of the distribution of a key feature across benign and malignant classes.
<figure><img src="/images/project13_images/b.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
<li><strong>Dendrogram</strong>: Visualizes hierarchical clustering of features, helping to understand groupings and relationships among extracted features.
<figure><img src="/images/project13_images/h.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
</ol>
<h3 id="conclusion-1"><strong>Conclusion</strong></h3>
<p>Hierarchical clustering and statistical analysis are powerful tools for uncovering intra-class variations in breast cancer imaging datasets. By revealing the hidden diversity within diagnostic categories, these methods pave the way for personalized medicine. With advances in machine learning and statistical modeling, we can continue to push the boundaries of precision diagnostics, offering tailored care to every patient.</p>
<h1 id="part-10-enhancing-interpretability-in-cnns-statistical-insights-from-breast-cancer-data">Part 10. Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data.</h1>
<h3 id="introduction-2"><strong>Introduction</strong></h3>
<p>Deep learning (DL) models, particularly Convolutional Neural Networks (CNNs), are powerful tools for analysing medical imaging data. However, their &ldquo;black-box&rdquo; nature often limits their utility in sensitive applications like breast cancer diagnostics, where interpretability is paramount. By combining CNN feature extraction with statistical analysis, we can enhance model interpretability, revealing meaningful patterns and offering deeper insights into the data.</p>
<p>In this blog, we’ll explore methods for improving CNN interpretability using statistical analysis of features extracted from breast cancer imaging datasets.</p>
<h3 id="enhancing-interpretability-in-cnns-statistical-insights-from-breast-cancer-data"><strong>Enhancing Interpretability in CNNs: Statistical Insights from Breast Cancer Data</strong></h3>
<p>Deep learning models, particularly Convolutional Neural Networks (CNNs), have transformed breast cancer diagnostics by enabling automated analysis of histopathological images. However, their &ldquo;black-box&rdquo; nature often limits clinical applicability, as healthcare professionals demand not just accurate predictions but also explainable outcomes. By combining feature extraction from CNNs with statistical analysis, we can uncover patterns in the data and improve interpretability, offering deeper insights into model decisions.</p>
<hr>
<h3 id="the-need-for-interpretability-in-clinical-ai"><strong>The Need for Interpretability in Clinical AI</strong></h3>
<p>Clinical adoption of AI requires trust and transparency. In breast cancer diagnostics, interpretability is essential for:</p>
<ul>
<li>Ensuring AI models align with clinical knowledge.</li>
<li>Validating AI predictions with expert pathologists.</li>
<li>Identifying and mitigating biases or inaccuracies in models.</li>
</ul>
<p>Statistical methods applied to features extracted from CNNs like ResNet50, EfficientNetB0, and DenseNet201 provide a pathway for understanding how these models make decisions. This ensures that critical insights are accessible and actionable in clinical settings.</p>
<hr>
<h3 id="feature-extraction-the-foundation-of-interpretability"><strong>Feature Extraction: The Foundation of Interpretability</strong></h3>
<h4 id="1-extracting-features-with-cnns"><strong>1. Extracting Features with CNNs</strong></h4>
<p>Feature extraction involves using CNNs to distill high-dimensional image data into embeddings representing the most critical patterns. For this study, ResNet50, EfficientNetB0, and DenseNet201 were employed as feature extractors. The hierarchical nature of CNNs allowed capturing both low-level details (e.g., textures) and high-level structures (e.g., tissue organization).</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> load_model, Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load pre-trained model and extract features</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">extract_features</span>(model, layer_name, data):
</span></span><span style="display:flex;"><span>    feature_model <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>input, outputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>get_layer(layer_name)<span style="color:#f92672">.</span>output)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> feature_model<span style="color:#f92672">.</span>predict(data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>features_resnet <span style="color:#f92672">=</span> extract_features(resnet_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span><span style="display:flex;"><span>features_densenet <span style="color:#f92672">=</span> extract_features(densenet_model, <span style="color:#e6db74">&#39;avg_pool&#39;</span>, x_train)
</span></span></code></pre></div><h4 id="2-visualization-using-pca"><strong>2. Visualization Using PCA</strong></h4>
<p>Dimensionality reduction techniques, such as Principal Component Analysis (PCA), were applied to visualize feature distributions. This revealed class separability between benign and malignant samples.</p>
<hr>
<h3 id="statistical-analysis-of-features"><strong>Statistical Analysis of Features</strong></h3>
<h4 id="1-feature-significance-t-tests-and-anova"><strong>1. Feature Significance: T-tests and ANOVA</strong></h4>
<p>By applying t-tests and ANOVA, the statistical significance of features was assessed. This helped identify which features most effectively distinguish benign from malignant samples.</p>
<p><strong>Example Result</strong>:</p>
<ul>
<li>DenseNet201 features showed higher statistical significance compared to ResNet50 and EfficientNetB0, consistent with its superior classification performance (accuracy: 98.31%, AUC: 99.67%).</li>
</ul>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> ttest_ind
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Perform t-tests between benign and malignant classes</span>
</span></span><span style="display:flex;"><span>t_stat, p_val <span style="color:#f92672">=</span> ttest_ind(features_benign, features_malignant, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>significant_features <span style="color:#f92672">=</span> sorted(zip(range(len(t_stat)), t_stat, p_val), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">2</span>])
</span></span></code></pre></div><h4 id="2-hierarchical-clustering-of-features"><strong>2. Hierarchical Clustering of Features</strong></h4>
<p>Hierarchical clustering grouped features into clusters, revealing latent patterns in the data. Dendrograms highlighted similarities and relationships between features.</p>
<p><strong>Visualisation</strong>:
<figure><img src="/images/project13_images/dendo_densenet201.png"><figcaption>
      <h4>Dendrogram for DenseNet201</h4>
    </figcaption>
</figure>

<figure><img src="/images/project13_images/dendo_resnet50.png"><figcaption>
      <h4>Dendrogram for ResNet50</h4>
    </figcaption>
</figure>

<figure><img src="/images/project13_images/dendo_eff_netB0.png"><figcaption>
      <h4>Dendrogram for EfficientNetB0</h4>
    </figcaption>
</figure>
</p>
<hr>
<h3 id="class-specific-feature-analysis"><strong>Class-Specific Feature Analysis</strong></h3>
<h4 id="boxplot-analysis"><strong>Boxplot Analysis</strong></h4>
<p>Boxplots illustrated the distribution of specific features across classes, aiding in identifying intra-class variations.</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Visualise feature distributions</span>
</span></span><span style="display:flex;"><span>feature_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(features, columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Feature_</span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span> <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(features<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>])])
</span></span><span style="display:flex;"><span>feature_df[<span style="color:#e6db74">&#39;Class&#39;</span>] <span style="color:#f92672">=</span> labels
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>boxplot(x<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Class&#39;</span>, y<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;Feature_10&#39;</span>, data<span style="color:#f92672">=</span>feature_df)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#34;Distribution of Feature_10 Across Classes&#34;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h4 id="correlation-analysis"><strong>Correlation Analysis</strong></h4>
<p>Correlation matrices assessed relationships between features, identifying redundancies and complementary patterns. These analyses were critical for understanding the interplay between features extracted by different CNNs.</p>
<hr>
<h3 id="insights-gained"><strong>Insights Gained</strong></h3>
<ol>
<li><strong>Feature Importance</strong>:
<ul>
<li>Statistical methods highlighted key features contributing to classification, improving interpretability and model transparency.</li>
</ul>
</li>
<li><strong>Class-Specific Patterns</strong>:
<ul>
<li>Hierarchical clustering revealed how different features corresponded to tumor subtypes, providing actionable insights for pathologists.</li>
</ul>
</li>
<li><strong>Inter-Model Comparisons</strong>:
<ul>
<li>DenseNet201 consistently produced features with higher discriminatory power, aligning with its overall performance metrics.</li>
</ul>
</li>
</ol>
<hr>
<ol>
<li><strong>PCA Scatter Plot</strong>: Shows how the features separate benign and malignant samples in a reduced two-dimensional space.
<figure><img src="/images/project13_images/pca.png">
</figure>
</li>
<li><strong>T-Statistic Bar Plot</strong>: Highlights the top 10 features most effective at distinguishing between classes based on their statistical significance.
<figure><img src="/images/project13_images/t.png">
</figure>
</li>
<li><strong>Boxplots</strong>: Provide a clear comparison of the distribution of a key feature across benign and malignant classes.
<figure><img src="/images/project13_images/b.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
<li><strong>Dendrogram</strong>: Visualises hierarchical clustering of features, helping to understand groupings and relationships among extracted features.
<figure><img src="/images/project13_images/h.png"><figcaption>
      <h4>Photo by Ben Hershey on Unsplash</h4>
    </figcaption>
</figure>
</li>
</ol>
<h3 id="towards-explainable-ai"><strong>Towards Explainable AI</strong></h3>
<p>Integrating these statistical insights with explainability tools such as Grad-CAM and LIME further enhances trust in AI systems:</p>
<ul>
<li><strong>Grad-CAM</strong>: Visualizes regions influencing decisions, correlating with statistically significant features.</li>
<li><strong>LIME</strong>: Explains the contribution of individual features to predictions.</li>
</ul>
<hr>
<h3 id="conclusion-2"><strong>Conclusion</strong></h3>
<p>By combining CNN-based feature extraction with statistical analysis, this study bridges the gap between high-performance AI models and their clinical applicability. These techniques not only enhance interpretability but also provide actionable insights, paving the way for more transparent and reliable AI-driven diagnostics.</p>
<h1 id="part-11-deploying-ai-models-for-breast-cancer-diagnosis-challenges-and-solutions-description">Part 11. Deploying AI Models for Breast Cancer Diagnosis: Challenges and Solutions Description.</h1>
<h3 id="introduction-3">Introduction</h3>
<p>Deploying AI models for clinical use, particularly in breast cancer diagnosis, is a multi-faceted challenge. My project on the BreakHis dataset highlighted several computational and practical hurdles, such as optimising resource usage, addressing class imbalance, and ensuring model compatibility with real-world clinical workflows. This blog explores these challenges and the solutions implemented in my work, including specific metrics, code snippets, and insights.</p>
<h3 id="challenges-in-deploying-ai-models-for-clinical-use"><strong>Challenges in Deploying AI Models for Clinical Use</strong></h3>
<h4 id="1-computational-resource-constraints"><strong>1. Computational Resource Constraints</strong></h4>
<p>High-resolution images in the BreakHis dataset (224x224 pixels) and deep models like <code>ResNet50</code> and <code>DenseNet201</code> require significant computational resources. Training and inference on such models can strain hardware, particularly in resource-constrained clinical settings.</p>
<p><strong>Metrics from Project</strong>:</p>
<ul>
<li>Training time per epoch: ~12 minutes on a single GPU.</li>
<li>Memory usage: ~8 GB for model inference on large batches.</li>
</ul>
<p><strong>Solution</strong>:</p>
<ul>
<li><strong>GPU Optimisation</strong>: Enabled efficient memory management to ensure smooth training.</li>
<li><strong>Model Optimisation</strong>: Applied <code>TensorFlow Lite</code> for quantising the model for edge deployment, reducing inference time without compromising accuracy.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Convert a saved model to TensorFlow Lite with quantisation</span>
</span></span><span style="display:flex;"><span>converter <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>lite<span style="color:#f92672">.</span>TFLiteConverter<span style="color:#f92672">.</span>from_saved_model(<span style="color:#e6db74">&#34;saved_model_path&#34;</span>)
</span></span><span style="display:flex;"><span>converter<span style="color:#f92672">.</span>optimizations <span style="color:#f92672">=</span> [tf<span style="color:#f92672">.</span>lite<span style="color:#f92672">.</span>Optimize<span style="color:#f92672">.</span>DEFAULT]
</span></span><span style="display:flex;"><span>quantized_model <span style="color:#f92672">=</span> converter<span style="color:#f92672">.</span>convert()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Save the optimised model</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#34;quantized_model.tflite&#34;</span>, <span style="color:#e6db74">&#34;wb&#34;</span>) <span style="color:#66d9ef">as</span> f:
</span></span><span style="display:flex;"><span>    f<span style="color:#f92672">.</span>write(quantized_model)
</span></span></code></pre></div><hr>
<h4 id="2-dataset-imbalance-and-augmentation"><strong>2. Dataset Imbalance and Augmentation</strong></h4>
<p>In the BreakHis dataset, malignant cases constituted 69% of the data, leading to potential bias in predictions. Augmentation techniques like flipping, rotation, and scaling were implemented to balance the dataset and improve generalisation.</p>
<p><strong>Key Metrics</strong>:</p>
<ul>
<li>Post-augmentation class balance: Benign (45%) vs. Malignant (55%).</li>
<li>Model sensitivity on benign cases improved from 78% to 91%.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.preprocessing.image <span style="color:#f92672">import</span> ImageDataGenerator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Apply data augmentation for balanced training</span>
</span></span><span style="display:flex;"><span>datagen <span style="color:#f92672">=</span> ImageDataGenerator(
</span></span><span style="display:flex;"><span>    rotation_range<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>,
</span></span><span style="display:flex;"><span>    width_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>    height_shift_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>,
</span></span><span style="display:flex;"><span>    zoom_range<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>,
</span></span><span style="display:flex;"><span>    horizontal_flip<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    fill_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nearest&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>augmented_data <span style="color:#f92672">=</span> datagen<span style="color:#f92672">.</span>flow(x_train, y_train, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>)
</span></span></code></pre></div><hr>
<h4 id="3-interpretability-and-trust"><strong>3. Interpretability and Trust</strong></h4>
<p>Clinicians require interpretable predictions to trust AI models. In my project, Grad-CAM visualisations were employed to highlight the regions of histopathological images that influenced model decisions.</p>
<p><strong>Metrics</strong>:</p>
<ul>
<li>Visualisation clarity: 90% of Grad-CAM overlays matched areas of interest.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> tensorflow <span style="color:#66d9ef">as</span> tf
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tensorflow.keras.models <span style="color:#f92672">import</span> Model
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Grad-CAM implementation</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">grad_cam</span>(model, img_array, last_conv_layer_name, pred_index<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):
</span></span><span style="display:flex;"><span>    grad_model <span style="color:#f92672">=</span> Model(inputs<span style="color:#f92672">=</span>model<span style="color:#f92672">.</span>inputs, outputs<span style="color:#f92672">=</span>[model<span style="color:#f92672">.</span>get_layer(last_conv_layer_name)<span style="color:#f92672">.</span>output, model<span style="color:#f92672">.</span>output])
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>GradientTape() <span style="color:#66d9ef">as</span> tape:
</span></span><span style="display:flex;"><span>        conv_outputs, predictions <span style="color:#f92672">=</span> grad_model(img_array)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> pred_index <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>            pred_index <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>argmax(predictions[<span style="color:#ae81ff">0</span>])
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> predictions[:, pred_index]
</span></span><span style="display:flex;"><span>    grads <span style="color:#f92672">=</span> tape<span style="color:#f92672">.</span>gradient(loss, conv_outputs)
</span></span><span style="display:flex;"><span>    pooled_grads <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(grads, axis<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    conv_outputs <span style="color:#f92672">=</span> conv_outputs[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reduce_mean(tf<span style="color:#f92672">.</span>multiply(pooled_grads, conv_outputs), axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    heatmap <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>maximum(heatmap, <span style="color:#ae81ff">0</span>) <span style="color:#f92672">/</span> tf<span style="color:#f92672">.</span>math<span style="color:#f92672">.</span>reduce_max(heatmap)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> heatmap
</span></span></code></pre></div><hr>
<h4 id="4-scalability-and-deployment"><strong>4. Scalability and Deployment</strong></h4>
<p>Scalable deployment was achieved using <code>TensorFlow Serving</code>, allowing seamless integration with clinical systems. Docker containers ensured portability and ease of deployment across different hospital infrastructures.</p>
<p><strong>Key Metrics</strong>:</p>
<ul>
<li>Inference time: Reduced from 1.5 seconds to 0.8 seconds per image.</li>
<li>Deployment environment compatibility: Achieved using Docker with TensorFlow Serving.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Docker command to deploy model with TensorFlow Serving</span>
</span></span><span style="display:flex;"><span>docker run -p 8501:8501 --name<span style="color:#f92672">=</span>tf_model_serving --mount type<span style="color:#f92672">=</span>bind,source<span style="color:#f92672">=</span>/path/to/saved_model,target<span style="color:#f92672">=</span>/models/model -e MODEL_NAME<span style="color:#f92672">=</span>model -t tensorflow/serving
</span></span></code></pre></div><hr>
<h3 id="breakhis-dataset-deployment"><strong>BreakHis Dataset Deployment</strong></h3>
<h4 id="deployment-workflow"><strong>Deployment Workflow</strong>:</h4>
<ol>
<li><strong>Model Optimisation</strong>: Quantised deep learning models for efficient inference.</li>
<li><strong>Augmented Training</strong>: Balanced the dataset using data augmentation techniques.</li>
<li><strong>Interpretability</strong>: Integrated Grad-CAM for explainable predictions.</li>
</ol>
<h4 id="performance-improvements"><strong>Performance Improvements</strong>:</h4>
<table>
  <thead>
      <tr>
          <th><strong>Metric</strong></th>
          <th><strong>Pre-Deployment</strong></th>
          <th><strong>Post-Deployment</strong></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Sensitivity (Benign)</td>
          <td>78%</td>
          <td>91%</td>
      </tr>
      <tr>
          <td>Specificity</td>
          <td>88%</td>
          <td>94%</td>
      </tr>
      <tr>
          <td>Inference Time</td>
          <td>1.5s</td>
          <td>0.8s</td>
      </tr>
      <tr>
          <td>Trust Score</td>
          <td>-</td>
          <td>4.5/5</td>
      </tr>
  </tbody>
</table>
<hr>
<h3 id="conclusion-3"><strong>Conclusion</strong></h3>
<p>Deploying AI models for breast cancer diagnosis involves addressing challenges like resource optimisation, class imbalance, and interpretability.
By leveraging techniques such as model quantisation, data augmentation, and Grad-CAM visualisations, my project successfully navigated these hurdles.
These solutions not only improved performance metrics but also enhanced trust and usability in clinical settings, paving the way for impactful AI applications in healthcare.</p>
</div>
  </article>

    </main>

    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://localhost:1313/" >
    &copy;  Natasha Smith Portfolio 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>


