<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Natasha Smith Portfolio</title><link>http://localhost:1313/tags/home/</link><description>Recent content in Home on Natasha Smith Portfolio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 12 May 2023 10:58:08 -0400</lastBuildDate><atom:link href="http://localhost:1313/tags/home/index.xml" rel="self" type="application/rss+xml"/><item><title>PART 4. Addressing Class Imbalance in Medical Image Datasets. Tackling Class Imbalance in Histopathology: Strategies and Insights</title><link>http://localhost:1313/projects/project10/project10_4/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_4/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>In medical imaging datasets like histopathology, class imbalance is a common and critical challenge. For instance, datasets may contain significantly more benign samples than malignant ones, making it harder for models to learn to detect the minority class accurately. This can lead to poor sensitivity (recall), which is especially problematic in healthcare where identifying true positives is critical.&lt;/p>
&lt;p>In this blog, we explore:&lt;/p></description></item><item><title>PART 5. Evaluation and Calibration: Building Trust in Medical AI Models</title><link>http://localhost:1313/projects/project10/project10_5/</link><pubDate>Fri, 12 May 2023 10:58:08 -0400</pubDate><guid>http://localhost:1313/projects/project10/project10_5/</guid><description>&lt;p>&lt;figure>&lt;img src="http://localhost:1313/images/project10_images/pr10.jpg">
&lt;/figure>

&lt;strong>View Project on GitHub&lt;/strong>:&lt;/p>
&lt;a href="https://github.com/drnsmith/ColourNorm-Histopathology-DeepLearning" target="_blank">
 &lt;img src="http://localhost:1313/images/github.png" alt="GitHub" style="width:40px; height:40px; vertical-align: middle;">
 &lt;/a>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>Deep learning models are increasingly used in critical domains like healthcare. However, high accuracy alone doesn’t guarantee a model’s reliability.&lt;/p>
&lt;p>For medical AI systems, evaluation and calibration are key to building trust, ensuring fair predictions, and avoiding costly mistakes.&lt;/p>
&lt;p>In this blog, we’ll explore:&lt;/p>
&lt;ul>
&lt;li>The importance of model calibration.&lt;/li>
&lt;li>Key metrics: &lt;strong>F1-score&lt;/strong>, &lt;strong>Brier score loss&lt;/strong>, &lt;strong>ROC-AUC&lt;/strong>, and &lt;strong>confusion matrices&lt;/strong>.&lt;/li>
&lt;li>How to visualise and measure calibration using calibration curves.&lt;/li>
&lt;/ul>
&lt;h3 id="why-model-calibration-and-evaluation-matter">Why Model Calibration and Evaluation Matter&lt;/h3>
&lt;p>Medical imaging models often predict probabilities (e.g., &amp;ldquo;90% chance of malignancy&amp;rdquo;). But probability alone isn’t useful unless it reflects reality. For instance:&lt;/p></description></item></channel></rss>