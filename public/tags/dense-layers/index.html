<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Dense Layers | Natasha Smith Portfolio</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.142.0"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=stylesheet href=/css/custom.css></head><body class="ma0 avenir bg-near-white"><nav class="pa3 pa4-ns flex justify-end items-center"><ul class="list flex ma0 pa0"><li class=ml3><a class="link dim dark-gray f5" href=/>Home</a></li><li class=ml3><a class="link dim dark-gray f5" href=/about/>About</a></li><li class=ml3><a class="link dim dark-gray f5" href=/projects/>Projects</a></li><li class=ml3><a class="link dim dark-gray f5" href=/contact/>Contact</a></li></ul></nav><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"><p>Below you will find pages that utilize the taxonomy term “Dense Layers”</p></div></article><div class="mw8 center"><section class="flex-ns flex-wrap justify-around mt5"><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 1. Decoding Fashion MNIST: A Modern Benchmark for Deep Learning." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_1/ class="link black dim">Part 1. Decoding Fashion MNIST: A Modern Benchmark for Deep Learning.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>You’ve seen it before: the iconic handwritten digits from the MNIST dataset, the quintessential benchmark for machine learning enthusiasts.</p><p>But here’s the thing—MNIST is old news. It’s solved, overused, and no longer representative of real-world challenges. Fashion MNIST is a modern, robust alternative that brings fresh complexity to the table.</p><p>Fashion MNIST is a game-changer. With its focus on apparel images like shirts, sneakers, and dresses, it mirrors the kind of messy, nuanced data we deal with today.</p></div><a href=/projects/project12/project12_1/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 2. Designing Dense Neural Networks: Lessons from Fashion MNIST." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_2/ class="link black dim">Part 2. Designing Dense Neural Networks: Lessons from Fashion MNIST.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Designing neural networks (NNs) is an art as much as it is a science. When faced with the challenge of classifying Fashion MNIST images, I needed a lightweight yet powerful architecture to handle the complexity of apparel images. Dense neural networks, with their fully connected layers, were the perfect choice for this task.</p><p>In this blog, we’ll walk through:</p><ul><li>How dense neural networks work and their role in image classification.</li><li>Designing an efficient architecture using activation functions and layers.</li><li>Practical lessons learned while optimising performance with Fashion MNIST.</li></ul><p>By the end, you’ll have a clear roadmap to build your own dense networks for image classification tasks.
Let’s dive in!</p></div><a href=/projects/project12/project12_2/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 3. ReLU vs Sigmoid: Which Activation Function Wins on Fashion MNIST?" class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_3/ class="link black dim">Part 3. ReLU vs Sigmoid: Which Activation Function Wins on Fashion MNIST?</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>When building neural networks (NNs), the activation function you choose can make or break your model. It’s the part of the network that decides whether a neuron &ldquo;fires&rdquo; and passes information forward.</p><p>For years, Sigmoid was the go-to activation function, but then ReLU came along, revolutionising deep learning with its simplicity and effectiveness. But how do these activation functions stack up against each other in practice?</p></div><a href=/projects/project12/project12_3/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 4. Hyperparameter Tuning for Neural Networks: The Fashion MNIST Approach." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_4/ class="link black dim">Part 4. Hyperparameter Tuning for Neural Networks: The Fashion MNIST Approach.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>When training neural networks (NNs), every parameter matters. Hyperparameters like learning rate and batch size aren’t learned by the model—they’re chosen by you. These settings can make or break your model’s performance. But how do you find the right combination?</p><p>In this blog, I’ll take you through my experience fine-tuning hyperparameters for a NN trained on the Fashion MNIST dataset. We’ll cover:</p></div><a href=/projects/project12/project12_4/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_5/ class="link black dim">Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Machine learning models are only as good as the data they’re trained on. But even the best dataset won’t save you if your data splits are flawed. Splitting data into training, validation, and test sets seems straightforward, but small mistakes can lead to big problems like overfitting, underfitting, or unreliable performance metrics.</p><p>You have a perfectly balanced dataset. Every class is equally represented, and it seems like splitting the data into training, testing, and validation sets should be a no-brainer. But even with balanced datasets like Fashion MNIST, thoughtful splitting is critical to ensure fair evaluation, reproducibility, and proper generalisation.</p></div><a href=/projects/project12/project12_5/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_6/ class="link black dim">Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Training a neural network requires more than just a good dataset or an effective architecture—it requires the right optimiser. Stochastic Gradient Descent (SGD) is a staple of deep learning.</p><p>In my Fashion MNIST project, I used Stochastic Gradient Descent (SGD) to optimise a dense neural network. Why? Because simplicity doesn’t just work—it excels, especially when resources are limited or interpretability is key.</p></div><a href=/projects/project12/project12_6/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification." class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_7/ class="link black dim">Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification.</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Choosing the right loss function is one of the most critical decisions when building a neural network. For multi-class classification tasks, like predicting clothing categories in Fashion MNIST, the sparse categorical crossentropy (SCC) loss function is often the go-to solution. But what makes it so effective?</p><p>This blog dives into:</p><ul><li>What sparse categorical crossentropy is and how it works.</li><li>Why it’s the ideal choice for tasks involving multiple classes.</li><li>How to implement it efficiently in TensorFlow/Keras.</li></ul><p>By the end, you’ll have a solid understanding of this loss function and when to use it in your own projects.</p></div><a href=/projects/project12/project12_7/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Part 8. Trial and Error in Neural Network Training: Lessons from Fashion MNIST" class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/project12_8/ class="link black dim">Part 8. Trial and Error in Neural Network Training: Lessons from Fashion MNIST</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><figure><img src=/images/project12_images/pr12.jpg></figure><p><strong>View Project on GitHub</strong>:</p><a href=https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST target=_blank><img src=/images/github.png alt=GitHub style=width:40px;height:40px;vertical-align:middle></a><h3 id=introduction>Introduction</h3><p>Training neural networks (NNs) is a lot like navigating uncharted waters. No matter how much preparation or theoretical knowledge you have, it’s the experiments—and the inevitable mistakes—that shape your skills.</p><p>As a data scientist working on Fashion MNIST, a dataset of 28x28 grayscale images representing 10 clothing categories, I realised that building effective models requires more than just writing code; it demands iteration, debugging, and adaptability.</p></div><a href=/projects/project12/project12_8/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div><div class="relative w-100 mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><img src=/images/project12_images/pr12.jpg alt="Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment" class="w-100 br2 mb3">
<span class="f6 db">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</span><h1 class="f3 near-black"><a href=/projects/project12/ class="link black dim">Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>This section showcases the technical blogs related to this project. Each blog focuses on a specific aspect. Click on each post to learn more about the project components.</p></div><a href=/projects/project12/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></div></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=http://localhost:1313/>&copy; Natasha Smith Portfolio 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>