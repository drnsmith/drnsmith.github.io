<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Activation Functions on Natasha Smith Portfolio</title>
    <link>http://localhost:1313/tags/activation-functions/</link>
    <description>Recent content in Activation Functions on Natasha Smith Portfolio</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 10:58:08 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/activation-functions/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Part 1. Decoding Fashion MNIST: A Modern Benchmark for Deep Learning.</title>
      <link>http://localhost:1313/projects/project12/project12_1/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_1/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;You’ve seen it before: the iconic handwritten digits from the MNIST dataset, the quintessential benchmark for machine learning enthusiasts.&lt;/p&gt;&#xA;&lt;p&gt;But here’s the thing—MNIST is old news. It’s solved, overused, and no longer representative of real-world challenges. Fashion MNIST is a modern, robust alternative that brings fresh complexity to the table.&lt;/p&gt;&#xA;&lt;p&gt;Fashion MNIST is a game-changer. With its focus on apparel images like shirts, sneakers, and dresses, it mirrors the kind of messy, nuanced data we deal with today.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 2. Designing Dense Neural Networks: Lessons from Fashion MNIST.</title>
      <link>http://localhost:1313/projects/project12/project12_2/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_2/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;Designing neural networks (NNs) is an art as much as it is a science. When faced with the challenge of classifying Fashion MNIST images, I needed a lightweight yet powerful architecture to handle the complexity of apparel images. Dense neural networks, with their fully connected layers, were the perfect choice for this task.&lt;/p&gt;&#xA;&lt;p&gt;In this blog, we’ll walk through:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;How dense neural networks work and their role in image classification.&lt;/li&gt;&#xA;&lt;li&gt;Designing an efficient architecture using activation functions and layers.&lt;/li&gt;&#xA;&lt;li&gt;Practical lessons learned while optimising performance with Fashion MNIST.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;By the end, you’ll have a clear roadmap to build your own dense networks for image classification tasks.&#xA;Let’s dive in!&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 3. ReLU vs Sigmoid: Which Activation Function Wins on Fashion MNIST?</title>
      <link>http://localhost:1313/projects/project12/project12_3/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_3/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;When building neural networks (NNs), the activation function you choose can make or break your model. It’s the part of the network that decides whether a neuron &amp;ldquo;fires&amp;rdquo; and passes information forward.&lt;/p&gt;&#xA;&lt;p&gt;For years, Sigmoid was the go-to activation function, but then ReLU came along, revolutionising deep learning with its simplicity and effectiveness. But how do these activation functions stack up against each other in practice?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 4. Hyperparameter Tuning for Neural Networks: The Fashion MNIST Approach.</title>
      <link>http://localhost:1313/projects/project12/project12_4/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_4/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;When training neural networks (NNs), every parameter matters. Hyperparameters like learning rate and batch size aren’t learned by the model—they’re chosen by you. These settings can make or break your model’s performance. But how do you find the right combination?&lt;/p&gt;&#xA;&lt;p&gt;In this blog, I’ll take you through my experience fine-tuning hyperparameters for a NN trained on the Fashion MNIST dataset. We’ll cover:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 5. Perfecting Data Splits: Train-Test and Validation Strategies for Reliable Results. How thoughtful data splitting practices ensure consistent performance in machine learning pipelines.</title>
      <link>http://localhost:1313/projects/project12/project12_5/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_5/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;Machine learning models are only as good as the data they’re trained on. But even the best dataset won’t save you if your data splits are flawed. Splitting data into training, validation, and test sets seems straightforward, but small mistakes can lead to big problems like overfitting, underfitting, or unreliable performance metrics.&lt;/p&gt;&#xA;&lt;p&gt;You have a perfectly balanced dataset. Every class is equally represented, and it seems like splitting the data into training, testing, and validation sets should be a no-brainer. But even with balanced datasets like Fashion MNIST, thoughtful splitting is critical to ensure fair evaluation, reproducibility, and proper generalisation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 6. Simplicity and Control in Optimising Neural Networks: The Stochastic Gradient Descent optimiser and its role in fine-tuning neural networks.</title>
      <link>http://localhost:1313/projects/project12/project12_6/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_6/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;Training a neural network requires more than just a good dataset or an effective architecture—it requires the right optimiser. Stochastic Gradient Descent (SGD) is a staple of deep learning.&lt;/p&gt;&#xA;&lt;p&gt;In my Fashion MNIST project, I used Stochastic Gradient Descent (SGD) to optimise a dense neural network. Why? Because simplicity doesn’t just work—it excels, especially when resources are limited or interpretability is key.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 7. The Power of Sparse Categorical Crossentropy: A guide to understanding loss functions for multi-class classification.</title>
      <link>http://localhost:1313/projects/project12/project12_7/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_7/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;Choosing the right loss function is one of the most critical decisions when building a neural network. For multi-class classification tasks, like predicting clothing categories in Fashion MNIST, the sparse categorical crossentropy (SCC) loss function is often the go-to solution. But what makes it so effective?&lt;/p&gt;&#xA;&lt;p&gt;This blog dives into:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;What sparse categorical crossentropy is and how it works.&lt;/li&gt;&#xA;&lt;li&gt;Why it’s the ideal choice for tasks involving multiple classes.&lt;/li&gt;&#xA;&lt;li&gt;How to implement it efficiently in TensorFlow/Keras.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;By the end, you’ll have a solid understanding of this loss function and when to use it in your own projects.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 8. Trial and Error in Neural Network Training: Lessons from Fashion MNIST</title>
      <link>http://localhost:1313/projects/project12/project12_8/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/project12_8/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;http://localhost:1313/images/project12_images/pr12.jpg&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;&lt;strong&gt;View Project on GitHub&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;a href=&#34;https://github.com/drnsmith/Designing-Dense-NNs-Using-MNIST&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img src=&#34;http://localhost:1313/images/github.png&#34; alt=&#34;GitHub&#34; style=&#34;width:40px; height:40px; vertical-align: middle;&#34;&gt;&#xA;  &lt;/a&gt;&#xA;&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;&#xA;&lt;p&gt;Training neural networks (NNs) is a lot like navigating uncharted waters. No matter how much preparation or theoretical knowledge you have, it’s the experiments—and the inevitable mistakes—that shape your skills.&lt;/p&gt;&#xA;&lt;p&gt;As a data scientist working on Fashion MNIST, a dataset of 28x28 grayscale images representing 10 clothing categories, I realised that building effective models requires more than just writing code; it demands iteration, debugging, and adaptability.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building Robust CNN Pipelines for Fashion MNIST: From Data to Deployment</title>
      <link>http://localhost:1313/projects/project12/</link>
      <pubDate>Wed, 05 Jun 2024 10:58:08 -0400</pubDate>
      <guid>http://localhost:1313/projects/project12/</guid>
      <description>&lt;p&gt;This section showcases the technical blogs related to this project. Each blog focuses on a specific aspect. Click on each post to learn more about the project components.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
